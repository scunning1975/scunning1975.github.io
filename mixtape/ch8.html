<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>9 Difference-in-Differences | Causal Inference</title>
<meta name="author" content="Scott Cunningham">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.6/header-attrs.js"></script><script src="libs/jquery-3.5.1/jquery-3.5.1.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.5.3/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.5.3/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.3.9000/tabs.js"></script><script src="libs/bs3compat-0.2.3.9000/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><style>
    @import url('https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,400;0,700;1,400;1,700&family=Roboto:ital,wght@0,700;1,300&display=swap');
    </style>
<script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS --><link rel="stylesheet" href="css/style.css">
<link rel="stylesheet" href="css/toc.css">
<link rel="stylesheet" href="css/causal_inference_style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="&lt;i&gt;The Mixtape&lt;/i&gt;"><span style="font-weight:bold">Causal Inference</span></a>:
        <small class="text-muted"><i>The Mixtape</i></small>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="ch1.html"><span class="header-section-number">2</span> Probability and Regression Review</a></li>
<li><a class="" href="ch2.html"><span class="header-section-number">3</span> Directed Acyclic Graphs</a></li>
<li><a class="" href="ch3.html"><span class="header-section-number">4</span> Potential Outcomes Causal Model</a></li>
<li><a class="" href="ch4.html"><span class="header-section-number">5</span> Matching and Subclassification</a></li>
<li><a class="" href="ch5.html"><span class="header-section-number">6</span> Regression Discontinuity</a></li>
<li><a class="" href="ch6.html"><span class="header-section-number">7</span> Instrumental Variables</a></li>
<li><a class="" href="ch7.html"><span class="header-section-number">8</span> Panel Data</a></li>
<li><a class="active" href="ch8.html"><span class="header-section-number">9</span> Difference-in-Differences</a></li>
<li><a class="" href="ch9.html"><span class="header-section-number">10</span> Synthetic Control</a></li>
<li><a class="" href="ch10.html"><span class="header-section-number">11</span> Conclusion</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/scunning1975/mixtape">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="ch8" class="section level1" number="9">
<h1>
<span class="header-section-number">9</span> Difference-in-Differences<a class="anchor" aria-label="anchor" href="#ch8"><i class="fas fa-link"></i></a>
</h1>
<div class="cover-box">
<div class="row">
    <div class="col-xs-8 col-md-4 cover-img">
        <a href="https://www.amazon.com/dp/0300251688"><img src="../images/cover.jpg" alt="Buy Today!"></a>
    </div>
    
    <div class="col-xs-12 col-md-8 cover-text-box">
            <h2> 
                Causal Inference: 
                <br><span style="font-style: italic; font-weight:bold; font-size: 20px;">The Mixtape.</span>
            </h2> 
            
        <div class="cover-text">
            <p>Buy the print version today:</p>
            
            <div class="chips">
                <a href="https://www.amazon.com/dp/0300251688" class="app-chip"> 
                    <i class="fab fa-amazon" aria-hidden="true"></i> Buy from Amazon 
                </a>
    
                <a href="https://yalebooks.yale.edu/book/9780300251685/causal-inference" class="app-chip"> 
                    <i class="fas fa-book" aria-hidden="true"></i> Buy from Yale Press 
                </a>
            </div>
        </div>
    </div>
</div>
</div>
<p>The difference-in-differences design is an early quasi-experimental identification strategy for estimating causal effects that predates the randomized experiment by roughly eighty-five years. It has become the single most popular research design in the quantitative social sciences, and as such, it merits careful study by researchers everywhere.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;A simple search on Google Scholar for phrase “difference-in-differences” yields over forty thousand hits.&lt;/p&gt;"><sup>139</sup></a> In this chapter, I will explain this popular and important research design both in its simplest form, where a group of units is treated at the same time, and the more common form, where groups of units are treated at different points in time. My focus will be on the identifying assumptions needed for estimating treatment effects, including several practical tests and robustness exercises commonly performed, and I will point you to some of the work on difference-in-differences design (DD) being done at the frontier of research. I have included several replication exercises as well.</p>
<div id="john-snows-cholera-hypothesis" class="section level2" number="9.1">
<h2>
<span class="header-section-number">9.1</span> John Snow’s Cholera Hypothesis<a class="anchor" aria-label="anchor" href="#john-snows-cholera-hypothesis"><i class="fas fa-link"></i></a>
</h2>
<p>When thinking about situations in which a difference-in-differences design can be used, one usually tries to find an instance where a consequential treatment was given to some people or units but denied to others “haphazardly.” This is sometimes called a “natural experiment” because it is based on naturally occurring variation in some treatment variable that affects only some units over time. All good difference-in-differences designs are based on some kind of natural experiment. And one of the most interesting natural experiments was also one of the first difference-in-differences designs. This is the story of how John Snow convinced the world that cholera was transmitted by water, not air, using an ingenious natural experiment <span class="citation">(Snow <a href="references.html#ref-Snow1854" role="doc-biblioref">1855</a>)</span>.</p>
<p>Cholera is a vicious disease that attacks victims suddenly, with acute symptoms such as vomiting and diarrhea. In the nineteenth century, it was usually fatal. There were three main epidemics that hit London, and like a tornado, they cut a path of devastation through the city. Snow, a physician, watched as tens of thousands suffered and died from a mysterious plague. Doctors could not help the victims because they were mistaken about the mechanism that caused cholera to spread between people.</p>
<p>The majority medical opinion about cholera transmission at that time was <em>miasma</em>, which said diseases were spread by microscopic poisonous particles that infected people by floating through the air. These particles were thought to be inanimate, and because microscopes at that time had incredibly poor resolution, it would be years before microorganisms would be seen. Treatments, therefore, tended to be designed to stop poisonous dirt from spreading through the air. But tried and true methods like quarantining the sick were strangely ineffective at slowing down this plague.</p>
<p>John Snow worked in London during these epidemics. Originally, Snow—like everyone—accepted the <em>miasma</em> theory and tried many ingenious approaches based on the theory to block these airborne poisons from reaching other people. He went so far as to cover the sick with burlap bags, for instance, but the disease still spread. People kept getting sick and dying. Faced with the theory’s failure to explain cholera, he did what good scientists do—he changed his mind and began look for a new explanation.</p>
<p>Snow developed a novel theory about cholera in which the active agent was not an inanimate particle but was rather a living organism. This microorganism entered the body through food and drink, flowed through the alimentary canal where it multiplied and generated a poison that caused the body to expel water. With each evacuation, the organism passed out of the body and, importantly, flowed into England’s water supply. People unknowingly drank contaminated water from the Thames River, which caused them to contract cholera. As they did, they would evacuate with vomit and diarrhea, which would flow into the water supply again and again, leading to new infections across the city. This process repeated through a multiplier effect which was why cholera would hit the city in epidemic waves.</p>
<p>Snow’s years of observing the clinical course of the disease led him to question the usefulness of <em>miasma</em> to explain cholera. While these were what we would call “anecdote,” the numerous observations and imperfect studies nonetheless shaped his thinking. Here’s just a few of the observations which puzzled him. He noticed that cholera transmission tended to follow human commerce. A sailor on a ship from a cholera-free country who arrived at a cholera-stricken port would only get sick after landing or taking on supplies; he would not get sick if he remained docked. Cholera hit the poorest communities worst, and those people were the very same people who lived in the most crowded housing with the worst hygiene. He might find two apartment buildings next to one another, one would be heavily hit with cholera, but strangely the other one wouldn’t. He then noticed that the first building would be contaminated by runoff from privies but the water supply in the second building was cleaner. While these observations weren’t impossible to reconcile with <em>miasma</em>, they were definitely unusual and didn’t seem obviously consistent with <em>miasmis</em>.</p>
<p>Snow tucked away more and more anecdotal evidence like these. But, while this evidence raised some doubts in his mind, he was not convinced. He needed a smoking gun if he were to eliminate all doubt that cholera was spread by water, not air. But where would he find that evidence? More importantly, what would evidence like that evenlook like?</p>
<p>Let’s imagine the following thought experiment. If Snow was a dictator with unlimited wealth and power, how could he test his theory that cholera is waterborne? One thing he could do is flip a coin over each household member—heads you drink from the contaminated Thames, tails you drink from some uncontaminated source. Once the assignments had been made, Snow could simply compare cholera mortality between the two groups. If those who drank the clean water were less likely to contract cholera, then this would suggest that cholera was waterborne.</p>
<p>Knowledge that physical randomization could be used to identify causal effects was still eighty-five years away. But there were other issues besides ignorance that kept Snow from physical randomization. Experiments like the one I just described are also impractical, infeasible, and maybe even unethical—which is why social scientists so often rely on natural experiments that mimic important elements of randomized experiments. But what natural experiment was there? Snow needed to find a situation where uncontaminated water had been distributed to a large number of people as if by random chance, and then calculate the difference between those those who did and did not drink contaminated water. Furthermore, the contaminated water would need to be allocated to people in ways that were unrelated to the ordinary determinants of cholera mortality, such as hygiene and poverty, implying a degree of balance on covariates between the groups. And then he remembered—a potential natural experiment in London a year earlier had reallocated clean water to citizens of London. Could this work?</p>
<p>In the 1800s, several water companies served different areas of the city. Some neighborhoods were even served by more than one company. They took their water from the Thames, which had been polluted by victims’ evacuations via runoff. But in 1849, the Lambeth water company had moved its intake pipes upstream higher up the Thames, above the main sewage discharge point, thus giving its customers uncontaminated water. They did this to obtain cleaner water, but it had the added benefit of being too high up the Thames to be infected with cholera from the runoff. Snow seized on this opportunity. He realized that it had given him a natural experiment that would allow him to test his hypothesis that cholera was waterborne by comparing the households. If his theory was right, then the Lambeth houses should have lower cholera death rates than some other set of households whose water was infected with runoff—what we might call today the explicit counterfactual. He found his explicit counterfactual in the Southwark and Vauxhall Waterworks Company.</p>
<p>Unlike Lambeth, the Southwark and Vauxhall Waterworks Company had <em>not</em> moved their intake point upstream, and Snow spent an entire book documenting similarities between the two companies’ households. For instance, sometimes their service cut an irregular path through neighborhoods and houses such that the households on either side were very similar; the only difference being they drank different water with different levels of contamination from runoff. Insofar as the kinds of people that each company serviced were observationally equivalent, then perhaps they were similar on the relevant unobservables as well.</p>
<p>Snow meticulously collected data on household enrollment in water supply companies, going door to door asking household heads the name of their utility company. Sometimes these individuals didn’t know, though, so he used a saline test to determine the source himself <span class="citation">(Coleman <a href="references.html#ref-Coleman2019" role="doc-biblioref">2019</a>)</span>. He matched those data with the city’s data on the cholera death rates at the household level. It was in many ways as advanced as any study we might see today for how he carefully collected, prepared, and linked a variety of data sources to show the relationship between water purity and mortality. But he also displayed scientific ingenuity for how he carefully framed the research question and how long he remained skeptical until the research design’s results convinced him otherwise. After combining everthing, he was able to generate extremely persuasive evidence that influenced policymakers in the city.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;John Snow is one of my personal heroes. He had a stubborn commitment to the truth and was unpersuaded by low-quality causal evidence. That simultaneous skepticism and open-mindedness gave him the willingness to question common sense when common sense failed to provide satisfactory explanations.&lt;/p&gt;"><sup>140</sup></a></p>
<p>Snow wrote up all of his analysis in a manuscript entitled <em>On the Mode of Communication of Cholera</em> <span class="citation">(Snow <a href="references.html#ref-Snow1854" role="doc-biblioref">1855</a>)</span>. Snow’s main evidence was striking, and I will discuss results based on Table XII and Table IX (not shown) in Table <a href="ch8.html#tab:snow1">9.1</a>. The main difference between my version and his version of Table XII is that I will use his data to estimate a treatment effect using difference-in-differences.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:snow1">Table 9.1: </span> Modified Table XII (Snow 1854).</caption>
<thead><tr class="header">
<th align="left"><strong>Company name</strong></th>
<th align="center"><strong>1849</strong></th>
<th align="center"><strong>1854</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Southwark and Vauxhall</td>
<td align="center">135</td>
<td align="center">147</td>
</tr>
<tr class="even">
<td align="left">Lambeth</td>
<td align="center">85</td>
<td align="center">19</td>
</tr>
</tbody>
</table></div>
<div id="table-xii" class="section level3" number="9.1.1">
<h3>
<span class="header-section-number">9.1.1</span> Table XII<a class="anchor" aria-label="anchor" href="#table-xii"><i class="fas fa-link"></i></a>
</h3>
<p>In 1849, there were 135 cases of cholera per 10,000 households at Southwark and Vauxhall and 85 for Lambeth. But in 1854, there were 147 per 100,000 in Southwark and Vauxhall, whereas Lambeth’s cholera cases per 10,000 households fell to 19.</p>
<p>While Snow did not explicitly calculate the difference-in-differences, the ability to do so was there <span class="citation">(Coleman <a href="references.html#ref-Coleman2019" role="doc-biblioref">2019</a>)</span>. If we difference Lambeth’s 1854 value from its 1849 value, followed by the same after and before differencing for Southwark and Vauxhall, we can calculate an estimate of the ATT equaling 78 fewer deaths per 10,000. While Snow would go on to produce evidence showing cholera deaths were concentrated around a pump on Broad Street contaminated with cholera, he allegedly considered the simple difference-in-differences the more convincing test of his hypothesis.</p>
<p>The importance of the work Snow undertook to understand the causes of cholera in London cannot be overstated. It not only lifted our ability to estimate causal effects with observational data, it advanced science and ultimately saved lives. Of Snow’s work on the cause of cholera transmission, <span class="citation">Freedman (<a href="references.html#ref-Freedman1991" role="doc-biblioref">1991</a>)</span> states:</p>
<blockquote>
<p>The force of Snow’s argument results from the clarity of the prior reasoning, the bringing together of many different lines of evidence, and the amount of shoe leather Snow was willing to use to get the data. Snow did some brilliant detective work on nonexperimental data. What is impressive is not the statistical technique but the handling of the scientific issues. He made steady progress from shrewd observation through case studies to analyze ecological data. In the end, he found and analyzed a natural experiment. (p.298)</p>
</blockquote>
</div>
</div>
<div id="estimation-1" class="section level2" number="9.2">
<h2>
<span class="header-section-number">9.2</span> Estimation<a class="anchor" aria-label="anchor" href="#estimation-1"><i class="fas fa-link"></i></a>
</h2>
<div id="a-simple-table" class="section level3" number="9.2.1">
<h3>
<span class="header-section-number">9.2.1</span> A simple table<a class="anchor" aria-label="anchor" href="#a-simple-table"><i class="fas fa-link"></i></a>
</h3>
<p>Let’s look at this example using some tables, which hopefully will help give you an idea of the intuition behind DD, as well as some of its identifying assumptions.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;You’ll sometimes see acronyms for difference-in-differences like DD, DiD, Diff-in-diff, or even, God forbid, DnD.&lt;/p&gt;"><sup>141</sup></a> Assume that the intervention is clean water, which I’ll write as <span class="math inline">\(D\)</span>, and our objective is to estimate <span class="math inline">\(D\)</span>’s causal effect on cholera deaths. Let cholera deaths be represented by the variable <span class="math inline">\(Y\)</span>. Can we identify the causal effect of <em>D</em> if we just compare the post-treatment 1854 Lambeth cholera death values to that of the 1854 Southwark and Vauxhall values? This is in many ways an obvious choice, and in fact, it is one of the more common naive approaches to causal inference. After all, we have a control group, don’t we? Why can’t we just compare a treatment group to a control group? Let’s look and see.</p>
<p>One of the things we immediately must remember is that the simple difference in outcomes, which is all we are doing here, only collapsed to the ATE if the treatment had been randomized. But it is never randomized in the real world where most choices if not all choices made by real people is endogenous to potential outcomes. Let’s represent now the differences between Lambeth and Southwark and Vauxhall with fixed level differences, or fixed effects, represented by <span class="math inline">\(L\)</span> and <span class="math inline">\(SV\)</span>. Both are unobserved, unique to each company, and fixed over time. What these fixed effects mean is that even if Lambeth hadn’t changed its water source there, would still be something determining cholera deaths, which is just the time-invariant unique differences between the two companies as it relates to cholera deathsin 1854.</p>
<div class="inline-table"><table class="table table-sm">
<caption>Compared to what? Different companies.</caption>
<thead><tr class="header">
<th align="left"><strong>Company</strong></th>
<th align="center"><strong>Outcome</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Lambeth</td>
<td align="center"><span class="math inline">\(Y=L + D\)</span></td>
</tr>
<tr class="even">
<td align="left">Southwark and Vauxhall</td>
<td align="center"><span class="math inline">\(Y=SV\)</span></td>
</tr>
</tbody>
</table></div>
<p>When we make a simple comparison between Lambeth and Southwark and Vauxhall, we get an estimated causal effect equalling <span class="math inline">\(D+(L-SV)\)</span>. Notice the second term, <span class="math inline">\(L-SV\)</span>. We’ve seen this before. It’s the selection bias we found from the decomposition of the simple difference in outcomes from earlier in the book.</p>
<p>Okay, so say we realize that we cannot simply make cross-sectional comparisons between two units because of selection bias. Surely, though, we can compare a unit to itself? This is sometimes called an interrupted time series. Let’s consider that simple before-and-after difference for Lambeth now.</p>
<div class="inline-table"><table class="table table-sm">
<caption>Compared to what? Before and after.</caption>
<thead><tr class="header">
<th align="left"><strong>Company</strong></th>
<th align="center"><strong>Time</strong></th>
<th align="center"><strong>Outcome</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Lambeth</td>
<td align="center">Before</td>
<td align="center"><span class="math inline">\(Y=L\)</span></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center">After</td>
<td align="center"><span class="math inline">\(Y=L + (T + D)\)</span></td>
</tr>
</tbody>
</table></div>
<p>While this procedure successfully eliminates the Lambeth fixed effect (unlike the cross-sectional difference), it doesn’t give me an unbiased estimate of <span class="math inline">\(D\)</span> because differences can’t eliminate the natural changes in the cholera deaths over time. Recall, these events were oscillating in waves. I can’t compare Lambeth before and after (<span class="math inline">\(T+D\)</span>) because of <span class="math inline">\(T\)</span>, which is an omitted variable.</p>
<p>The intuition of the DD strategy is remarkably simple: combine these two simpler approaches so the selection bias and the effect of time are, in turns, eliminated. Let’s look at it in the followingtable.</p>
<div class="inline-table"><table class="table table-sm">
<caption>Compared to what? Difference in each company’s differences.</caption>
<thead><tr class="header">
<th align="left"><strong>Companies</strong></th>
<th align="left"><strong>Time</strong></th>
<th align="left"><strong>Outcome</strong></th>
<th align="left"><span class="math inline">\(D_1\)</span></th>
<th align="left"><span class="math inline">\(D_2\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Lambeth</td>
<td align="left">Before</td>
<td align="left"><span class="math inline">\(Y=L\)</span></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left">After</td>
<td align="left"><span class="math inline">\(Y=L + T + D\)</span></td>
<td align="left"><span class="math inline">\(T+D\)</span></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"><span class="math inline">\(D\)</span></td>
</tr>
<tr class="even">
<td align="left">Southwark and Vauxhall</td>
<td align="left">Before</td>
<td align="left"><span class="math inline">\(Y=SV\)</span></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left">After</td>
<td align="left"><span class="math inline">\(Y=SV + T\)</span></td>
<td align="left"><span class="math inline">\(T\)</span></td>
<td align="left"></td>
</tr>
</tbody>
</table></div>
<p>The first difference, <span class="math inline">\(D_1\)</span>, does the simple before-and-after difference. This ultimately eliminates the unit-specific fixed effects. Then, once those differences are made, we difference the differences (hence the name) to get the unbiased estimate of <span class="math inline">\(D\)</span>.</p>
<p>But there’s a a key assumption with a DD design, and that assumption is discernible even in this table. We are assuming that there is no time-variant company specific unobservables. Nothing unobserved in Lambeth households that is changing between these two periods that <em>also</em> determines cholera deaths. This is equivalent to assuming that <span class="math inline">\(T\)</span> is the same for all units. And we call this the <em>parallel trends</em> assumption. We will discuss this assumption repeatedly as the chapter proceeds, as it is the most important assumption in the design’s engine. If you can buy off on the parallel trends assumption, then DD will identify the causal effect.</p>
<p>DD is a powerful, yet amazingly simple design. Using repeated observations on a treatment and control unit (usually several units), we can eliminate the unobserved heterogeneity to provide a credible estimate of the average treatment effect on the treated (ATT) by transforming the data in very specific ways. But when and why does this process yield the correct answer? Turns out, there is more to it than meets the eye. And it is imperative on the front end that you understand what’s under the hood so that you can avoid conceptual errors about this design.</p>
</div>
<div id="the-simple-2times-2-dd" class="section level3" number="9.2.2">
<h3>
<span class="header-section-number">9.2.2</span> The simple <span class="math inline">\(2\times 2\)</span> DD<a class="anchor" aria-label="anchor" href="#the-simple-2times-2-dd"><i class="fas fa-link"></i></a>
</h3>
<p>The cholera case is a particular kind of DD design that <span class="citation">Goodman-Bacon (<a href="references.html#ref-Bacon2019" role="doc-biblioref">2019</a>)</span> calls the <span class="math inline">\(2\times 2\)</span> DD design. The <span class="math inline">\(2\times 2\)</span> DD design has a treatment group <span class="math inline">\(k\)</span> and untreated group <span class="math inline">\(U\)</span>. There is a pre-period for the treatment group, <span class="math inline">\(\mathop{\mathrm{pre}}(k)\)</span>; a post-period for the treatment group, <span class="math inline">\(\mathop{\mathrm{post}}(k)\)</span>; a pre-treatment period for the untreated group, <span class="math inline">\(\mathop{\mathrm{pre}}(U)\)</span>; and a post-period for the untreated group, <span class="math inline">\(\mathop{\mathrm{post}}(U)\)</span> So:
<span class="math display">\[
  \widehat{\delta}^{2\times 2}_{kU} = \bigg ( \overline{y}_k^{\mathop{\mathrm{post}}(k)} - \overline{y}_k^{\mathop{\mathrm{pre}}(k)} \bigg ) - \bigg ( \overline{y}_U^{\mathop{\mathrm{post}}(k)} - \overline{y}_U^{\mathop{\mathrm{pre}}(k)} \bigg )
\]</span>
where <span class="math inline">\(\widehat{\delta}_{kU}\)</span> is the estimated ATT for group <span class="math inline">\(k\)</span>, and <span class="math inline">\(\overline{y}\)</span> is the sample mean for that particular group in a particular time period. The first paragraph differences the treatment group, <span class="math inline">\(k\)</span>, after minus before, the second paragraph differences the untreated group, <span class="math inline">\(U\)</span>, after minus before. And once those quantities are obtained, we difference the second term from the first.</p>
<p>But this is simply the mechanics of calculations. What exactly is this estimated parameter mapping onto? To understand that, we must convert these sample averages into conditional expectations of potential outcomes. But that is easy to do when working with sample averages, as we will see here. First let’s rewrite this as a conditional expectation.
<span class="math display">\[
\widehat{\delta}^{2\times 2}_{kU} = \bigg(E\big[Y_k  \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Post}}\big] - E\big[Y_k  \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Pre}}\big]\bigg)- \bigg(E\big[Y_U  \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Post}}\big] - E\big[Y_U  \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Pre}}\big]\bigg)
\]</span></p>
<p>Now let’s use the switching equation, which transforms historical quantities of <span class="math inline">\(Y\)</span> into potential outcomes. As we’ve done before, we’ll do a little trick where we add zero to the right-hand side so that we can use those terms to help illustrate something important.
<span class="math display">\[\begin{align}
&amp;\widehat{\delta}^{2\times 2}_{kU} = \bigg ( \underbrace{E\big[Y^1_k  \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Post}}\big] - E\big[Y^0_k  \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Pre}}\big] \bigg ) - \bigg(E\big[Y^0_U  \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Post}}\big] - E\big[ Y^0_U  \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Pre}}\big]}_{\text{Switching equation}} \bigg) \\
&amp;+ \underbrace{E\big[Y_k^0  \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Post}}\big] - E\big[Y^0_k  \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Post}}\big]}_{\text{Adding zero}}
\end{align}\]</span></p>
<p>Now we simply rearrange these terms to get the decomposition of the <span class="math inline">\(2\times 2\)</span> DD in terms of conditional expected potential outcomes.
<span class="math display">\[\begin{align}
&amp;\widehat{\delta}^{2\times 2}_{kU} = \underbrace{E\big[Y^1_k \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Post}}\big] - E\big[Y^0_k \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Post}}\big]}_{\text{ATT}} \\
&amp;+\Big[\underbrace{E\big[Y^0_k \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Post}}\big] - E\big[Y^0_k \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Pre}}\big] \Big] - \Big[E\big[Y^0_U \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Post}}\big] - E\big[Y_U^0 \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Pre}}\big] }_{\text{Non-parallel trends bias in $2\times 2$ case}} \Big]
\end{align}\]</span></p>
<p>Now, let’s study this last term closely. This simple <span class="math inline">\(2\times 2\)</span> difference-in-differences will isolate the ATT (the first term) if and only if the second term zeroes out. But why would this second term be zero? It would equal zero if the first difference involving the treatment group, <span class="math inline">\(k\)</span>, equaled the second difference involving the untreated group, <span class="math inline">\(U\)</span>.</p>
<p>But notice the term in the second line. Notice anything strange about it? The object of interest is <span class="math inline">\(Y^0\)</span>, which is some outcome in a world without the treatment. But it’s the <em>post</em> period, and in the post period, <span class="math inline">\(Y=Y^1\)</span> not <span class="math inline">\(Y^0\)</span> by the switching equation. Thus, the first term is <em>counterfactual</em>. And as we’ve said over and over, counterfactuals are not observable. This bottom line is often called the parallel trends assumption and it is by definition untestable since we cannot observe this counterfactual conditional expectation. We will return to this again, but for now I simply present it for your consideration.</p>
</div>
<div id="dd-and-the-minimum-wage" class="section level3" number="9.2.3">
<h3>
<span class="header-section-number">9.2.3</span> DD and the Minimum Wage<a class="anchor" aria-label="anchor" href="#dd-and-the-minimum-wage"><i class="fas fa-link"></i></a>
</h3>
<p>Now I’d like to talk about more explicit economic content, and the minimum wage is as good a topic as any. The modern use of DD was brought into the social sciences through esteemed labor economist Orley <span class="citation">Ashenfelter (<a href="references.html#ref-Ashenfelter1978" role="doc-biblioref">1978</a>)</span>. His study was no doubt influential to his advisee, David Card, arguably the greatest labor economist of his generation. Card would go on to use the method in several pioneering studies, such as <span class="citation">Card (<a href="references.html#ref-Card1990" role="doc-biblioref">1990</a>)</span>. But I will focus on one in particular—his now-classic minimum wage study <span class="citation">(Card and Krueger <a href="references.html#ref-Card1994" role="doc-biblioref">1994</a>)</span>.</p>
<p><span class="citation">Card and Krueger (<a href="references.html#ref-Card1994" role="doc-biblioref">1994</a>)</span> is an infamous study both because of its use of an explicit counterfactual for estimation, and because the study challenges many people’s common beliefs about the negative effects of the minimum wage. It lionized a massive back-and-forth minimum-wage literature that continues to this day.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;That literature is too extensive to cite here, but one can find reviews of a great deal of the contemporary literature on minimum wages in &lt;span class="citation"&gt;Neumark, Salas, and Wascher (&lt;a href="references.html#ref-Neumark2014" role="doc-biblioref"&gt;2014&lt;/a&gt;)&lt;/span&gt; and &lt;span class="citation"&gt;Cengiz et al. (&lt;a href="references.html#ref-Cengiz2019" role="doc-biblioref"&gt;2019&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;'><sup>142</sup></a> So controversial was this study that James Buchanan, the Nobel Prize winner, called those influenced by <span class="citation">Card and Krueger (<a href="references.html#ref-Card1994" role="doc-biblioref">1994</a>)</span> “camp following whores” in a letter to the editor of the <em>Wall Street Journal</em> <span class="citation">(Buchanan <a href="references.html#ref-Buchanan1996" role="doc-biblioref">1996</a>)</span>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;James Buchanan won the Nobel Prize for his pioneering work on the theory of public choice. He was not, though, a labor economist, and to my knowledge did not have experience estimating causal effects using explicit counterfactuals with observational data. A Google Scholar search for “James Buchanan minimum wage” returned only one hit, the previously mentioned &lt;em&gt;Wall Street Journal&lt;/em&gt; letter to the editor. I consider his criticism to be ideologically motivated ad hominem and as such unhelpful in this debate.&lt;/p&gt;"><sup>143</sup></a></p>
<p>Suppose you are interested in the effect of minimum wages on employment. Theoretically, you might expect that in competitive labor markets, an increase in the minimum wage would move us up a downward-sloping demand curve, causing employment to fall. But in labor markets characterized by monopsony, minimum wages can increase employment. Therefore, there are strong theoretical reasons to believe that the effect of the minimum wage on employment is ultimately an empirical question depending on many local contextual factors. This is where <span class="citation">Card and Krueger (<a href="references.html#ref-Card1994" role="doc-biblioref">1994</a>)</span> entered. Could they uncover whether minimum wages were ultimately harmful or helpful in some local economy?</p>
<p>It’s always useful to start these questions with a simple thought experiment: if you had a billion dollars, complete discretion and could run a randomized experiment, how would you test whether minimum wages increased or decreased employment? You might go across the hundreds of local labor markets in the United States and flip a coin—heads, you raise the minimum wage; tails, you keep it at the status quo. As we’ve done before, these kinds of thought experiments are useful for clarifying both the research design and the causal question.</p>
<p>Lacking a randomized experiment, <span class="citation">Card and Krueger (<a href="references.html#ref-Card1994" role="doc-biblioref">1994</a>)</span> decided on a next-best solution by comparing two neighboring states before and after a minimum-wage increase. It was essentially the same strategy that Snow used in his cholera study and a strategy that economists continue to use, in one form or another, to this day <span class="citation">(Dube, Lester, and Reich <a href="references.html#ref-Dube2010" role="doc-biblioref">2010</a>)</span>.</p>
<p>New Jersey was set to experience an increase in the state minimum wage from $4.25 to $5.05 in November 1992, but neighboring Pennsylvania’s minimum wage was staying at $4.25. Realizing they had an opportunity to evaluate the effect of the minimum-wage increase by comparing the two states before and after, they fielded a survey of about four hundred fast-food restaurants in both states—once in February 1992 (before) and again in November (after). The responses from this survey were then used to measure the outcomes they cared about (i.e., employment). As we saw with Snow, we see again here that shoe leather is as important as any statistical technique in causal inference.</p>
<p>Let’s look at whether the minimum-wage hike in New Jersey in fact raised the minimum wage by examining the distribution of wages in the fast food stores they surveyed. Figure <a href="ch8.html#fig:wage-dist">9.1</a> shows the distribution of wages in November 1992 after the minimum-wage hike. As can be seen, the minimum-wage hike was binding, evidenced by the mass of wages at the minimum wage in New Jersey.</p>
<p>As a caveat, notice how effective this is at convincing the reader that the minimum wage in New Jersey was binding. This piece of data visualization is not a trivial, or even optional, strategy to be taken in studies such as this. Even John Snow presented carefully designed maps of the distribution of cholera deaths throughout London. Beautiful pictures displaying the “first stage” effect of the intervention on the treatment are crucial in the rhetoric of causal inference, and few have done it as well as Card and Krueger.</p>
<div class="figure">
<span id="fig:wage-dist"></span>
<img src="graphics/CK_fig2.jpg" alt="Distribution of wages for NJ and PA in November 1992. Reprinted from @Card1994." width="100%"><p class="caption">
Figure 9.1: Distribution of wages for NJ and PA in November 1992. Reprinted from <span class="citation">Card and Krueger (<a href="references.html#ref-Card1994" role="doc-biblioref">1994</a>)</span>.
</p>
</div>
<p>Let’s remind ourselves what we’re after—the average causal effect of the minimum-wage hike on employment, or the ATT. Using our decomposition of the <span class="math inline">\(2\times 2\)</span> DD from earlier, we can write it out as:
<span class="math display">\[\begin{align}
&amp;\widehat{\delta}^{2\times 2}_{NJ,PA} = \underbrace{E\big[Y^1_{NJ} \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Post}}\big] - E\big[Y^0_{NJ} \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Post}}\big]}_{\text{ATT}} \\
&amp;+ \Big[\underbrace{E\big[Y^0_{NJ} \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Post}}\big] - E\big[Y^0_{NJ} \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Pre}}\big] \Big]-\Big[E\big[Y^0_{PA} \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Post}}\big] - E\big[Y_{PA}^0 \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Pre}}\big] }_{\text{Non-parallel trends bias}} \Big]
\end{align}\]</span></p>
<p>Again, we see the key assumption: the parallel-trends assumption, which is represented by the first difference in the second line. Insofar as parallel trends holds in this situation, then the second term goes to zero, and the <span class="math inline">\(2\times 2\)</span> DD collapses to the ATT.</p>
<p>The <span class="math inline">\(2\times 2\)</span> DD requires differencing employment in NJ and PA, then differencing those first differences. This set of steps estimates the true ATT so long as the parallel-trends bias is zero. When that is true, <span class="math inline">\(\widehat{\delta}^{2\times 2}\)</span> is equal to <span class="math inline">\(\delta^{ATT}\)</span>. If this bottom line is not zero, though, then simple <span class="math inline">\(2\times 2\)</span> suffers from unknown bias—could bias it upwards, could bias it downwards, could flip the sign entirely. Table @ref(tab:minwage_dd) shows the results of this exercise from <span class="citation">Card and Krueger (<a href="references.html#ref-Card1994" role="doc-biblioref">1994</a>)</span>.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:minwage-dd">Table 9.2: </span> Simple DD using sample averages on full-time employment.</caption>
<tbody>
<tr class="odd">
<td></td>
<td></td>
<td align="center">Stores by State</td>
<td></td>
</tr>
<tr class="even">
<td>Dependent Variable</td>
<td>PA</td>
<td align="center">NJ</td>
<td>NJ – PA</td>
</tr>
<tr class="odd">
<td>FTW before</td>
<td>23.3</td>
<td align="center">20.44</td>
<td><span class="math inline">\(-2.89\)</span></td>
</tr>
<tr class="even">
<td></td>
<td>(1.35)</td>
<td align="center">(0.51)</td>
<td>(1.44)</td>
</tr>
<tr class="odd">
<td>FTE after</td>
<td>21.147</td>
<td align="center">21.03</td>
<td><span class="math inline">\(-0.14\)</span></td>
</tr>
<tr class="even">
<td></td>
<td>(0.94)</td>
<td align="center">(0.52)</td>
<td>(1.07)</td>
</tr>
<tr class="odd">
<td>Change in mean FTE</td>
<td><span class="math inline">\(-2.16\)</span></td>
<td align="center">0.59</td>
<td>2.76</td>
</tr>
<tr class="even">
<td></td>
<td>(1.25)</td>
<td align="center">(0.54)</td>
<td>(1.36)</td>
</tr>
</tbody>
</table></div>
<p class="footnote">
Standard errors in parentheses.
</p>
<p>Here you see the result that surprised many people. <span class="citation">Card and Krueger (<a href="references.html#ref-Card1994" role="doc-biblioref">1994</a>)</span> estimate an ATT of +2.76 additional mean full-time-equivalent employment, as opposed to some negative value which would be consistent with competitive input markets. Herein we get Buchanan’s frustration with the paper, which is based mainly on a particular model he had in mind, rather than a criticism of the research design the authors used.</p>
<p>While differences in sample averages will identify the ATT under the parallel assumption, we may want to use multivariate regression instead. For instance, if you need to avoid omitted variable bias through controlling for endogenous covariates that vary over time, then you may want to use regression. Such strategies are another way of saying that you will need to close some known critical backdoor. Another reason for the equation is that by controlling for more appropriate covariates, you can reduce residual variance and improve the precision of your DD estimate.</p>
<p>Using the switching equation, and assuming a constant state fixed effect and time fixed effect, we can write out a simple regression model estimating the causal effect of the minimum wage on employment, <span class="math inline">\(Y\)</span>. This simple <span class="math inline">\(2\times 2\)</span> is estimated with the following equation:
<span class="math display">\[
Y_{its} = \alpha + \gamma NJ_s + \lambda D_t + \delta (NJ \times D)_{st} + \varepsilon_{its}
\]</span>
<em>NJ</em> is a dummy equal to 1 if the observation is from NJ, and <span class="math inline">\(D\)</span> is a dummy equal to 1 if the observation is from November (the post period). This equation takes the following values, which I will list in order according to setting the dummies equal to one and/or zero:</p>
<ol style="list-style-type: decimal">
<li><p>PA Pre: <span class="math inline">\(\alpha\)</span></p></li>
<li><p>PA Post: <span class="math inline">\(\alpha + \lambda\)</span></p></li>
<li><p>NJ Pre: <span class="math inline">\(\alpha + \gamma\)</span></p></li>
<li><p>NJ Post: <span class="math inline">\(\alpha+\gamma +\lambda+\delta\)</span></p></li>
</ol>
<p>We can visualize the <span class="math inline">\(2\times 2\)</span> DD parameter in Figure <a href="ch8.html#fig:dd-diagram">9.2</a>.</p>
<div class="figure" style="text-align: center">
<span id="fig:dd-diagram"></span>
<img src="causal_inference_mixtape_files/figure-html/dd-diagram-1.png" alt="DD regression diagram" width="100%"><p class="caption">
Figure 9.2: DD regression diagram
</p>
</div>
<p>Now before we hammer the parallel trends assumption for the billionth time, I wanted to point something out here which is a bit subtle. But do you see the <span class="math inline">\(\delta\)</span> parameter floating in the air above the November line in the Figure 55? This is the difference between a counterfactual level of employment (the bottom black circle in November on the negatively sloped dashed line) and the actual level of employment (the above black circle in November on the positively sloped solid line) for New Jersey. It is therefore the ATT, because the ATT is equal to
<span class="math display">\[ 
\delta=E[Y^1_{NJ,\mathop{\mathrm{Post}}}] - E[Y^0_{NJ,\mathop{\mathrm{Post}}}]
\]</span>
wherein the first is observed (because <span class="math inline">\(Y=Y^1\)</span> in the post period) and the latter is unobserved for the same reason.</p>
<p>Now here’s the kicker: OLS will always estimate that <span class="math inline">\(\delta\)</span> line <em>even if the counterfactual slope had been something else.</em> That’s because OLS uses Pennsylvania’s change over time to project a point starting at New Jersey’s pre-treatment value. When OLS has filled in that missing amount, the parameter estimate is equal to the difference between the observed post-treatment value and that projected value based on the slope of Pennsylvania <em>regardless of whether that Pennsylvania slope was the correct benchmark for measuring New Jersey’s counterfactual slope.</em> OLS always estimates an effect size using the slope of the untreated group as the counterfactual, regardless of whether that slope is in fact the correct one.</p>
<p><em>But</em>, see what happens when Pennsylvania’s slope is equal to New Jersey’s counterfactual slope? Then that Pennsylvania slope used in regression will mechanically estimate the ATT. In other words, only when the Pennsylvania slope is the counterfactual slope for New Jersey will OLS coincidentally identify that true effect. Let’s see that here in Figure <a href="ch8.html#fig:dd-diagram2">9.3</a>.</p>
<div class="figure" style="text-align: center">
<span id="fig:dd-diagram2"></span>
<img src="causal_inference_mixtape_files/figure-html/dd-diagram2-1.png" alt="DD regression diagram without parallel trends" width="100%"><p class="caption">
Figure 9.3: DD regression diagram without parallel trends
</p>
</div>
<p>Notice the two <span class="math inline">\(\delta\)</span> listed: on the left is the true parameter <span class="math inline">\(\delta^{ATT}\)</span>. On the right is the one estimated by OLS, <span class="math inline">\(\widehat{\delta}^{OLS}\)</span>. The falling solid line is the observed Pennsylvania change, whereas the falling solid line labeled “observed NJ” is the change in observed employment for New Jersey between the two periods.</p>
<p>The true causal effect, <span class="math inline">\(\delta^{ATT}\)</span>, is the line from the “observed NJ” point and the “counterfactual NJ” point. But OLS does not estimate this line. Instead, OLS uses the falling Pennsylvania line to draw a parallel line from the February NJ point, which is shown in thin gray. And OLS simply estimates the vertical line from the observed NJ point to the post NJ point, which as can be seen underestimates the true causaleffect.</p>
<p>Here we see the importance of the parallel trends assumption. The only situation under which the OLS estimate equals the ATT is when the counterfactual NJ just coincidentally lined up with the gray OLS line, which is a line parallel to the slope of the Pennsylvania line. Herein lies the source of understandable skepticism of many who have been paying attention: why should we base estimation on this belief in a coincidence? After all, this is a counterfactual trend, and therefore it is unobserved, given it never occurred. Maybe the counterfactual would’ve been the gray line, but maybe it would’ve been some other unknown line. It could’ve been anything—we just don’t know.</p>
<p>This is why I like to tell people that the parallel trends assumption is actually just a restatement of the strict exogeneity assumption we discussed in the panel chapter. What we are saying when we appeal to parallel trends is that we have found a control group who approximates the traveling path of the treatment group <em>and</em> that the treatment is not endogenous. If it is endogenous, then parallel trends is always violated because in counterfactual the treatment group would’ve diverged anyway, regardless of the treatment.</p>
<p>Before we see the number of tests that economists have devised to provide some reasonable confidence in the belief of the parallel trends, I’d like to quickly talk about standard errors in a DD design.</p>
</div>
</div>
<div id="inference-1" class="section level2" number="9.3">
<h2>
<span class="header-section-number">9.3</span> Inference<a class="anchor" aria-label="anchor" href="#inference-1"><i class="fas fa-link"></i></a>
</h2>
<p>Many studies employing DD strategies use data from many years—not just one pre-treatment and one post-treatment period like <span class="citation">Card and Krueger (<a href="references.html#ref-Card1994" role="doc-biblioref">1994</a>)</span>. The variables of interest in many of these setups only vary at a group level, such as the state, and outcome variables are often serially correlated. In <span class="citation">Card and Krueger (<a href="references.html#ref-Card1994" role="doc-biblioref">1994</a>)</span>, it is very likely for instance that employment in each state is not only correlated within the state but also serially correlated. <span class="citation">Bertrand, Duflo, and Mullainathan (<a href="references.html#ref-Bertrand2004" role="doc-biblioref">2004</a>)</span> point out that the conventional standard errors often severely understate the standard deviation of the estimators, and so standard errors are biased downward, “too small,” and therefore overreject the null hypothesis. <span class="citation">Bertrand, Duflo, and Mullainathan (<a href="references.html#ref-Bertrand2004" role="doc-biblioref">2004</a>)</span> propose the following solutions:</p>
<ol style="list-style-type: decimal">
<li><p>Block bootstrapping standard errors.</p></li>
<li><p>Aggregating the data into one pre and one post period.</p></li>
<li><p>Clustering standard errors at the group level.</p></li>
</ol>
<div id="block-bootstrapping" class="section level3" number="9.3.1">
<h3>
<span class="header-section-number">9.3.1</span> Block bootstrapping<a class="anchor" aria-label="anchor" href="#block-bootstrapping"><i class="fas fa-link"></i></a>
</h3>
<p>If the block is a state, then you simply sample states with replacement for bootstrapping. Block bootstrap is straightforward and only requires a little programming involving loops and storing the estimates. As the mechanics are similar to that of randomization inference, I leave it to the reader to think about how they might tackle this.</p>
</div>
<div id="aggregation" class="section level3" number="9.3.2">
<h3>
<span class="header-section-number">9.3.2</span> Aggregation<a class="anchor" aria-label="anchor" href="#aggregation"><i class="fas fa-link"></i></a>
</h3>
<p>This approach ignores the time-series dimensions altogether, and if there is only one pre and post period and one untreated group, it’s as simple as it sounds. You simply average the groups into one pre and post period, and conduct difference-in-differences on those aggregated. But if you have differential timing, it’s a bit unusual because you will need to partial out state and year fixed effects before turning the analysis into an analysis involving residualization. Essentially, for those common situations where you have multiple treatment time periods (which we discuss later in greater detail), you would regress the outcome onto panel unit and time fixed effects and any covariates. You’d then obtain the residuals for only the treatment group. You then divide the residuals only into a pre and post period; you are essentially at this point ignoring the never-treated groups. And then you regress the residuals on the after dummy. It’s a strange procedure, and does not recover the original point estimate, so I focus instead on the third.</p>
</div>
<div id="clustering" class="section level3" number="9.3.3">
<h3>
<span class="header-section-number">9.3.3</span> Clustering<a class="anchor" aria-label="anchor" href="#clustering"><i class="fas fa-link"></i></a>
</h3>
<p>Correct treatment of standard errors sometimes makes the number of groups very small: in <span class="citation">Card and Krueger (<a href="references.html#ref-Card1994" role="doc-biblioref">1994</a>)</span>, the number of groups is only two. More common than not, researchers will use the third option (clustering the standard errors by group). I have only one time seen someone do all three of these; it’s rare though. Most people will present just the clustering solution—most likely because it requires minimal programming.</p>
<p>For clustering, there is no programming required, as most software packages allow for it already. You simply adjust standard errors by clustering at the group level, as we discussed in the earlier chapter, or the level of treatment. For state-level panels, that would mean clustering at the state level, which allows for arbitrary serial correlation in errors within a state over time. This is the most common solution employed.</p>
<p>Inference in a panel setting is independently an interesting area. When the number of clusters is small, then simple solutions like clustering the standard errors no longer suffice because of a growing false positive problem. In the extreme case with only one treatment unit, the over-rejection rate at a significance of 5% can be as high as 80% in simulations even using the wild bootstrap technique which has been suggested for smaller numbers of clusters <span class="citation">(Cameron, Gelbach, and Miller <a href="references.html#ref-Cameron2008" role="doc-biblioref">2008</a>; MacKinnon and Webb <a href="references.html#ref-MacKinnon2017" role="doc-biblioref">2017</a>)</span>. In such extreme cases where there is only one treatment group, I have preferred to use randomization inference following <span class="citation">Buchmueller, DiNardo, and Valletta (<a href="references.html#ref-Buchmueller" role="doc-biblioref">2011</a>)</span>.</p>
</div>
</div>
<div id="providing-evidence-for-parallel-trends-through-event-studies-and-parallel-leads" class="section level2" number="9.4">
<h2>
<span class="header-section-number">9.4</span> Providing Evidence for Parallel Trends Through Event Studies and Parallel Leads<a class="anchor" aria-label="anchor" href="#providing-evidence-for-parallel-trends-through-event-studies-and-parallel-leads"><i class="fas fa-link"></i></a>
</h2>
<div id="a-redundant-rant-about-parallel-pre-treatment-dd-coefficients-because-im-worried-one-was-not-enough" class="section level3" number="9.4.1">
<h3>
<span class="header-section-number">9.4.1</span> A redundant rant about parallel pre-treatment DD coefficients (because I’m worried one was not enough)<a class="anchor" aria-label="anchor" href="#a-redundant-rant-about-parallel-pre-treatment-dd-coefficients-because-im-worried-one-was-not-enough"><i class="fas fa-link"></i></a>
</h3>
<p>Given the critical importance of the parallel trends assumption in identifying causal effects with the DD design, and given that one of the observations needed to evaluate the parallel-trends assumption is not available to the researcher, one might throw up their hands in despair. But economists are stubborn, and they have spent decades devising ways to test whether it’s reasonable to believe in parallel trends. We now discuss the obligatory test for any DD design—the event study. Let’s rewrite the decomposition of the <span class="math inline">\(2 \times 2\)</span> DD again.
<span class="math display">\[\begin{align}
&amp;\widehat{\delta}^{2\times 2}_{kU} =
\underbrace{E\big[Y^1_{k} \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Post}}\big] - E\big[Y^0_{k} \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Post}}\big]}_{\text{ATT}} \\
&amp;+\Big[\underbrace{E\big[Y^0_k \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Post}}\big] - E\big[Y^0_k \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Pre}}\big] \Big]-\Big[E\big[Y^0_U \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Post}}\big] - E\big[Y_{U}^0 \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Pre}}\big] }_{\text{Non-parallel trends bias}} \Big]
\end{align}\]</span></p>
<p>We are interested in the first term, ATT, but it is contaminated by selection bias when the second term does not equal zero. Since evaluating the second term requires the counterfactual, <span class="math inline">\(E[Y^0_k \mathop{\mathrm{\,\vert\,}}Post]\)</span>, we are unable to do so directly. What economists typically do, instead, is compare placebo pre-treatment leads of the DD coefficient. If DD coefficients in the pre-treatment periods are statistically zero, then the difference-in-differences between treatment and control groups followed a similar trend prior to treatment. And here’s the rhetorical art of the design: <em>if</em> they had been similar before, <em>then</em> why wouldn’t they continue to be post-treatment?</p>
<p>But notice that this rhetoric is a kind of proof by assertion. Just because they were similar before does not logically require they be the same after. Assuming that the future is like the past is a form of the gambler’s fallacy called the “reverse position.” Just because a coin came up heads three times in a row does not mean it will come up heads the fourth time—not without further assumptions. Likewise, we are not obligated to believe that that counterfactual trends would be the same post-treatment because they had been similar pre-treatment without further assumptions about the predictive power of pre-treatment trends. But to make such assumptions is again to make untestable assumptions, and so we are back where we started.</p>
<p>One situation where parallel trends would be obviously violated is if the treatment itself was endogenous. In such a scenario, the assignment of the treatment status would be directly dependent on potential outcomes, and absent the treatment, potential outcomes would’ve changed regardless. Such traditional endogeneity requires more than merely lazy visualizations of parallel leads. While the test is important, technically pre-treatment similarities are neither necessary nor sufficient to guarantee parallel counterfactual trends <span class="citation">(Kahn-Lang and Lang <a href="references.html#ref-KahnLang2019" role="doc-biblioref">2019</a>)</span>. The assumption is not so easily proven. You can never stop being diligent in attempting to determine whether groups of units endogenously selected into treatment, the presence of omitted variable biases, various sources of selection bias, and open backdoor paths. When the structural error term in a dynamic regression model is uncorrelated with the treatment variable, you have strict exogeneity, and that is what gives you parallel trends, and that is what makes you able to make meaningful statements about your estimates.</p>
</div>
<div id="checking-the-pre-treatment-balance-between-treatment-and-control-groups" class="section level3" number="9.4.2">
<h3>
<span class="header-section-number">9.4.2</span> Checking the pre-treatment balance between treatment and control groups<a class="anchor" aria-label="anchor" href="#checking-the-pre-treatment-balance-between-treatment-and-control-groups"><i class="fas fa-link"></i></a>
</h3>
<p>Now with that pessimism out of the way, let’s discuss event study plots because though they are not direct tests of the parallel trends assumption, they have their place because they show that the two groups of units were comparable on dynamics in the pre-treatment period.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Financial economics also has a procedure called the event study &lt;span class="citation"&gt;(Binder &lt;a href="references.html#ref-Binder1998" role="doc-biblioref"&gt;1998&lt;/a&gt;)&lt;/span&gt;, but the way that event study is often used in contemporary causal inference is nothing more than a difference-in-differences design where, instead of a single post-treatment dummy, you saturate a model with leads and lags based on the timing of treatment.&lt;/p&gt;'><sup>144</sup></a> Such conditional independence concepts have been used profitably throughout this book, and we do so again now.</p>
<p>Authors have tried showing the differences between treatment and control groups a few different ways. One way is to simply show the raw data, which you can do if you have a set of groups who received the treatment at the same point in time. Then you would just visually inspect whether the pre-treatment dynamics of the treatment group differed from that of the control group units.</p>
<p>But what if you do not have a single treatment date? What if instead you have differential timing wherein groups of units adopt the treatment at different points? Then the concept of pre-treatment becomes complex. If New Jersey raised its minimum wage in 1992 and New York raised its minimum wage in 1994, but Pennsylvania never raised its minimum wage, the pre-treatment period is defined for New Jersey (1991) and New York (1993), but not Pennsylvania. Thus, how do we go about testing for pre-treatment differences in that case? People have done it in a variety of ways.</p>
<p>One possibility is to plot the raw data, year by year, and simply eyeball. You would compare the treatment group with the never-treated, for instance, which might require a lot of graphs and may also be awkward looking. <span class="citation">Cheng and Hoekstra (<a href="references.html#ref-Cheng2013" role="doc-biblioref">2013</a>)</span> took this route, and created a separate graph comparing treatment groups with an untreated group for each different year of treatment. The advantage is its transparent display of the raw unadjusted data. No funny business. The disadvantage of this several-fold. First, it may be cumbersome when the number of treatment groups is large, making it practically impossible. Second, it may not be beautiful. But third, this necessarily assumes that the only control group is the never-treated group, which in fact is not true given what <span class="citation">Goodman-Bacon (<a href="references.html#ref-Bacon2019" role="doc-biblioref">2019</a>)</span> has shown. Any DD is a combination of a comparison between the treatment and the never treated, an early treated compared to a late treated, and a late treated compared to an early treated. Thus only showing the comparison with the never treated is actually a misleading presentation of the underlying mechanization of identification using an twoway fixed-effects model with differential timing.</p>
<p><span class="citation">Anderson, Hansen, and Rees (<a href="references.html#ref-Anderson2013" role="doc-biblioref">2013</a>)</span> took an alternative, creative approach to show the comparability of states with legalized medical marijuana and states without. As I said, the concept of a pre-treatment period for a control state is undefined when pre-treatment is always in reference to a specific treatment date which varies across groups. So, the authors construct a recentered time path of traffic fatality rates for the control states by assigning random treatment dates to all control counties and then plotting the average traffic fatality rates for each group in years leading up to treatment and beyond. This approach has a few advantages. First, it plots the raw data, rather than coefficients from a regression (as we will see next). Second, it plots that data against controls. But its weakness is that technically, the control series is not in fact <em>true</em>. It is chosen so as to give a comparison, but when regressions are eventually run, it will not be based on this series. But the main main shortcoming is that technically it is not displaying any of the control groups that will be used for estimation <span class="citation">Goodman-Bacon (<a href="references.html#ref-Bacon2019" role="doc-biblioref">2019</a>)</span>. It is not displaying a comparison between the treated and the never treated; it is not a comparison between the early and late treated; it is not a comparison between the late and early treated. While a creative attempt to evaluate the pre-treatment differences in leads, it does not in fact technically show that.</p>
<p>The current way in which authors evaluate the pre-treatment dynamics between a treatment and control group with differential timing is to estimate a regression model that includes treatment leads and lags. I find that it is always useful to teach these concepts in the context of an actual paper, so let’s review an interesting working paper by <span class="citation">Miller et al. (<a href="references.html#ref-Miller2019" role="doc-biblioref">2019</a>)</span>.</p>
</div>
<div id="affordable-care-act-expanding-medicaid-and-population-mortality" class="section level3" number="9.4.3">
<h3>
<span class="header-section-number">9.4.3</span> Affordable Care Act, expanding Medicaid and population mortality<a class="anchor" aria-label="anchor" href="#affordable-care-act-expanding-medicaid-and-population-mortality"><i class="fas fa-link"></i></a>
</h3>
<p>A provocative new study by <span class="citation">Miller et al. (<a href="references.html#ref-Miller2019" role="doc-biblioref">2019</a>)</span> examined the expansion of Medicaid under the Affordable Care Act. They were primarily interested in the effect that this expansion had on population mortality. Earlier work had cast doubt on Medicaid’s effect on mortality <span class="citation">(Finkelstein et al. <a href="references.html#ref-Finkelstein2012" role="doc-biblioref">2012</a>; Baicker et al. <a href="references.html#ref-Baicker2013" role="doc-biblioref">2013</a>)</span>, so revisiting the question with a larger sample size had value.</p>
<p>Like Snow before them, the authors link data sets on deaths with a large-scale federal survey data, thus showing that shoe leather often goes hand in hand with good design. They use these data to evaluate the causal impact of Medicaid enrollment on mortality using a DD design. Their focus is on the near-elderly adults in states with and without the Affordable Care Act Medicaid expansions and they find a 0.13-percentage-point decline in annual mortality, which is a 9.3% reduction over the sample mean, as a result of the ACA expansion. This effect is a result of a reduction in disease-related deaths and gets larger over time. Medicaid, in this estimation, saved a non-trivial number of lives.</p>
<p>As with many contemporary DD designs, <span class="citation">Miller et al. (<a href="references.html#ref-Miller2019" role="doc-biblioref">2019</a>)</span> evaluate the pre-treatment leads instead of plotting the raw data by treatment and control. Post-estimation, they plotted regression coefficients with 95% confidence intervals on their treatment leads and lags. Including leads and lags into the DD model allowed the reader to check both the degree to which the post-treatment treatment effects were dynamic, and whether the two groups were comparable on outcome dynamics pre-treatment. Models like this one usually follow a form like:
<span class="math display">\[
  Y_{its} = \gamma_s + \lambda_t + \sum_{\tau=-q}^{-1}\gamma_{\tau}D_{s\tau} + \sum_{\tau=0}^m\delta_{\tau}D_{s\tau}+x_{ist}+ \varepsilon_{ist}
\]</span>
Treatment occurs in year 0. You include <span class="math inline">\(q\)</span> leads or anticipatory effects and <span class="math inline">\(m\)</span> lags or post-treatment effects.</p>
<p><span class="citation">Miller et al. (<a href="references.html#ref-Miller2019" role="doc-biblioref">2019</a>)</span> produce four event studies that when taken together tell the main parts of the story of their paper. This is, quite frankly, the art of the rhetoric of causal inference—visualization of key estimates, such as “first stages” as well as outcomes and placebos. The event study plots are so powerfully persuasive, they will make you a bit jealous, since oftentimes yours won’t be nearly so nice. Let’s look at the first three. State expansion of Medicaid under the Affordable Care Act increased Medicaid eligibility (Figure <a href="ch8.html#fig:miller1">9.4</a>), which is not altogether surprising. It also caused an increase in Medicaid coverage (Figure <a href="ch8.html#fig:miller2">9.5</a>), and as a consequence reduced the percentage of the uninsured population (Figure <a href="ch8.html#fig:miller3">9.6</a>). All three of these are simply showing that the ACA Medicaid expansion had “bite”—people enrolled and became insured.</p>
<div class="figure">
<span id="fig:miller1"></span>
<img src="graphics/elig.jpg" alt="Estimates of Medicaid expansion's effects on **eligibility** using leads and lags in an event study model. Reprint from @Miller2019." width="100%"><p class="caption">
Figure 9.4: Estimates of Medicaid expansion’s effects on <strong>eligibility</strong> using leads and lags in an event study model. Reprint from <span class="citation">Miller et al. (<a href="references.html#ref-Miller2019" role="doc-biblioref">2019</a>)</span>.
</p>
</div>
<div class="figure">
<span id="fig:miller2"></span>
<img src="graphics/mcaid.jpg" alt="Estimates of Medicaid expansion's effects on **coverage** using leads and lags in an event study model. Reprint from @Miller2019." width="100%"><p class="caption">
Figure 9.5: Estimates of Medicaid expansion’s effects on <strong>coverage</strong> using leads and lags in an event study model. Reprint from <span class="citation">Miller et al. (<a href="references.html#ref-Miller2019" role="doc-biblioref">2019</a>)</span>.
</p>
</div>
<div class="figure">
<span id="fig:miller3"></span>
<img src="graphics/unins.jpg" alt="Estimates of Medicaid expansion's effects on the **uninsured** state using leads and lags in an event study model. Reprint from @Miller2019." width="100%"><p class="caption">
Figure 9.6: Estimates of Medicaid expansion’s effects on the <strong>uninsured</strong> state using leads and lags in an event study model. Reprint from <span class="citation">Miller et al. (<a href="references.html#ref-Miller2019" role="doc-biblioref">2019</a>)</span>.
</p>
</div>
<p>There are several features of these event studies that should catch your eye. First, look at Figure <a href="ch8.html#fig:miller1">9.4</a>. The pre-treatment coefficients are nearly on the zero line itself. Not only are they nearly zero in their point estimate, but their standard errors are very small. This means these are very precisely estimated zero differences between individuals in the two groups of states prior to the expansion.</p>
<p>The second thing you see, though, is the elephant in the room. Post-treatment, the probability that someone becomes eligible for Medicaid immediately shoots up to 0.4 and while not as precise as the pre-treatment coefficients, the authors can rule out effects as low as 0.3 to 0.35. These are large increases in eligibility, and the fact that the coefficients prior to the treatment are basically zero, we find it easy to believe that the risen coefficients post-treatment were caused by the ACA’s expansion of Medicaid in states.</p>
<p>Of course, I would not be me if I did not say that <em>technically</em> the zeroes pre-treatment do not therefore mean that the post-treatment difference between counterfactual trends and observed trends are zero, but doesn’t it seem compelling when you see it? Doesn’t it compel you, just a little bit, that the changes in enrollment and insurance status were probably caused by the Medicaid expansion? I daresay a table of coefficients with leads, lags, and standard errors would probably not be as compelling even though it is the identical information. Also, it is only fair that the skeptic refuse these patterns with new evidence of what it is other than the Medicaid expansion. It is not enough to merely hand wave a criticism of omitted variable bias; the critic must be as engaged in this phenomenon as the authors themselves, which is how empiricists earn the right to critique someoneelse’s work.</p>
<p>Similar graphs are shown for coverage—prior to treatment, the two groups of individuals in treatment and control were similar with regards to their coverage and uninsured rate. But post-treatment, they diverge dramatically. Taken together, we have the “first stage,” which means we can see that the Medicaid expansion under the ACA had “bite.” Had the authors failed to find changes in eligibility, coverage, or uninsured rates, then any evidence from the secondary outcomes would have doubt built in. This is the reason it is so important that you examine the first stage (treatment’s effect on usage), as well as the second stage (treatment’s effect on the outcomes of interest).</p>
<p>But now let’s look at the main result—what effect did this have on population mortality itself? Recall, <span class="citation">Miller et al. (<a href="references.html#ref-Miller2019" role="doc-biblioref">2019</a>)</span> linked administrative death records with a large-scale federal survey. So they actually know who is on Medicaid and who is not. John Snow would be proud of this design, the meticulous collection of high-quality data, and all the shoeleather the authors showed.</p>
<p>This event study is presented in Figure <a href="ch8.html#fig:miller4">9.7</a>. A graph like this is the contemporary heart and soul of a DD design, both because it conveys key information regarding the comparability of the treatment and control groups in their dynamics just prior to treatment, and because such strong data visualization of main effects are powerfully persuasive. It’s quite clear looking at it that there was no difference between the trending tendencies of the two sets of state prior to treatment, making the subsequent divergence all the more striking.</p>
<div class="figure">
<span id="fig:miller4"></span>
<img src="graphics/mort.jpg" alt="@Miller2019 estimates of Medicaid expansion's effects on on **annual mortality** using leads and lags in an event study model" width="100%"><p class="caption">
Figure 9.7: <span class="citation">Miller et al. (<a href="references.html#ref-Miller2019" role="doc-biblioref">2019</a>)</span> estimates of Medicaid expansion’s effects on on <strong>annual mortality</strong> using leads and lags in an event study model
</p>
</div>
<p>But a picture like this is only as important as the thing that it is studying, and it is worth summarizing what <span class="citation">Miller et al. (<a href="references.html#ref-Miller2019" role="doc-biblioref">2019</a>)</span> have revealed here. The expansion of ACA Medicaid led to large swaths of people becoming eligible for Medicaid. In turn, they enrolled in Medicaid, which caused the uninsured rate to drop considerably. The authors then find amazingly using linked administrative data on death records that the expansion of ACA Medicaid led to a 0.13 percentage point decline in annual mortality, which is a 9.3 percent reduction over the mean. They go on to try and understand the mechanism (another key feature of this high-quality study) by which such amazing effects may have occurred, and conclude that Medicaid caused near-elderly individuals to receive treatment for life-threatening illnesses. I suspect we will be hearing about this study for many years.</p>
</div>
</div>
<div id="the-importance-of-placebos-in-dd" class="section level2" number="9.5">
<h2>
<span class="header-section-number">9.5</span> The Importance of Placebos in DD<a class="anchor" aria-label="anchor" href="#the-importance-of-placebos-in-dd"><i class="fas fa-link"></i></a>
</h2>
<p>There are several tests of the validity of a DD strategy. I have already discussed one—comparability between treatment and control groups on observable pre-treatment dynamics. Next, I will discuss other credible ways to evaluate whether estimated causal effects are credible by emphasizing the use of placebo falsification.</p>
<p>The idea of placebo falsification is simple. Say that you are finding some negative effect of the minimum wage on low-wage employment. Is the hypothesis true if we find evidence in favor? Maybe, maybe not. Maybe what would really help, though, is if you had in mind an alternative hypothesis and then tried to test that alternative hypothesis. If you cannot reject the null on the alternative hypothesis, then it provides some credibility to your original analysis. For instance, maybe you are picking up something spurious, like cyclical factors or other unobservables not easily captured by a time or state fixed effects. So what can you do?</p>
<p>One candidate placebo falsification might simply be to use data for an alternative type of worker whose wages would not be affected by the binding minimum wage. For instance, minimum wages affect employment and earnings of low-wage workers as these are the workers who literally are hired based on the market wage. Without some serious general equilibrium gymnastics, the minimum wage should not affect the employment of higher wage workers, because the minimum wage is not binding on high wage workers. Since high- and low-wage workers are employed in very different sectors, they are unlikely to be substitutes. This reasoning might lead us to consider the possibility that higher wage workers <em>might</em> function as a placebo.</p>
<p>There are two ways you can go about incorporating this idea into our analysis. Many people like to be straightforward and simply fit the same DD design using high wage employment as the outcome. If the coefficient on minimum wages is zero when using high wage worker employment as the outcome, but the coefficient on minimum wages for low wage workers is negative, then we have provided stronger evidence that complements the earlier analysis we did when on the low wage workers. But there is another method that uses the within-state placebo for identification called the difference-in-differences-in-differences (“triple differences”). I will discuss that design now.</p>
<div id="triple-differences" class="section level3" number="9.5.1">
<h3>
<span class="header-section-number">9.5.1</span> Triple differences<a class="anchor" aria-label="anchor" href="#triple-differences"><i class="fas fa-link"></i></a>
</h3>
<p>In our earlier analysis, we assumed that the only thing that happened to New Jersey after it passed the minimum wage was a common shock, <span class="math inline">\(T\)</span>, but what if there were state-specific time shocks such as <span class="math inline">\(NJ_t\)</span> or <span class="math inline">\(PA_t\)</span>? Then even DD cannot recover the treatment effect. Let’s see for ourselves using a modification of the simple minimum-wage table from earlier, which will include the within-state workers who hypothetically were untreated by the minimum wage—the “high-wage workers.”</p>
<div class="inline-table"><table class="table table-sm">
<caption>Triple differences design.</caption>
<thead><tr class="header">
<th align="left"><strong>States</strong></th>
<th align="left"><strong>Group</strong></th>
<th align="left"><strong>Period</strong></th>
<th align="left"><strong>Outcomes</strong></th>
<th align="left"><span class="math inline">\(D_1\)</span></th>
<th align="left"><span class="math inline">\(D_2\)</span></th>
<th align="left"><span class="math inline">\(D_3\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">NJ</td>
<td align="left">Low-wage workers</td>
<td align="left">After</td>
<td align="left"><span class="math inline">\(NJ_l+T+NJ_t+l_t+D\)</span></td>
<td align="left"><span class="math inline">\(T+NJ_t+\)</span></td>
<td align="left"><span class="math inline">\((l_t-h_t)+D\)</span></td>
<td align="left"><span class="math inline">\(D\)</span></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"></td>
<td align="left">Before</td>
<td align="left"><span class="math inline">\(NJ_l\)</span></td>
<td align="left"><span class="math inline">\(l_t+D\)</span></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left">High-wage workers</td>
<td align="left">After</td>
<td align="left"><span class="math inline">\(NJ_h+T+NJ_t+h_t\)</span></td>
<td align="left"><span class="math inline">\(T+NJ_t+h_t\)</span></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"></td>
<td align="left">Before</td>
<td align="left"><span class="math inline">\(NJ_h\)</span></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">PA</td>
<td align="left">Low-wage workers</td>
<td align="left">After</td>
<td align="left"><span class="math inline">\(PA_l+T+PA_t+l_t\)</span></td>
<td align="left"><span class="math inline">\(T+PA_t+l_t\)</span></td>
<td align="left"><span class="math inline">\(l_t-h_t\)</span></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"></td>
<td align="left">Before</td>
<td align="left"><span class="math inline">\(PA_l\)</span></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left">High-wage workers</td>
<td align="left">After</td>
<td align="left"><span class="math inline">\(PA_h+T+PA_t+h_t\)</span></td>
<td align="left"><span class="math inline">\(T+PA_t+h_t\)</span></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"></td>
<td align="left">Before</td>
<td align="left"><span class="math inline">\(PA_h\)</span></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table></div>
<p>Before the minimum-wage increase, low- and high-wage employment in New Jersey is determined by a group-specific New Jersey fixed effect (e.g., <span class="math inline">\(NJ_h\)</span>). The same is true for Pennsylvania. But after the minimum-wage hike, four things change in New Jersey: national trends cause employment to change by <span class="math inline">\(T\)</span>; New Jersey-specific time shocks change employment by <span class="math inline">\(NJ_t\)</span>; generic trends in low-wage workers change employment by <span class="math inline">\(l_t\)</span>; and the minimum-wage has some unknown effect <span class="math inline">\(D\)</span>. We have the same setup in Pennsylvania except there is no minimum wage, and Pennsylvania experiences its own time shocks.</p>
<p>Now if we take first differences for each set of states, we only eliminate the state fixed effect. The first difference estimate for New Jersey includes the minimum-wage effect, <span class="math inline">\(D\)</span>, but is also hopelessly contaminated by confounders (i.e., <span class="math inline">\(T+NJ_t+l_t\)</span>). So we take a second difference for each state, and doing so, we eliminate two of the confounders: <span class="math inline">\(T\)</span> disappears and <span class="math inline">\(NJ_t\)</span> disappears. But while this DD strategy has eliminated several confounders, it has also introduced new ones (i.e., <span class="math inline">\((l_t-h_t)\)</span>). This is the final source of selection bias that triple differences are designed to resolve. But, by differencing Pennsylvania’s second difference from New Jersey, the <span class="math inline">\((l_t-h_t)\)</span> is deleted and the minimum-wage effect is isolated.</p>
<p>Now, this solution is not without its own set of unique parallel-trends assumptions. But one of the parallel trends here I’d like you to see is the <span class="math inline">\(l_t-h_t\)</span> term. This parallel trends assumption states that the effect can be isolated if the gap between high- and low-wage employment would’ve evolved similarly in the treatment state counterfactual as it did in the historical control states. And we should probably provide some credible evidence that this is true with leads and lags in an event study as before.</p>
</div>
<div id="state-mandated-maternity-benefits" class="section level3" number="9.5.2">
<h3>
<span class="header-section-number">9.5.2</span> State-mandated maternity benefits<a class="anchor" aria-label="anchor" href="#state-mandated-maternity-benefits"><i class="fas fa-link"></i></a>
</h3>
<p>The triple differences design was first introduced by <span class="citation">Gruber (<a href="references.html#ref-Gruber1994" role="doc-biblioref">1994</a>)</span> in a study of state-level policies providing maternity benefits. I present his main results in Table @ref(tab:gruber_ddd). Notice that he uses as his treatment group married women of childbearing age in treatment and control states, but he also uses a set of placebo units (older women and single men 20–40) as within-state controls. He then goes through the differences in means to get the difference-in-differences for each set of groups, after which he calculates the DDD as the difference between these two difference-in-differences.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:gruber-ddd">Table 9.3: </span> DDD Estimates of the Impact of State Mandates on Hourly Wages</caption>
<thead><tr class="header">
<th align="left"><strong>Location/year</strong></th>
<th align="center"><strong>Pre-law</strong></th>
<th align="center"><strong>Post-law</strong></th>
<th align="center"><strong>Difference</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left"><em>A. Treatment: Married women, 20-40yo</em></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">Experimental states</td>
<td align="center">1.547</td>
<td align="center">1.513</td>
<td align="center"><span class="math inline">\(-0.034\)</span></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="center">(0.012)</td>
<td align="center">(0.012)</td>
<td align="center">(0.017)</td>
</tr>
<tr class="even">
<td align="left">Control states</td>
<td align="center">1.369</td>
<td align="center">1.397</td>
<td align="center">0.028</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="center">(0.010)</td>
<td align="center">(0.010)</td>
<td align="center">(0.014)</td>
</tr>
<tr class="even">
<td align="left">Difference</td>
<td align="center">0.178</td>
<td align="center">0.116</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="center">(0.016)</td>
<td align="center">(0.015)</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left"><em>Difference-in-difference</em></td>
<td align="center">−0.062</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="center">(0.022)</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left"><em>B. Control: Over 40 and Single Males 20-40</em></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">Experimental states</td>
<td align="center">1.759</td>
<td align="center">1.748</td>
<td align="center"><span class="math inline">\(-0.011\)</span></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center">(0.007)</td>
<td align="center">(0.007)</td>
<td align="center">(0.010)</td>
</tr>
<tr class="odd">
<td align="left">Control states</td>
<td align="center">1.630</td>
<td align="center">1.627</td>
<td align="center"><span class="math inline">\(-0.003\)</span></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center">(0.007)</td>
<td align="center">(0.007)</td>
<td align="center">(0.010)</td>
</tr>
<tr class="odd">
<td align="left">Difference</td>
<td align="center">1.09</td>
<td align="center">1.21</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center">(0.010)</td>
<td align="center">(0.010)</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left"><em>Difference-in-difference</em></td>
<td align="center">-0.008</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center">(0.014)</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left"><em>DDD</em></td>
<td align="center">-0.054</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center">(0.026)</td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table></div>
<p class="footnote">
Standard errors in parentheses.
</p>
<p>Ideally when you do a DDD estimate, the causal effect estimate will come from changes in the treatment units, not changes in the control units. That’s precisely what we see in <span class="citation">Gruber (<a href="references.html#ref-Gruber1994" role="doc-biblioref">1994</a>)</span>: the action comes from changes in the married women age 20–40 (<span class="math inline">\(-0.062\)</span>); there’s little movement among the placebo units <span class="math inline">\((-0.008)\)</span>. Thus when we calculate the DDD, we know that most of that calculation is coming from the first DD, and not so much from the second. We emphasize this because DDD is really just another falsification exercise, and just as we would expect no effect had we done the DD on this placebo group, we hope that our DDD estimate is also based on negligible effects among the control group.</p>
<p>What we have done up to now is show how to use sample analogs and simple differences in means to estimate the treatment effect using DDD. But we can also use regression to control for additional covariates that perhaps are necessary to close backdoor paths and so forth. What does that regression equation look like? Both the regression itself, and the data structure upon which the regression is based, are complicated because of the stacking of different groups and the sheer number of interactions involved. Estimating a DDD model requires estimating the following regression:
<span class="math display">\[\begin{align}
Y_{ijt} &amp;= \alpha + \psi X_{ijt} + \beta_1 \tau_t + \beta_2 \delta_j + \beta_3 D_i + \beta_4(\delta \times \tau)_{jt}
\\
&amp; +\beta_5(\tau \times D)_{ti} + \beta_6(\delta \times D)_{ij} + \beta_7(\delta \times \tau \times D)_{ijt}+ \varepsilon_{ijt}
\end{align}\]</span>
where the parameter of interest is <span class="math inline">\(\beta_7\)</span>. First, notice the additional subscript, <span class="math inline">\(j\)</span>. This <span class="math inline">\(j\)</span> indexes whether it’s the main category of interest (e.g., low-wage employment) or the within-state comparison group (e.g., high-wage employment). This requires a stacking of the data into a panel structure by group, as well as state. Second, the DDD model requires that you include all possible interactions across the group dummy <span class="math inline">\(\delta_j\)</span>, the post-treatment dummy <span class="math inline">\(\tau_t\)</span> and the treatment state dummy <span class="math inline">\(D_i\)</span>. The regression must include each dummy independently, each individual interaction, and the triple differences interaction. One of these will be dropped due to multicollinearity, but I include them in the equation so that you can visualize all the factors used in the product of these terms.</p>
</div>
<div id="abortion-legalization-and-long-term-gonorrhea-incidence" class="section level3" number="9.5.3">
<h3>
<span class="header-section-number">9.5.3</span> Abortion legalization and long-term gonorrhea incidence<a class="anchor" aria-label="anchor" href="#abortion-legalization-and-long-term-gonorrhea-incidence"><i class="fas fa-link"></i></a>
</h3>
<p>Now that we know a little about the DD design, it would probably be beneficial to replicate a paper. And since the DDD requires reshaping panel data multiple times, that makes working through a detailed replication even more important. The study we will be replicating is <span class="citation">Cunningham and Cornwell (<a href="references.html#ref-Cunningham2013" role="doc-biblioref">2013</a>)</span>, one of my first publications and the third chapter of my dissertation. Buckle up, as this will be a bit of a roller-coaster ride.</p>
<p><span class="citation">Gruber, Levine, and Staiger (<a href="references.html#ref-Gruber1999" role="doc-biblioref">1999</a>)</span> was the beginning of what would become a controversial literature in reproductive health. They wanted to know the characteristics of the marginal child aborted had that child reached their teen years. The authors found that the marginal counterfactual child aborted was 60% more likely to grow up in a single-parent household, 50% more likely to live in poverty, and 45% more likely to be a welfare recipient. Clearly there were strong selection effects related to early abortion whereby it selected on families with fewer resources.</p>
<p>Their finding about the marginal child led John Donohue and Steven Levitt to wonder if there might be far-reaching effects of abortion legalization given the strong selection associated with its usage in the early 1970s. In <span class="citation">Donohue and Levitt (<a href="references.html#ref-Donohue2001" role="doc-biblioref">2001</a>)</span>, the authors argued that they had found evidence that abortion legalization had also led to massive declines in crime rates. Their interpretation of the results was that abortion legalization had reduced crime by removing high-risk individuals from a birth cohort, and as that cohort aged, those counterfactual crimes disappeared. <span class="citation">Levitt (<a href="references.html#ref-Levitt2004" role="doc-biblioref">2004</a>)</span> attributed as much as 10% of the decline in crime between 1991 and 2001 to abortion legalization in the 1970s.</p>
<p>This study was, not surprisingly, incredibly controversial—some of it warranted but some unwarranted. For instance, some attacked the paper on ethical grounds and argued the paper was revitalizing the pseudoscience of eugenics. But Levitt was careful to focus only on the scientific issues and causal effects and did not offer policy advice based on his own private views, whatever those may be.</p>
<p>But some of the criticism the authors received was legitimate precisely because it centered on the research design and execution itself. <span class="citation">Joyce (<a href="references.html#ref-Joyce2004" role="doc-biblioref">2004</a>)</span>, <span class="citation">Joyce (<a href="references.html#ref-Joyce2009" role="doc-biblioref">2009</a>)</span>, and <span class="citation">Foote and Goetz (<a href="references.html#ref-Foote2008" role="doc-biblioref">2008</a>)</span> disputed the abortion-crime findings—some through replication exercises using different data, some with different research designs, and some through the discovery of key coding errors and erroneous variable construction.</p>
<p>One study in particular challenged the whole enterprise of estimating longrun improvements due to abortion legalization. For instance, Ted Joyce, an expert on reproductive health, cast doubt on the abortion-crime hypothesis using a DDD design <span class="citation">(Joyce <a href="references.html#ref-Joyce2009" role="doc-biblioref">2009</a>)</span>. In addition to challenging <span class="citation">Donohue and Levitt (<a href="references.html#ref-Donohue2001" role="doc-biblioref">2001</a>)</span>, Joyce also threw down a gauntlet. He argued that if abortion legalization had such extreme negative selection as claimed by by <span class="citation">Gruber, Levine, and Staiger (<a href="references.html#ref-Gruber1999" role="doc-biblioref">1999</a>)</span> and <span class="citation">Donohue and Levitt (<a href="references.html#ref-Donohue2001" role="doc-biblioref">2001</a>)</span>, then it shouldn’t show up just in crime. It should show up <em>everywhere</em>. Joyce writes:</p>
<blockquote>
<p>If abortion lowers homicide rates by 20–30%, then it is likely to have affected an entire spectrum of outcomes associated with well-being: infant health, child development, schooling, earnings and marital status. Similarly, the policy implications are broader than abortion. Other interventions that affect fertility control and that lead to fewer unwanted births—contraception or sexual abstinence—have huge potential payoffs. In short, a causal relationship between legalized abortion and crime has such significant ramifications for social policy and at the same time is so controversial, that further assessment of the identifying assumptions and their robustness to alternative strategies is warranted. (p.112)</p>
</blockquote>
<p><span class="citation">Cunningham and Cornwell (<a href="references.html#ref-Cunningham2013" role="doc-biblioref">2013</a>)</span> took up Joyce’s challenge. Our study estimated the effects of abortion legalization on long-term gonorrhea incidence. Why gonorrhea? For one, single-parent households are a risk factor that lead to earlier sexual activity and unprotected sex, and <span class="citation">Levine et al. (<a href="references.html#ref-Levine1999" role="doc-biblioref">1999</a>)</span> found that abortion legalization caused teen childbearing to fall by 12%. Other risky outcomes had been found by numerous authors. <span class="citation">Charles and Stephens (<a href="references.html#ref-Charles2006b" role="doc-biblioref">2006</a>)</span> reported that children exposed in utero to a legalized abortion regime were less likely to use illegal substances, which is correlated with risky sexual behavior.</p>
<p>My research design differed from <span class="citation">Donohue and Levitt (<a href="references.html#ref-Donohue2001" role="doc-biblioref">2001</a>)</span> in that they used state-level lagged values of an abortion ratio, whereas I used difference-in-differences. My design exploited the early repeal of abortion in five states in 1970 and compared those states to the states that were legalized under <em>Roe v. Wade</em> in 1973. To do this, I needed cohort-specific data on gonorrhea incidence by state and year, but as those data are not collected by the CDC, I had to settle for second best. That second best was the CDC’s gonorrhea data broken into five-year age categories (e.g., age 15–19, age 20–24). But this might still be useful because even with aggregate data, it might be possible to test the model I had in mind.</p>
<p>To understand this next part, which I consider the best part of my study, you must first accept a basic view of science that good theories make very specific falsifiable hypotheses. The more specific the hypothesis, the more convincing the theory, because if we find evidence exactly where the theory predicts, a Bayesian is likely to update her beliefs towards accepting the theory’s credibility. Let me illustrate what I mean with a brief detour involving Albert Einstein’s theory of relativity.</p>
<p>Einstein’s theory made several falsifiable hypotheses. One of them involved a precise prediction of the warping of light as it moved past a large object, such as a star. The problem was that testing this theory involved observing distance between stars at night and comparing it to measurements made during the day as the starlight moved past the sun. Problem was, the sun is too bright in the daytime to see the stars, so those critical measurements can’t be made. But Andrew Crommelin and Arthur Eddington realized the measurements could be made using an ingenious natural experiment. That natural experiment was an eclipse. They shipped telescopes to different parts of the world under the eclipse’s path so that they had multiple chances to make the measurements. They decided to measure the distances of a large cluster of stars passing by the sun when it was dark and then immediately during an eclipse (Figure <a href="ch8.html#fig:einstein">9.8</a>). That test was over a decade after Einstein’s work was first published <span class="citation">(Coles <a href="references.html#ref-Coles2019" role="doc-biblioref">2019</a>)</span>. Think about it for a second—Einstein’s theory by deduction is making predictions about phenomena that no one had ever really observed before. If this phenomena turned out to exist, then how couldn’t the Bayesian update her beliefs and accept that the theory was credible? Incredibly, Einstein was right—just as he predicted, the apparent position of these stars shifted when moving around the sun. Incredible!</p>
<div class="figure">
<span id="fig:einstein"></span>
<img src="graphics/scotteinstein.jpg" alt="Light bending around the sun, predicted by Einstein, and confirmed in a natural experiment involving an eclipse. Artwork by Seth Hahne (c) 2020." width="100%"><p class="caption">
Figure 9.8: Light bending around the sun, predicted by Einstein, and confirmed in a natural experiment involving an eclipse. Artwork by Seth Hahne (c) 2020.
</p>
</div>
<p>So what does that have to do with my study of abortion legalization and gonorrhea? The theory of abortion legalization having strong selection effects on cohorts makes very specific predictions about the shape of observed treatment effects. And if we found evidence for that shape, we’d be forced to take the theory seriously. So what what were these unusual yet testable predictions exactly?</p>
<p>The testable prediction from the staggered adoption of abortion legalization concerned the age-year-state profile of gonorrhea. The early repeal of abortion by five states three years before the rest of the country predicts lower incidence among 15- to 19-year-olds in the repeal states only during the 1986–1992 period relative to their <em>Roe</em> counterparts as the treated cohorts aged. That’s not really all that special a prediction though. Maybe something happens in those same states fifteen to nineteen years later that isn’t controlled for, for instance. What else?</p>
<p>The abortion legalization theory also predicted the <em>shape</em> of the observed treatment effects in this particular staggered adoption. Specifically, we should observe nonlinear treatment effects. These treatment effects should be increasingly negative from 1986 to 1989, plateau from 1989 to 1991, then gradually dissipate until 1992. In other words, the abortion legalization hypothesis predicts a parabolic treatment effect as treated cohorts move through the age distribution. All coefficients on the DD coefficients beyond 1992 should be zero and/or statistically insignificant.</p>
<p>I illustrate these predictions in Figure <a href="ch8.html#fig:cohort-id">9.9</a>. The top horizontal axis shows the year of the panel, the vertical axis shows the age in calendar years, and the cells show the cohort for a given person of a certain age in that given year. For instance, consider a 15-year-old in 1985. She was born in 1970. A 15-year-old in 1986 was born in 1971. A 15-year-old in 1987 was born in 1972, and so forth. I mark the cohorts who were treated by either repeal or <em>Roe</em> in different shades of gray.</p>
<div class="figure">
<span id="fig:cohort-id"></span>
<img src="graphics/cohort_id.jpg" alt="Theoretical predictions of abortion legalization on age profiles of gonorrhea incidence. Reprinted from @Cunningham2013." width="100%"><p class="caption">
Figure 9.9: Theoretical predictions of abortion legalization on age profiles of gonorrhea incidence. Reprinted from <span class="citation">Cunningham and Cornwell (<a href="references.html#ref-Cunningham2013" role="doc-biblioref">2013</a>)</span>.
</p>
</div>
<p>The theoretical predictions of the staggered rollout is shown at the bottom of Figure <a href="ch8.html#fig:cohort-id">9.9</a>. In 1986, only one cohort (the 1971 cohort) was treated and only in the repeal states. Therefore, we should see small declines in gonorrhea incidence among 15-year-olds in 1986 relative to <em>Roe</em> states. In 1987, two cohorts in our data are treated in the repeal states relative to <em>Roe</em>, so we should see larger effects in absolute value than we saw in 1986. But from 1988 to 1991, we should at most see only three <em>net</em> treated cohorts in the repeal states because starting in 1988, the <em>Roe</em> state cohorts enter and begin erasing those differences. Starting in 1992, the effects should get smaller in absolute value until 1992, beyond which there should be no difference between repeal and <em>Roe</em> states.</p>
<p>It is interesting that something so simple as a staggered policy rollout should provide two testable hypotheses that together can provide some insight into whether there is credibility to the negative selection in abortion legalization story. If we cannot find evidence for a negative parabola during this specific, narrow window, then the abortion legalization hypothesis has one more nail in its coffin.</p>
<p>A simple graphic for gonorrhea incidence among black 15- to 19-year-olds can help illustrate our findings. Remember, a picture is worth a thousand words, and whether it’s RDD or DD, it’s helpful to show pictures like these to prepare the reader for the table after table of regression coefficients. So notice what the raw data looks like in Figure <a href="ch8.html#fig:bf15-dd2">9.10</a>.</p>
<div class="figure">
<span id="fig:bf15-dd2"></span>
<img src="graphics/bf15_dd_raw.jpg" alt="Differences in black female gonorrhea incidence between repeal and *Roe* cohorts expressed as coefficient plots. Reprinted from @Cunningham2013." width="100%"><p class="caption">
Figure 9.10: Differences in black female gonorrhea incidence between repeal and <em>Roe</em> cohorts expressed as coefficient plots. Reprinted from <span class="citation">Cunningham and Cornwell (<a href="references.html#ref-Cunningham2013" role="doc-biblioref">2013</a>)</span>.
</p>
</div>
<p>First let’s look at the raw data. I have shaded the years corresponding to the window where we expect to find effects. In Figure <a href="ch8.html#fig:bf15-dd2">9.10</a>, we see the dynamics that will ultimately be picked up in the regression coefficients—the <em>Roe</em> states experienced a large and sustained gonorrhea epidemic that only waned once the treated cohorts emerged and overtook the entire data series.</p>
<p>Now let’s look at regression coefficients. Our estimating equation is as follows:
<span class="math display">\[ 
Y_{st} =\beta_1Repeals +\beta_2 DT_t +\beta_{3t} Repeal_s \times DT_t +X_{st} \psi+\alpha_{s}DS_s + \varepsilon_{st}
\]</span>
where <span class="math inline">\(Y\)</span> is the log number of new gonorrhea cases for 15- to 19-year-olds (per 100,000 of the population); Repeal<span class="math inline">\(_s\)</span> equals 1 if the state legalized abortion prior to <em>Roe</em>; <span class="math inline">\(DT_t\)</span> is a year dummy; <span class="math inline">\(DS_s\)</span> is a state dummy; <span class="math inline">\(t\)</span> is a time trend; <span class="math inline">\(X\)</span> is a matrix of covariates. In the paper, I sometimes included state-specific linear trends, but for this analysis, I present the simpler model. Finally, <span class="math inline">\(\varepsilon_{st}\)</span> is a structural error term assumed to be conditionally independent of the regressors. All standard errors, furthermore, were clustered at the state level allowing for arbitrary serial correlation.</p>
<p>I present the plotted coefficients from this regression for simplicity (and because pictures can be so powerful) in Figure <a href="ch8.html#fig:bf15-dd">9.11</a>. As can be seen in Figure <a href="ch8.html#fig:bf15-dd">9.11</a>, there is a negative effect during the window where <em>Roe</em> has not <em>fully</em> caught up, and that negative effect forms a parabola—just as our theory predicted.</p>
<div class="figure">
<span id="fig:bf15-dd"></span>
<img src="graphics/bf15_dd.jpg" alt="Coefficients and standard errors from DD regression equation" width="100%"><p class="caption">
Figure 9.11: Coefficients and standard errors from DD regression equation
</p>
</div>
<p>Now, a lot of people might be done, but if you are reading this book, you have revealed that you are not like a lot of people. <em>Credibly</em> identified causal effects requires both finding effects, and ruling out alternative explanations. This is necessary because the fundamental problem of causal inference keeps us blind to the truth. But one way to alleviate some of that doubt is through rigorous placebo analysis. Here I present evidence from a triple difference in which an untreated cohort is used as a within-state control.</p>
<p>We chose the 25- to 29-year-olds in the same states as within-state comparison groups instead of 20- to 24-year-olds after a lot of thought. Our reasoning was that we needed an age group that was close enough to capture common trends but far enough so as not to violate SUTVA. Since 15- to 19-year-olds were more likely than 25- to 29-year-olds to have sex with 20- to 24-year-olds, we chose the slightly older group as the within-stage control. But there’s a trade-off here. Choose a group too close and you get SUTVA violations. Choose a group too far and they no longer can credibly soak up the heterogeneities you’re worried about. The estimating equation for thisregression is:
<span class="math display">\[\begin{align}
Y_{ast} &amp;=\beta_1\text{~Repeal}_s + \beta_2DT_t + \beta_{3t}\text{~Repeal}_s\cdot DT_t +\delta_1\,DA + \delta_2\text{~Repeal}_s\cdot DA
\\
&amp;+ \delta_{3t}\,DA\cdot DT_t + \delta_{4t}\text{~Repeal}_s\cdot DA \cdot DT_t + X_{st}\xi + \alpha_{1s}DS_s + \alpha_{2s}DS_s\cdot DA
\\
&amp;+ \gamma_1\,t + \gamma_{2s}DS_s\cdot t + \gamma_3\,DA\cdot t+ \gamma_{4s}DS_s\cdot DA \cdot t + \epsilon_{ast},
\end{align}\]</span>
where the DDD parameter we are estimating is <span class="math inline">\(\delta_{4t}\)</span>—the full interaction. In case this wasn’t obvious, there are 7 separate dummies because our DDD parameter has all three interactions. Thus since there are eight combinations, we had to drop one as the omitted group, and control separately for the other seven. Here we present the table of coefficients. Note that the effect should be concentrated only among the treatment years as before, and second, it should form a parabola. The results are presented in Figure <a href="ch8.html#fig:bf-ddd">9.12</a>.</p>
<div class="figure">
<span id="fig:bf-ddd"></span>
<img src="graphics/bf15_ddd.jpg" alt="DDD Estimates of Abortion Legalization on 15-19yo Black Female Log Gonorrhea" width="100%"><p class="caption">
Figure 9.12: DDD Estimates of Abortion Legalization on 15-19yo Black Female Log Gonorrhea
</p>
</div>
<p>Here we see the prediction start to break down. Though there are negative effects for years 1986 to 1990, the 1991 and 1992 coefficients are positive, which is not consistent with our hypothesis. Furthermore, only the first four coefficients are statistically significant. Nevertheless, given the demanding nature of DDD, perhaps this is a small victory in favor of <span class="citation">Gruber, Levine, and Staiger (<a href="references.html#ref-Gruber1999" role="doc-biblioref">1999</a>)</span> and <span class="citation">Donohue and Levitt (<a href="references.html#ref-Donohue2001" role="doc-biblioref">2001</a>)</span>. Perhaps the theory that abortion legalization had strong selection effects on cohorts has some validity.</p>
<p>Putting aside whether you believe the results, it is still valuable to replicate the results based on this staggered design. Recall that I said the DDD design requires stacking the data, which may seem like a bit of a black box, so I’d like to examine these data now.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;In the original &lt;span class="citation"&gt;Cunningham and Cornwell (&lt;a href="references.html#ref-Cunningham2013" role="doc-biblioref"&gt;2013&lt;/a&gt;)&lt;/span&gt;, we estimated models with multi-way clustering correction, but the package for this in Stata is no longer supported. Therefore, we will estimate the same models as in &lt;span class="citation"&gt;Cunningham and Cornwell (&lt;a href="references.html#ref-Cunningham2013" role="doc-biblioref"&gt;2013&lt;/a&gt;)&lt;/span&gt; using cluster robust standard errors. In all prior analysis, I clustered the standard errors at the state level so as to maintain consistency with this code.&lt;/p&gt;'><sup>145</sup></a></p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/abortion_dd.do"><code>abortion_dd.do</code></a></em></p>
<div class="sourceCode" id="cb85"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb85-1"><a href="ch8.html#cb85-1" aria-hidden="true"></a>* DD estimate <span class="kw">of</span> 15-19 <span class="fu">year</span> olds <span class="kw">in</span> repeal states vs Roe states </span>
<span id="cb85-2"><a href="ch8.html#cb85-2" aria-hidden="true"></a><span class="kw">use</span> https:<span class="co">//github.com/scunning1975/mixtape/raw/master/abortion.dta, clear</span></span>
<span id="cb85-3"><a href="ch8.html#cb85-3" aria-hidden="true"></a><span class="kw">xi</span>: <span class="kw">reg</span> lnr i.repeal*i.<span class="fu">year</span> i.fip acc <span class="kw">ir</span> <span class="kw">pi</span> alcohol crack poverty income ur <span class="kw">if</span> bf15==1 [<span class="kw">aweight</span>=totpop], <span class="kw">cluster</span>(fip)</span>
<span id="cb85-4"><a href="ch8.html#cb85-4" aria-hidden="true"></a></span>
<span id="cb85-5"><a href="ch8.html#cb85-5" aria-hidden="true"></a>* <span class="kw">ssc</span> install parmest, <span class="kw">replace</span></span>
<span id="cb85-6"><a href="ch8.html#cb85-6" aria-hidden="true"></a>    </span>
<span id="cb85-7"><a href="ch8.html#cb85-7" aria-hidden="true"></a>parmest, <span class="kw">label</span> <span class="kw">for</span>(estimate min95 max95 %8.2f) li(parm <span class="kw">label</span> estimate min95 max95) <span class="kw">saving</span>(bf15_DD.dta, <span class="kw">replace</span>)</span>
<span id="cb85-8"><a href="ch8.html#cb85-8" aria-hidden="true"></a></span>
<span id="cb85-9"><a href="ch8.html#cb85-9" aria-hidden="true"></a><span class="kw">use</span> ./bf15_DD.dta, <span class="kw">replace</span></span>
<span id="cb85-10"><a href="ch8.html#cb85-10" aria-hidden="true"></a></span>
<span id="cb85-11"><a href="ch8.html#cb85-11" aria-hidden="true"></a><span class="kw">keep</span> <span class="kw">in</span> 17/31</span>
<span id="cb85-12"><a href="ch8.html#cb85-12" aria-hidden="true"></a></span>
<span id="cb85-13"><a href="ch8.html#cb85-13" aria-hidden="true"></a><span class="kw">gen</span>     <span class="fu">year</span>=1986 <span class="kw">in</span> 1</span>
<span id="cb85-14"><a href="ch8.html#cb85-14" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">year</span>=1987 <span class="kw">in</span> 2</span>
<span id="cb85-15"><a href="ch8.html#cb85-15" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">year</span>=1988 <span class="kw">in</span> 3</span>
<span id="cb85-16"><a href="ch8.html#cb85-16" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">year</span>=1989 <span class="kw">in</span> 4</span>
<span id="cb85-17"><a href="ch8.html#cb85-17" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">year</span>=1990 <span class="kw">in</span> 5</span>
<span id="cb85-18"><a href="ch8.html#cb85-18" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">year</span>=1991 <span class="kw">in</span> 6</span>
<span id="cb85-19"><a href="ch8.html#cb85-19" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">year</span>=1992 <span class="kw">in</span> 7</span>
<span id="cb85-20"><a href="ch8.html#cb85-20" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">year</span>=1993 <span class="kw">in</span> 8</span>
<span id="cb85-21"><a href="ch8.html#cb85-21" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">year</span>=1994 <span class="kw">in</span> 9</span>
<span id="cb85-22"><a href="ch8.html#cb85-22" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">year</span>=1995 <span class="kw">in</span> 10</span>
<span id="cb85-23"><a href="ch8.html#cb85-23" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">year</span>=1996 <span class="kw">in</span> 11</span>
<span id="cb85-24"><a href="ch8.html#cb85-24" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">year</span>=1997 <span class="kw">in</span> 12</span>
<span id="cb85-25"><a href="ch8.html#cb85-25" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">year</span>=1998 <span class="kw">in</span> 13</span>
<span id="cb85-26"><a href="ch8.html#cb85-26" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">year</span>=1999 <span class="kw">in</span> 14</span>
<span id="cb85-27"><a href="ch8.html#cb85-27" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">year</span>=2000 <span class="kw">in</span> 15</span>
<span id="cb85-28"><a href="ch8.html#cb85-28" aria-hidden="true"></a></span>
<span id="cb85-29"><a href="ch8.html#cb85-29" aria-hidden="true"></a><span class="kw">sort</span> <span class="fu">year</span></span>
<span id="cb85-30"><a href="ch8.html#cb85-30" aria-hidden="true"></a></span>
<span id="cb85-31"><a href="ch8.html#cb85-31" aria-hidden="true"></a><span class="kw">twoway</span> (<span class="kw">scatter</span> estimate <span class="fu">year</span>, <span class="bn">mlabel</span>(<span class="fu">year</span>) <span class="bn">mlabsize</span>(vsmall) msize(tiny)) (rcap min95 max95 <span class="fu">year</span>, msize(vsmall)), <span class="bn">ytitle</span>(Repeal x <span class="fu">year</span> estimated coefficient) <span class="bn">yscale</span>(titlegap(2)) <span class="kw">yline</span>(0, lwidth(vvvthin) lcolor(<span class="bn">black</span>)) <span class="bn">xtitle</span>(Year) <span class="bn">xline</span>(1986 1987 1988 1989 1990 1991 1992, lwidth(vvvthick) lpattern(solid) lcolor(<span class="bn">ltblue</span>)) <span class="bn">xscale</span>(titlegap(2)) <span class="bn">title</span>(Estimated effect <span class="kw">of</span> abortion legalization <span class="kw">on</span> gonorrhea) <span class="bn">subtitle</span>(Black females 15-19 <span class="fu">year</span>-olds) <span class="kw">note</span>(Whisker plots are estimated coefficients <span class="kw">of</span> DD estimator from Column b <span class="kw">of</span> Table 2.) <span class="bn">legend</span>(<span class="kw">off</span>)</span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/abortion_dd.R"><code>abortion_dd.R</code></a></em></p>
<div class="sourceCode" id="cb86"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">#-- DD estimate of 15-19 year olds in repeal states vs Roe states</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://haven.tidyverse.org">haven</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">estimatr</span><span class="op">)</span>

<span class="va">read_data</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">df</span><span class="op">)</span>
<span class="op">{</span>
  <span class="va">full_path</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"https://raw.github.com/scunning1975/mixtape/master/"</span>, 
                     <span class="va">df</span>, sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span>
  <span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/read_dta.html">read_dta</a></span><span class="op">(</span><span class="va">full_path</span><span class="op">)</span>
  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">df</span><span class="op">)</span>
<span class="op">}</span>

<span class="va">abortion</span> <span class="op">&lt;-</span> <span class="fu">read_data</span><span class="op">(</span><span class="st">"abortion.dta"</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>
    repeal <span class="op">=</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="va">repeal</span><span class="op">)</span>,
    year   <span class="op">=</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="va">year</span><span class="op">)</span>,
    fip    <span class="op">=</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="va">fip</span><span class="op">)</span>,
    fa     <span class="op">=</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="va">fa</span><span class="op">)</span>,
  <span class="op">)</span>

<span class="va">reg</span> <span class="op">&lt;-</span> <span class="va">abortion</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">bf15</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu">lm_robust</span><span class="op">(</span><span class="va">lnr</span> <span class="op">~</span> <span class="va">repeal</span><span class="op">*</span><span class="va">year</span> <span class="op">+</span> <span class="va">fip</span> <span class="op">+</span> <span class="va">acc</span> <span class="op">+</span> <span class="va">ir</span> <span class="op">+</span> <span class="va">pi</span> <span class="op">+</span> <span class="va">alcohol</span><span class="op">+</span> <span class="va">crack</span> <span class="op">+</span> <span class="va">poverty</span><span class="op">+</span> <span class="va">income</span><span class="op">+</span> <span class="va">ur</span>,
            data <span class="op">=</span> <span class="va">.</span>, weights <span class="op">=</span> <span class="va">totpop</span>, clusters <span class="op">=</span> <span class="va">fip</span><span class="op">)</span>

<span class="va">abortion_plot</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble</a></span><span class="op">(</span>
  sd <span class="op">=</span> <span class="va">reg</span><span class="op">$</span><span class="va">std.error</span><span class="op">[</span><span class="op">-</span><span class="fl">1</span><span class="op">:</span><span class="op">-</span><span class="fl">75</span><span class="op">]</span>,
  mean <span class="op">=</span> <span class="va">reg</span><span class="op">$</span><span class="va">coefficients</span><span class="op">[</span><span class="op">-</span><span class="fl">1</span><span class="op">:</span><span class="op">-</span><span class="fl">75</span><span class="op">]</span>,
  year <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1986</span><span class="op">:</span><span class="fl">2000</span><span class="op">)</span><span class="op">)</span>

<span class="va">abortion_plot</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">year</span>, y <span class="op">=</span> <span class="va">mean</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_tile.html">geom_rect</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>xmin<span class="op">=</span><span class="fl">1986</span>, xmax<span class="op">=</span><span class="fl">1992</span>, ymin<span class="op">=</span><span class="op">-</span><span class="cn">Inf</span>, ymax<span class="op">=</span><span class="cn">Inf</span><span class="op">)</span>, fill <span class="op">=</span> <span class="st">"cyan"</span>, alpha <span class="op">=</span> <span class="fl">0.01</span><span class="op">)</span><span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span><span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_text.html">geom_text</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>label <span class="op">=</span> <span class="va">year</span><span class="op">)</span>, hjust<span class="op">=</span><span class="op">-</span><span class="fl">0.002</span>, vjust <span class="op">=</span> <span class="op">-</span><span class="fl">0.03</span><span class="op">)</span><span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_hline</a></span><span class="op">(</span>yintercept <span class="op">=</span> <span class="fl">0</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_linerange.html">geom_errorbar</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>ymin <span class="op">=</span> <span class="va">mean</span> <span class="op">-</span> <span class="va">sd</span><span class="op">*</span><span class="fl">1.96</span>, ymax <span class="op">=</span> <span class="va">mean</span> <span class="op">+</span> <span class="va">sd</span><span class="op">*</span><span class="fl">1.96</span><span class="op">)</span>, width <span class="op">=</span> <span class="fl">0.2</span>,
                position <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/position_dodge.html">position_dodge</a></span><span class="op">(</span><span class="fl">0.05</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p>The second line estimates the regression equation. The dynamic DD coefficients are captured by the repeal-year interactions. These are the coefficients we used to create box plots in Figure <a href="ch8.html#fig:bf15-dd">9.11</a>. You can check these yourself.</p>
<p>Note, for simplicity, I only estimated this for the black females (<code>bf15==1</code>) but you could estimate for the black males (<code>bm15==1</code>), white females (<code>wf15==1</code>), or white males (<code>wm15==1</code>). We do all four in the paper, but here we only focus on the black females aged 15–19 because the purpose of this section is to help you understand the estimation. I encourage you to play around with this model to see how robust the effects are in your mind using only this linear estimation.</p>
<p>But now I want to show you the code for estimating a triple difference model. Some reshaping had to be done behind the scenes for this data structure, but it would take too long to post that here. For now, I will simply produce the commands that produce the black female result, and I encourage you to explore the panel data structure so as to familiarize yourself with the way in which the data are organized.</p>
<p>Notice that some of these were already interactions (e.g., <code>yr</code>), which was my way to compactly include all of the interactions. I did this primarily to give myself more control over what variables I was using. But I encourage you to study the data structure itself so that when you need to estimate your own DDD, you’ll have a good handle on what form the data must be in in order to execute so many interactions.</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/abortion_ddd.do"><code>abortion_ddd.do</code></a></em></p>
<div class="sourceCode" id="cb87"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb87-1"><a href="ch8.html#cb87-1" aria-hidden="true"></a><span class="kw">use</span> https:<span class="co">//github.com/scunning1975/mixtape/raw/master/abortion.dta, clear</span></span>
<span id="cb87-2"><a href="ch8.html#cb87-2" aria-hidden="true"></a></span>
<span id="cb87-3"><a href="ch8.html#cb87-3" aria-hidden="true"></a>* DDD estimate <span class="kw">for</span> 15-19 <span class="fu">year</span> olds vs. 20-24 <span class="fu">year</span> olds <span class="kw">in</span> repeal vs Roe states</span>
<span id="cb87-4"><a href="ch8.html#cb87-4" aria-hidden="true"></a><span class="kw">gen</span> yr=(repeal) &amp; (younger==1)</span>
<span id="cb87-5"><a href="ch8.html#cb87-5" aria-hidden="true"></a><span class="kw">gen</span> wm=(wht==1) &amp; (male==1)</span>
<span id="cb87-6"><a href="ch8.html#cb87-6" aria-hidden="true"></a><span class="kw">gen</span> wf=(wht==1) &amp; (male==0)</span>
<span id="cb87-7"><a href="ch8.html#cb87-7" aria-hidden="true"></a><span class="kw">gen</span> bm=(wht==0) &amp; (male==1)</span>
<span id="cb87-8"><a href="ch8.html#cb87-8" aria-hidden="true"></a><span class="kw">gen</span> bf=(wht==0) &amp; (male==0)</span>
<span id="cb87-9"><a href="ch8.html#cb87-9" aria-hidden="true"></a><span class="fu">char</span> <span class="fu">year</span>[omit] 1985</span>
<span id="cb87-10"><a href="ch8.html#cb87-10" aria-hidden="true"></a><span class="fu">char</span> repeal[omit] 0</span>
<span id="cb87-11"><a href="ch8.html#cb87-11" aria-hidden="true"></a><span class="fu">char</span> younger[omit] 0</span>
<span id="cb87-12"><a href="ch8.html#cb87-12" aria-hidden="true"></a><span class="fu">char</span> fip[omit] 1</span>
<span id="cb87-13"><a href="ch8.html#cb87-13" aria-hidden="true"></a><span class="fu">char</span> fa[omit] 0</span>
<span id="cb87-14"><a href="ch8.html#cb87-14" aria-hidden="true"></a><span class="fu">char</span> yr[omit] 0 </span>
<span id="cb87-15"><a href="ch8.html#cb87-15" aria-hidden="true"></a><span class="kw">xi</span>: <span class="kw">reg</span> lnr i.repeal*i.<span class="fu">year</span> i.younger*i.repeal i.younger*i.<span class="fu">year</span> i.yr*i.<span class="fu">year</span> i.fip*t acc <span class="kw">pi</span> <span class="kw">ir</span> alcohol crack  poverty income ur <span class="kw">if</span> bf==1 &amp; (age==15 | age==25) [<span class="kw">aweight</span>=totpop], <span class="kw">cluster</span>(fip)</span>
<span id="cb87-16"><a href="ch8.html#cb87-16" aria-hidden="true"></a>    </span>
<span id="cb87-17"><a href="ch8.html#cb87-17" aria-hidden="true"></a>parmest, <span class="kw">label</span> <span class="kw">for</span>(estimate min95 max95 %8.2f) li(parm <span class="kw">label</span> estimate min95 max95) <span class="kw">saving</span>(bf15_DDD.dta, <span class="kw">replace</span>)</span>
<span id="cb87-18"><a href="ch8.html#cb87-18" aria-hidden="true"></a></span>
<span id="cb87-19"><a href="ch8.html#cb87-19" aria-hidden="true"></a><span class="kw">use</span> ./bf15_DDD.dta, <span class="kw">replace</span></span>
<span id="cb87-20"><a href="ch8.html#cb87-20" aria-hidden="true"></a></span>
<span id="cb87-21"><a href="ch8.html#cb87-21" aria-hidden="true"></a><span class="kw">keep</span> <span class="kw">in</span> 82/96</span>
<span id="cb87-22"><a href="ch8.html#cb87-22" aria-hidden="true"></a></span>
<span id="cb87-23"><a href="ch8.html#cb87-23" aria-hidden="true"></a><span class="kw">gen</span>     <span class="fu">year</span>=1986 <span class="kw">in</span> 1</span>
<span id="cb87-24"><a href="ch8.html#cb87-24" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">year</span>=1987 <span class="kw">in</span> 2</span>
<span id="cb87-25"><a href="ch8.html#cb87-25" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">year</span>=1988 <span class="kw">in</span> 3</span>
<span id="cb87-26"><a href="ch8.html#cb87-26" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">year</span>=1989 <span class="kw">in</span> 4</span>
<span id="cb87-27"><a href="ch8.html#cb87-27" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">year</span>=1990 <span class="kw">in</span> 5</span>
<span id="cb87-28"><a href="ch8.html#cb87-28" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">year</span>=1991 <span class="kw">in</span> 6</span>
<span id="cb87-29"><a href="ch8.html#cb87-29" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">year</span>=1992 <span class="kw">in</span> 7</span>
<span id="cb87-30"><a href="ch8.html#cb87-30" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">year</span>=1993 <span class="kw">in</span> 8</span>
<span id="cb87-31"><a href="ch8.html#cb87-31" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">year</span>=1994 <span class="kw">in</span> 9</span>
<span id="cb87-32"><a href="ch8.html#cb87-32" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">year</span>=1995 <span class="kw">in</span> 10</span>
<span id="cb87-33"><a href="ch8.html#cb87-33" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">year</span>=1996 <span class="kw">in</span> 11</span>
<span id="cb87-34"><a href="ch8.html#cb87-34" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">year</span>=1997 <span class="kw">in</span> 12</span>
<span id="cb87-35"><a href="ch8.html#cb87-35" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">year</span>=1998 <span class="kw">in</span> 13</span>
<span id="cb87-36"><a href="ch8.html#cb87-36" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">year</span>=1999 <span class="kw">in</span> 14</span>
<span id="cb87-37"><a href="ch8.html#cb87-37" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">year</span>=2000 <span class="kw">in</span> 15</span>
<span id="cb87-38"><a href="ch8.html#cb87-38" aria-hidden="true"></a></span>
<span id="cb87-39"><a href="ch8.html#cb87-39" aria-hidden="true"></a><span class="kw">sort</span> <span class="fu">year</span></span>
<span id="cb87-40"><a href="ch8.html#cb87-40" aria-hidden="true"></a></span>
<span id="cb87-41"><a href="ch8.html#cb87-41" aria-hidden="true"></a><span class="kw">twoway</span> (<span class="kw">scatter</span> estimate <span class="fu">year</span>, <span class="bn">mlabel</span>(<span class="fu">year</span>) <span class="bn">mlabsize</span>(vsmall) msize(tiny)) (rcap min95 max95 <span class="fu">year</span>, msize(vsmall)), <span class="bn">ytitle</span>(Repeal x 20-24yo x <span class="fu">year</span> estimated coefficient) <span class="bn">yscale</span>(titlegap(2)) <span class="kw">yline</span>(0, lwidth(vvvthin) lcolor(<span class="bn">black</span>)) <span class="bn">xtitle</span>(Year) <span class="bn">xline</span>(1986 1987 1988 1989 1990 1991 1992, lwidth(vvvthick) lpattern(solid) lcolor(<span class="bn">ltblue</span>)) <span class="bn">xscale</span>(titlegap(2)) <span class="bn">title</span>(Estimated effect <span class="kw">of</span> abortion legalization <span class="kw">on</span> gonorrhea) <span class="bn">subtitle</span>(Black females 15-19 <span class="fu">year</span>-olds) <span class="kw">note</span>(Whisker plots are estimated coefficients <span class="kw">of</span> DDD estimator from Column b <span class="kw">of</span> Table 2.) <span class="bn">legend</span>(<span class="kw">off</span>)</span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/abortion_ddd.R"><code>abortion_ddd.R</code></a></em></p>
<div class="sourceCode" id="cb88"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://haven.tidyverse.org">haven</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">estimatr</span><span class="op">)</span>

<span class="va">read_data</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">df</span><span class="op">)</span>
<span class="op">{</span>
  <span class="va">full_path</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"https://raw.github.com/scunning1975/mixtape/master/"</span>, 
                     <span class="va">df</span>, sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span>
  <span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/read_dta.html">read_dta</a></span><span class="op">(</span><span class="va">full_path</span><span class="op">)</span>
  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">df</span><span class="op">)</span>
<span class="op">}</span>

<span class="va">abortion</span> <span class="op">&lt;-</span> <span class="fu">read_data</span><span class="op">(</span><span class="st">"abortion.dta"</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>
    repeal  <span class="op">=</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="va">repeal</span><span class="op">)</span>,
    year    <span class="op">=</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="va">year</span><span class="op">)</span>,
    fip     <span class="op">=</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="va">fip</span><span class="op">)</span>,
    fa      <span class="op">=</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="va">fa</span><span class="op">)</span>,
    younger <span class="op">=</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="va">younger</span><span class="op">)</span>,
    yr      <span class="op">=</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">repeal</span> <span class="op">==</span> <span class="fl">1</span> <span class="op">&amp;</span> <span class="va">younger</span> <span class="op">==</span> <span class="fl">1</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span><span class="op">)</span>,
    wm      <span class="op">=</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">wht</span> <span class="op">==</span> <span class="fl">1</span> <span class="op">&amp;</span> <span class="va">male</span> <span class="op">==</span> <span class="fl">1</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span><span class="op">)</span>,
    wf      <span class="op">=</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">wht</span> <span class="op">==</span> <span class="fl">1</span> <span class="op">&amp;</span> <span class="va">male</span> <span class="op">==</span> <span class="fl">0</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span><span class="op">)</span>,
    bm      <span class="op">=</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">wht</span> <span class="op">==</span> <span class="fl">0</span> <span class="op">&amp;</span> <span class="va">male</span> <span class="op">==</span> <span class="fl">1</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span><span class="op">)</span>,
    bf      <span class="op">=</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">wht</span> <span class="op">==</span> <span class="fl">0</span> <span class="op">&amp;</span> <span class="va">male</span> <span class="op">==</span> <span class="fl">0</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span><span class="op">)</span>
  <span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">bf</span> <span class="op">==</span> <span class="fl">1</span> <span class="op">&amp;</span> <span class="op">(</span><span class="va">age</span> <span class="op">==</span> <span class="fl">15</span> <span class="op">|</span> <span class="va">age</span> <span class="op">==</span> <span class="fl">25</span><span class="op">)</span><span class="op">)</span>

<span class="va">regddd</span> <span class="op">&lt;-</span> <span class="fu">lm_robust</span><span class="op">(</span><span class="va">lnr</span> <span class="op">~</span> <span class="va">repeal</span><span class="op">*</span><span class="va">year</span> <span class="op">+</span> <span class="va">younger</span><span class="op">*</span><span class="va">repeal</span> <span class="op">+</span> <span class="va">younger</span><span class="op">*</span><span class="va">year</span> <span class="op">+</span> <span class="va">yr</span><span class="op">*</span><span class="va">year</span> <span class="op">+</span> <span class="va">fip</span><span class="op">*</span><span class="va">t</span> <span class="op">+</span> <span class="va">acc</span> <span class="op">+</span> <span class="va">ir</span> <span class="op">+</span> <span class="va">pi</span> <span class="op">+</span> <span class="va">alcohol</span> <span class="op">+</span> <span class="va">crack</span> <span class="op">+</span> <span class="va">poverty</span> <span class="op">+</span> <span class="va">income</span> <span class="op">+</span> <span class="va">ur</span>,
                    data <span class="op">=</span> <span class="va">abortion</span>, weights <span class="op">=</span> <span class="va">totpop</span>, clusters <span class="op">=</span> <span class="va">fip</span><span class="op">)</span>

<span class="va">abortion_plot</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble</a></span><span class="op">(</span>
  sd <span class="op">=</span> <span class="va">regddd</span><span class="op">$</span><span class="va">std.error</span><span class="op">[</span><span class="fl">110</span><span class="op">:</span><span class="fl">124</span><span class="op">]</span>,
  mean <span class="op">=</span> <span class="va">regddd</span><span class="op">$</span><span class="va">coefficients</span><span class="op">[</span><span class="fl">110</span><span class="op">:</span><span class="fl">124</span><span class="op">]</span>,
  year <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1986</span><span class="op">:</span><span class="fl">2000</span><span class="op">)</span><span class="op">)</span>

<span class="va">abortion_plot</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">year</span>, y <span class="op">=</span> <span class="va">mean</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_tile.html">geom_rect</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>xmin<span class="op">=</span><span class="fl">1986</span>, xmax<span class="op">=</span><span class="fl">1992</span>, ymin<span class="op">=</span><span class="op">-</span><span class="cn">Inf</span>, ymax<span class="op">=</span><span class="cn">Inf</span><span class="op">)</span>, fill <span class="op">=</span> <span class="st">"cyan"</span>, alpha <span class="op">=</span> <span class="fl">0.01</span><span class="op">)</span><span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span><span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_text.html">geom_text</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>label <span class="op">=</span> <span class="va">year</span><span class="op">)</span>, hjust<span class="op">=</span><span class="op">-</span><span class="fl">0.002</span>, vjust <span class="op">=</span> <span class="op">-</span><span class="fl">0.03</span><span class="op">)</span><span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_hline</a></span><span class="op">(</span>yintercept <span class="op">=</span> <span class="fl">0</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_linerange.html">geom_errorbar</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>ymin <span class="op">=</span> <span class="va">mean</span><span class="op">-</span><span class="va">sd</span><span class="op">*</span><span class="fl">1.96</span>, ymax <span class="op">=</span> <span class="va">mean</span><span class="op">+</span><span class="va">sd</span><span class="op">*</span><span class="fl">1.96</span><span class="op">)</span>, width <span class="op">=</span> <span class="fl">0.2</span>,
                position <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/position_dodge.html">position_dodge</a></span><span class="op">(</span><span class="fl">0.05</span><span class="op">)</span><span class="op">)</span></code></pre></div>
</div>
<div id="going-beyond-cunningham2013" class="section level3" number="9.5.4">
<h3>
<span class="header-section-number">9.5.4</span> Going beyond <span class="citation">Cunningham and Cornwell (<a href="references.html#ref-Cunningham2013" role="doc-biblioref">2013</a>)</span><a class="anchor" aria-label="anchor" href="#going-beyond-cunningham2013"><i class="fas fa-link"></i></a>
</h3>
<p>The US experience with abortion legalization predicted a parabola from 1986 to 1992 for 15- to 19-year-olds, and using a DD design, that’s what I found. I also estimated the effect using a DDD design, and while the effects weren’t as pretty as what I found with DD, there appeared to be something going on in the general vicinity of where the model predicted. So boom goes the dynamite, right? Can’t we be done finally? Not quite.</p>
<p>Whereas my original study stopped there, I would like to go a little farther. The reason can be seen in the following Figure <a href="ch8.html#fig:cohort-id2">9.13</a>. This is a modified version of Figure <a href="ch8.html#fig:cohort-id">9.9</a>, with the main difference being I have created a new parabola for the 20- to 24-year-olds.</p>
<p>Look carefully at Figure <a href="ch8.html#fig:cohort-id2">9.13</a>. Insofar as the early 1970s cohorts were treated in utero with abortion legalization, then we should see not just a parabola for the 15- to 19-year-olds for 1986 to 1992 but also for the 20- to 24-year-olds for years 1991 to 1997 as the cohorts continuedto age.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;There is a third prediction on the 25- to 29-year-olds, but for the sake of space, I only focus on the 20- to 24-year-olds.&lt;/p&gt;"><sup>146</sup></a></p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/abortion_dd2.do"><code>abortion_dd2.do</code></a></em></p>
<div class="sourceCode" id="cb89"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb89-1"><a href="ch8.html#cb89-1" aria-hidden="true"></a><span class="kw">use</span> https:<span class="co">//github.com/scunning1975/mixtape/raw/master/abortion.dta, clear</span></span>
<span id="cb89-2"><a href="ch8.html#cb89-2" aria-hidden="true"></a></span>
<span id="cb89-3"><a href="ch8.html#cb89-3" aria-hidden="true"></a>* Second DD <span class="kw">model</span> <span class="kw">for</span> 20-24 <span class="fu">year</span> old <span class="bn">black</span> females</span>
<span id="cb89-4"><a href="ch8.html#cb89-4" aria-hidden="true"></a><span class="fu">char</span> <span class="fu">year</span>[omit] 1985 </span>
<span id="cb89-5"><a href="ch8.html#cb89-5" aria-hidden="true"></a><span class="kw">xi</span>: <span class="kw">reg</span> lnr i.repeal*i.<span class="fu">year</span> i.fip acc <span class="kw">ir</span> <span class="kw">pi</span> alcohol crack poverty income ur <span class="kw">if</span> (race==2 &amp; sex==2 &amp; age==20) [<span class="kw">aweight</span>=totpop], <span class="kw">cluster</span>(fip) </span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/abortion_dd2.R"><code>abortion_dd2.R</code></a></em></p>
<div class="sourceCode" id="cb90"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://haven.tidyverse.org">haven</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">estimatr</span><span class="op">)</span>

<span class="va">read_data</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">df</span><span class="op">)</span>
<span class="op">{</span>
  <span class="va">full_path</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"https://raw.github.com/scunning1975/mixtape/master/"</span>, 
                     <span class="va">df</span>, sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span>
  <span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/read_dta.html">read_dta</a></span><span class="op">(</span><span class="va">full_path</span><span class="op">)</span>
  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">df</span><span class="op">)</span>
<span class="op">}</span>

<span class="va">abortion</span> <span class="op">&lt;-</span> <span class="fu">read_data</span><span class="op">(</span><span class="st">"abortion.dta"</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>
    repeal <span class="op">=</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="va">repeal</span><span class="op">)</span>,
    year   <span class="op">=</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="va">year</span><span class="op">)</span>,
    fip    <span class="op">=</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="va">fip</span><span class="op">)</span>,
    fa     <span class="op">=</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="va">fa</span><span class="op">)</span>,
  <span class="op">)</span>

<span class="va">reg</span> <span class="op">&lt;-</span> <span class="va">abortion</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">race</span> <span class="op">==</span> <span class="fl">2</span> <span class="op">&amp;</span> <span class="va">sex</span> <span class="op">==</span> <span class="fl">2</span> <span class="op">&amp;</span> <span class="va">age</span> <span class="op">==</span> <span class="fl">20</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu">lm_robust</span><span class="op">(</span><span class="va">lnr</span> <span class="op">~</span> <span class="va">repeal</span><span class="op">*</span><span class="va">year</span> <span class="op">+</span> <span class="va">fip</span> <span class="op">+</span> <span class="va">acc</span> <span class="op">+</span> <span class="va">ir</span> <span class="op">+</span> <span class="va">pi</span> <span class="op">+</span> <span class="va">alcohol</span><span class="op">+</span> <span class="va">crack</span> <span class="op">+</span> <span class="va">poverty</span><span class="op">+</span> <span class="va">income</span><span class="op">+</span> <span class="va">ur</span>,
            data <span class="op">=</span> <span class="va">.</span>, weights <span class="op">=</span> <span class="va">totpop</span>, clusters <span class="op">=</span> <span class="va">fip</span><span class="op">)</span></code></pre></div>
<p>I did not examine the 20- to 24-year-old cohort when I first wrote this paper because at that time I doubted that the selection effects for risky sex would persist into adulthood given that youth display considerable risk-taking behavior. But with time come new perspectives, and these days I don’t have strong priors that the selection effects would necessarily vanish after teenage years. So I’d like to conduct that additional analysis here and now for the first time. Let’s estimate the same DD model as before, only for Black females aged 20–24.</p>
<p>As before, we will focus just on the coefficient plots. We show that in Figure <a href="ch8.html#fig:bf20-dd">9.14</a>. There are a couple of things about this regression output that are troubling. First, there is a negative parabola showing up where there wasn’t necessarily one predicted—the 1986–1992 period. Note that is the period where only the 15- to 19-year-olds were the treated cohorts, suggesting that our 15- to 19-year-old analysis was picking up something other than abortion legalization. But that was also the justification for using DDD, as clearly something else is going on in the repeal versus <em>Roe</em> states during those years that we cannot adequately control for with our controls and fixed effects.</p>
<div class="figure">
<span id="fig:cohort-id2"></span>
<img src="graphics/cohort_id2.jpg" alt="Theoretical predictions of abortion legalization on age profiles of gonorrhea incidence for 20--24 year olds" width="100%"><p class="caption">
Figure 9.13: Theoretical predictions of abortion legalization on age profiles of gonorrhea incidence for 20–24 year olds
</p>
</div>
<p>The second thing to notice is that there is <em>no</em> parabola in the treatment window for the treatment cohort. The effect sizes are negative in the beginning, but shrink in absolute value when they should be growing. In fact, the 1991 to 1997 period is one of convergence to zero, not divergence between these two sets of states.</p>
<div class="figure">
<span id="fig:bf20-dd"></span>
<img src="graphics/bf20_dd.jpg" alt="Coefficients and standard errors from DD regression equation for the 20--24 year olds" width="100%"><p class="caption">
Figure 9.14: Coefficients and standard errors from DD regression equation for the 20–24 year olds
</p>
</div>
<p>But as before, maybe there are strong trending unobservables for all groups masking the abortion legalization effect. To check, let’s use my DDD strategy with the 25- to 29-year-olds as the within-state control group. We can implement this by using the Stata code, abortion_ddd2.do and abortion_ddd2.R.</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/abortion_ddd2.do"><code>abortion_ddd2.do</code></a></em></p>
<div class="sourceCode" id="cb91"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb91-1"><a href="ch8.html#cb91-1" aria-hidden="true"></a><span class="kw">use</span> https:<span class="co">//github.com/scunning1975/mixtape/raw/master/abortion.dta, clear</span></span>
<span id="cb91-2"><a href="ch8.html#cb91-2" aria-hidden="true"></a></span>
<span id="cb91-3"><a href="ch8.html#cb91-3" aria-hidden="true"></a>* Second DDD <span class="kw">model</span> <span class="kw">for</span> 20-24 <span class="fu">year</span> olds vs 25-29 <span class="fu">year</span> olds <span class="bn">black</span> females <span class="kw">in</span> repeal vs Roe states</span>
<span id="cb91-4"><a href="ch8.html#cb91-4" aria-hidden="true"></a><span class="kw">gen</span> younger2 = 0 </span>
<span id="cb91-5"><a href="ch8.html#cb91-5" aria-hidden="true"></a><span class="kw">replace</span> younger2 = 1 <span class="kw">if</span> age == 20</span>
<span id="cb91-6"><a href="ch8.html#cb91-6" aria-hidden="true"></a><span class="kw">gen</span> yr2=(repeal==1) &amp; (younger2==1)</span>
<span id="cb91-7"><a href="ch8.html#cb91-7" aria-hidden="true"></a><span class="kw">gen</span> wm=(wht==1) &amp; (male==1)</span>
<span id="cb91-8"><a href="ch8.html#cb91-8" aria-hidden="true"></a><span class="kw">gen</span> wf=(wht==1) &amp; (male==0)</span>
<span id="cb91-9"><a href="ch8.html#cb91-9" aria-hidden="true"></a><span class="kw">gen</span> bm=(wht==0) &amp; (male==1)</span>
<span id="cb91-10"><a href="ch8.html#cb91-10" aria-hidden="true"></a><span class="kw">gen</span> bf=(wht==0) &amp; (male==0)</span>
<span id="cb91-11"><a href="ch8.html#cb91-11" aria-hidden="true"></a><span class="fu">char</span> <span class="fu">year</span>[omit] 1985 </span>
<span id="cb91-12"><a href="ch8.html#cb91-12" aria-hidden="true"></a><span class="fu">char</span> repeal[omit] 0 </span>
<span id="cb91-13"><a href="ch8.html#cb91-13" aria-hidden="true"></a><span class="fu">char</span> younger2[omit] 0 </span>
<span id="cb91-14"><a href="ch8.html#cb91-14" aria-hidden="true"></a><span class="fu">char</span> fip[omit] 1 </span>
<span id="cb91-15"><a href="ch8.html#cb91-15" aria-hidden="true"></a><span class="fu">char</span> fa[omit] 0 </span>
<span id="cb91-16"><a href="ch8.html#cb91-16" aria-hidden="true"></a><span class="fu">char</span> yr2[omit] 0  </span>
<span id="cb91-17"><a href="ch8.html#cb91-17" aria-hidden="true"></a><span class="kw">xi</span>: <span class="kw">reg</span> lnr i.repeal*i.<span class="fu">year</span> i.younger2*i.repeal i.younger2*i.<span class="fu">year</span> i.yr2*i.<span class="fu">year</span> i.fip*t acc <span class="kw">pi</span> <span class="kw">ir</span> alcohol crack  poverty income ur <span class="kw">if</span> bf==1 &amp; (age==20 | age==25) [<span class="kw">aweight</span>=totpop], <span class="kw">cluster</span>(fip) </span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/abortion_ddd2.R"><code>abortion_ddd2.R</code></a></em></p>
<div class="sourceCode" id="cb92"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://haven.tidyverse.org">haven</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">estimatr</span><span class="op">)</span>

<span class="va">read_data</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">df</span><span class="op">)</span>
<span class="op">{</span>
  <span class="va">full_path</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"https://raw.github.com/scunning1975/mixtape/master/"</span>, 
                     <span class="va">df</span>, sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span>
  <span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/read_dta.html">read_dta</a></span><span class="op">(</span><span class="va">full_path</span><span class="op">)</span>
  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">df</span><span class="op">)</span>
<span class="op">}</span>

<span class="va">abortion</span> <span class="op">&lt;-</span> <span class="fu">read_data</span><span class="op">(</span><span class="st">"abortion.dta"</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>
    repeal   <span class="op">=</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="va">repeal</span><span class="op">)</span>,
    year     <span class="op">=</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="va">year</span><span class="op">)</span>,
    fip      <span class="op">=</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="va">fip</span><span class="op">)</span>,
    fa       <span class="op">=</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="va">fa</span><span class="op">)</span>,
    younger2 <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">age</span> <span class="op">==</span> <span class="fl">20</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span>,
    yr2      <span class="op">=</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">repeal</span> <span class="op">==</span> <span class="fl">1</span> <span class="op">&amp;</span> <span class="va">younger2</span> <span class="op">==</span> <span class="fl">1</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span><span class="op">)</span>,
    wm       <span class="op">=</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">wht</span> <span class="op">==</span> <span class="fl">1</span> <span class="op">&amp;</span> <span class="va">male</span> <span class="op">==</span> <span class="fl">1</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span><span class="op">)</span>,
    wf       <span class="op">=</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">wht</span> <span class="op">==</span> <span class="fl">1</span> <span class="op">&amp;</span> <span class="va">male</span> <span class="op">==</span> <span class="fl">0</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span><span class="op">)</span>,
    bm       <span class="op">=</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">wht</span> <span class="op">==</span> <span class="fl">0</span> <span class="op">&amp;</span> <span class="va">male</span> <span class="op">==</span> <span class="fl">1</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span><span class="op">)</span>,
    bf       <span class="op">=</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">wht</span> <span class="op">==</span> <span class="fl">0</span> <span class="op">&amp;</span> <span class="va">male</span> <span class="op">==</span> <span class="fl">0</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span><span class="op">)</span>
  <span class="op">)</span>

<span class="va">regddd</span> <span class="op">&lt;-</span> <span class="va">abortion</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">bf</span> <span class="op">==</span> <span class="fl">1</span> <span class="op">&amp;</span> <span class="op">(</span><span class="va">age</span> <span class="op">==</span> <span class="fl">20</span> <span class="op">|</span> <span class="va">age</span> <span class="op">==</span><span class="fl">25</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu">lm_robust</span><span class="op">(</span><span class="va">lnr</span> <span class="op">~</span> <span class="va">repeal</span><span class="op">*</span><span class="va">year</span> <span class="op">+</span> <span class="va">acc</span> <span class="op">+</span> <span class="va">ir</span> <span class="op">+</span> <span class="va">pi</span> <span class="op">+</span> <span class="va">alcohol</span> <span class="op">+</span> <span class="va">crack</span> <span class="op">+</span> <span class="va">poverty</span> <span class="op">+</span> <span class="va">income</span> <span class="op">+</span> <span class="va">ur</span>,
            data <span class="op">=</span> <span class="va">.</span>, weights <span class="op">=</span> <span class="va">totpop</span>, clusters <span class="op">=</span> <span class="va">fip</span><span class="op">)</span></code></pre></div>
<p>Figure <a href="ch8.html#fig:bf20-ddd">9.15</a> shows the DDD estimated coefficients for the treated cohort relative to a slightly older 25- to 29-year-old cohort. It’s possible that the 25- to 29-year-old cohort is too close in age to function as a satisfactory within-state control; if those age 20–24 have sex with those who are age 25–29, for instance, then SUTVA is violated. There are other age groups, though, that you can try in place of the 25- to 29-year-olds, and I encourage you to do it for both the experience and the insights you might gleam.</p>
<div class="figure">
<span id="fig:bf20-ddd"></span>
<img src="graphics/bf20_ddd.jpg" alt="Coefficients and standard errors from DDD regression equation for the 20--24 year olds vs 25--29 year olds" width="100%"><p class="caption">
Figure 9.15: Coefficients and standard errors from DDD regression equation for the 20–24 year olds vs 25–29 year olds
</p>
</div>
<p>But let’s back up and remember the big picture. The abortion legalization hypothesis made a series of predictions about where negative parabolic treatment effects should appear in the data. And while we found some initial support, when we exploited more of those predictions, the results fell apart. A fair interpretation of this exercise is that our analysis does <em>not</em> support the abortion legalization hypothesis. Figure <a href="ch8.html#fig:bf20-ddd">9.15</a> shows several point estimates at nearly zero, and standard errors so large as to include both positive and negative values for these interactions.</p>
<p>I included this analysis because I wanted to show you the power of a theory with numerous unusual yet testable predictions. Imagine for a moment if a parabola had showed up for all age groups in precisely the years predicted by the theory. Wouldn’t we <em>have</em> to update our priors about the abortion legalization selection hypothesis? With predictions so narrow, what else could be causing it? It’s precisely because the predictions are so specific, though, that we are able to reject the abortion legalization hypothesis, at least for gonorrhea.</p>
</div>
<div id="placebos-as-critique" class="section level3" number="9.5.5">
<h3>
<span class="header-section-number">9.5.5</span> Placebos as critique<a class="anchor" aria-label="anchor" href="#placebos-as-critique"><i class="fas fa-link"></i></a>
</h3>
<p>Since the fundamental problem of causal inference blocks our direct observation of causal effects, we rely on many direct and indirect pieces of evidence to establish credible causality. And as I said in the previous section on DDD, one of those indirect pieces of evidence is placebo analysis. The reasoning goes that if we find, using our preferred research design, effects where there shouldn’t be, then maybe our original findings weren’t credible in the first place. Using placebo analysis within your own work has become an essential part of empirical work for this reason.</p>
<p>But another use of placebo analysis is to evaluate the credibility of popular estimation strategies themselves. This kind of use helps improve a literature by uncovering flaws in a research design which can then help stimulate the creation of stronger methods and models. Let’s take two exemplary studies that accomplished this well: <span class="citation">Auld and Grootendorst (<a href="references.html#ref-Auld2004" role="doc-biblioref">2004</a>)</span> and <span class="citation">Cohen-Cole and Fletcher (<a href="references.html#ref-Fletcher2008" role="doc-biblioref">2008</a>)</span>.</p>
<p>To say that the <span class="citation">Becker and Murphy (<a href="references.html#ref-Becker1988b" role="doc-biblioref">1988</a>)</span> “rational addiction” model has been influential would be an understatement. It has over 4,000 cites and has become one of the most common frameworks in health economics. It created a cottage industry of empirical studies that persists to this day. Alcohol, tobacco, gambling, even sports, have all been found to be “rationally addictive” commodities and activities using various empirical approaches.</p>
<p>But some researchers cautioned the research community about these empirical studies. <span class="citation">Rogeberg (<a href="references.html#ref-Rogeberg2004" role="doc-biblioref">2004</a>)</span> critiqued the theory on its own grounds, but I’d like to focus on the empirical studies based on the theory. Rather than talk about any specific paper, I’d like to provide a quote from <span class="citation">Melberg (<a href="references.html#ref-Merlberg2008" role="doc-biblioref">2008</a>)</span>, who surveyed researchers who had written on rational addiction:</p>
<blockquote>
<p>A majority of our respondents believe the literature is a success story that demonstrates the power of economic reasoning. At the same time, they also believe the empirical evidence is weak, and they disagree both on the type of evidence that would validate the theory and the policy implications. Taken together, this points to an interesting gap. On the one hand, most of the respondents claim that the theory has valuable real world implications. On the other hand, they do not believe the theory has received empiricalsupport. (p.1)</p>
</blockquote>
<p>Rational addiction should be held to the same empirical standards as in theory. The strength of the model has always been based on the economic reasoning, which economists obviously find compelling. But were the empirical designs flawed? How could we know?</p>
<p><span class="citation">Auld and Grootendorst (<a href="references.html#ref-Auld2004" role="doc-biblioref">2004</a>)</span> is not a test of the rational addiction model. On the contrary, it is an “anti-test” of the empirical rational addiction models common at the time. Their goal was not to evaluate the theoretical rational addiction model, in other words, but rather the empirical rational addiction models themselves. How do they do this? <span class="citation">Auld and Grootendorst (<a href="references.html#ref-Auld2004" role="doc-biblioref">2004</a>)</span> used the empirical rational addiction model to evaluate commodities that could not plausibly be considered addictive, such as eggs, milk, orange, and apples. They found that the empirical rational addiction model implied milk was extremely addictive, perhaps one of the most addictive commodities studied.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Milk is ironically my favorite drink, even over IPAs, so I am not persuaded by this anti-test.&lt;/p&gt;"><sup>147</sup></a> Is it credible to believe that eggs and milk are “rationally addictive” or is it more likely the research designs used to evaluate the rational addiction model were flawed? <span class="citation">Auld and Grootendorst (<a href="references.html#ref-Auld2004" role="doc-biblioref">2004</a>)</span> study cast doubt on the empirical rational addiction model, not the theory.</p>
<p>Another problematic literature was the peer-effects literature. Estimating peer effects is notoriously hard. <span class="citation">Manski (<a href="references.html#ref-Manski1993" role="doc-biblioref">1993</a>)</span> said that the deep endogeneity of social interactions made the identification of peer effects difficult and possibly even impossible. He called this problem the “mirroring” problem. If “birds of a feather flock together,” then identifying peer effects in observational settings may just be impossible due to the profound endogeneities at play.</p>
<p>Several studies found significant network effects on outcomes like obesity, smoking, alcohol use, and happiness. This led many researchers to conclude that these kinds of risk behaviors were “contagious” through peer effects <span class="citation">(Christakis and Fowler <a href="references.html#ref-Christakis2007" role="doc-biblioref">2007</a>)</span>. But these studies did not exploit randomized social groups. The peer groups were purely endogenous. <span class="citation">Cohen-Cole and Fletcher (<a href="references.html#ref-Fletcher2008" role="doc-biblioref">2008</a>)</span> showed using similar models and data that even attributes that <em>couldn’t</em> be transmitted between peers—acne, height, and headaches—appeared “contagious” in observational data using the <span class="citation">Christakis and Fowler (<a href="references.html#ref-Christakis2007" role="doc-biblioref">2007</a>)</span> model for estimation. Note, <span class="citation">Cohen-Cole and Fletcher (<a href="references.html#ref-Fletcher2008" role="doc-biblioref">2008</a>)</span> does not reject the idea of theoretical contagions. Rather, they point out that the Manski critique should guide peer effect analysis if social interactions are endogenous. They provide evidence for this indirectly using placebo analysis.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Breakthroughs in identifying peer effects eventually emerged, but only from studies that serendipitously had randomized peer groups such as &lt;span class="citation"&gt;Sacerdote (&lt;a href="references.html#ref-Sacerdote2001" role="doc-biblioref"&gt;2001&lt;/a&gt;)&lt;/span&gt;, &lt;span class="citation"&gt;Lyle (&lt;a href="references.html#ref-Lyle2009" role="doc-biblioref"&gt;2009&lt;/a&gt;)&lt;/span&gt;, &lt;span class="citation"&gt;Carrell, Hoekstra, and West (&lt;a href="references.html#ref-Carrell2019" role="doc-biblioref"&gt;2019&lt;/a&gt;)&lt;/span&gt;, &lt;span class="citation"&gt;Kofoed and McGovney (&lt;a href="references.html#ref-Kofoed2019" role="doc-biblioref"&gt;2019&lt;/a&gt;)&lt;/span&gt;, and several others. Many of these papers either used randomized roommates or randomized companies at military academies. Such natural experiments are rare opportunities for studying peer effects for their ability to overcome the mirror problem.&lt;/p&gt;'><sup>148</sup></a></p>
</div>
<div id="compositional-change-within-repeated-cross-sections" class="section level3" number="9.5.6">
<h3>
<span class="header-section-number">9.5.6</span> Compositional change within repeated cross-sections<a class="anchor" aria-label="anchor" href="#compositional-change-within-repeated-cross-sections"><i class="fas fa-link"></i></a>
</h3>
<p>DD can be applied to repeated cross-sections, as well as panel data. But one of the risks of working with the repeated cross-sections is that unlike panel data (e.g., individual-level panel data), repeated cross-sections run the risk of compositional changes. <span class="citation">Hong (<a href="references.html#ref-Hong2013" role="doc-biblioref">2013</a>)</span> used repeated cross-sectional data from the Consumer Expenditure Survey (CEX) containing music expenditure and internet use for a random sample of households. The author’s study exploited the emergence and immense popularity of Napster, the first file-sharing software widely used by Internet users, in June 1999 as a natural experiment. The study compared Internet users and Internet non-users before and after the emergence of Napster. At first glance, they found that as Internet diffusion increased from 1996 to 2001, spending on music for Internet users fell faster than that for non-Internet users. This was initially evidence that Napster was responsible for the decline, until this was investigated more carefully.</p>
<p>But when we look at Table <a href="ch8.html#tab:hong">9.4</a>, we see evidence of compositional changes. While music expenditure fell over the treatment period, the demographics of the two groups also changed over this period. For instance, the age of Internet users grew while income fell. If older people are less likely to buy music in the first place, then this could independently explain some of the decline. This kind of compositional change is a like an omitted variable bias built into the sample itself caused by time-variant unobservables. Diffusion of the Internet appears to be related to changing samples as younger music fans are early adopters. Identification of causal effects would need for the treatment itself to be exogenous to such changes in the composition.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:hong">Table 9.4: </span> Changes between Internet and non-Internet users over time.</caption>
<tbody>
<tr class="odd">
<td align="left">Year</td>
<td></td>
<td align="center">1997</td>
<td></td>
<td align="center">1998</td>
<td></td>
<td align="center">1999</td>
</tr>
<tr class="even">
<td align="left"></td>
<td><strong>Internet user</strong></td>
<td align="center"><strong>Non-user</strong></td>
<td><strong>Internet user</strong></td>
<td align="center"><strong>Non-user</strong></td>
<td><strong>Internet user</strong></td>
<td align="center"><strong>Non-user</strong></td>
</tr>
<tr class="odd">
<td align="left"><em>Average expenditure</em></td>
<td></td>
<td align="center"></td>
<td></td>
<td align="center"></td>
<td></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">Recorded music</td>
<td>$25.73</td>
<td align="center">$10.90</td>
<td>$24.18</td>
<td align="center">$9.97</td>
<td>$20.92</td>
<td align="center">$9.37</td>
</tr>
<tr class="odd">
<td align="left">Entertainment</td>
<td>$195.03</td>
<td align="center">$96.71</td>
<td>$193.38</td>
<td align="center">$84.92</td>
<td>$182.42</td>
<td align="center">$80.19</td>
</tr>
<tr class="even">
<td align="left"><em>Zero expenditure</em></td>
<td></td>
<td align="center"></td>
<td></td>
<td align="center"></td>
<td></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">Recorded music</td>
<td>0.56</td>
<td align="center">0.79</td>
<td>0.60</td>
<td align="center">0.80</td>
<td>0.64</td>
<td align="center">0.81</td>
</tr>
<tr class="even">
<td align="left">Entertainment</td>
<td>0.08</td>
<td align="center">0.32</td>
<td>0.09</td>
<td align="center">0.35</td>
<td>0.14</td>
<td align="center">0.39</td>
</tr>
<tr class="odd">
<td align="left"><em>Demographics</em></td>
<td></td>
<td align="center"></td>
<td></td>
<td align="center"></td>
<td></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">Age</td>
<td>40.2</td>
<td align="center">49.0</td>
<td>42.3</td>
<td align="center">49.0</td>
<td>44.1</td>
<td align="center">49.4</td>
</tr>
<tr class="odd">
<td align="left">Income</td>
<td>$52,887</td>
<td align="center">$30,459</td>
<td>$51,995</td>
<td align="center">$26,189</td>
<td>$49,970</td>
<td align="center">$26,649</td>
</tr>
<tr class="even">
<td align="left">High school graduate</td>
<td>0.18</td>
<td align="center">0.31</td>
<td>0.17</td>
<td align="center">0.32</td>
<td>0.21</td>
<td align="center">0.32</td>
</tr>
<tr class="odd">
<td align="left">Some college</td>
<td>0.37</td>
<td align="center">0.28</td>
<td>0.35</td>
<td align="center">0.27</td>
<td>0.34</td>
<td align="center">0.27</td>
</tr>
<tr class="even">
<td align="left">College grad</td>
<td>0.43</td>
<td align="center">0.21</td>
<td>0.45</td>
<td align="center">0.21</td>
<td>0.42</td>
<td align="center">0.20</td>
</tr>
<tr class="odd">
<td align="left">Manager</td>
<td>0.16</td>
<td align="center">0.08</td>
<td>0.16</td>
<td align="center">0.08</td>
<td>0.14</td>
<td align="center">0.08</td>
</tr>
</tbody>
</table></div>
<p class="footnote">
Sample means from the Consumer Expenditure Survey.
</p>
</div>
<div id="final-thoughts" class="section level3" number="9.5.7">
<h3>
<span class="header-section-number">9.5.7</span> Final thoughts<a class="anchor" aria-label="anchor" href="#final-thoughts"><i class="fas fa-link"></i></a>
</h3>
<p>There are a few other caveats I’d like to make before moving on. First, it is important to remember the concepts we learned in the early DAG chapter. In choosing covariates in a DD design, you must resist the temptation to simply load the regression up with a kitchen sink of regressors. You should resist if only because in so doing, you may inadvertently include a collider, and if a collider is conditioned on, it introduces strange patterns that may mislead you and your audience. There is unfortunately no way forward except, again, deep institutional familiarity with both the factors that determined treatment assignment on the ground, as well as economic theory itself. Second, another issue I skipped over entirely is the question of how the outcome is modeled. Very little thought if any is given to how exactly we should model some outcome. Just to take one example, should we use the log or the levels themselves? Should we use the quartic root? Should we use rates? These, it turns, out are critically important because for many of them, the parallel trends assumption needed for identification will not be achieved—even though it will be achieved under some other unknown transformation. It is for this reason that you can think of many DD designs as having a parametric element because you must make strong commitments about the functional form itself. I cannot provide guidance to you on this, except that maybe using the pre-treatment leads as a way of finding parallelism could be a useful guide.</p>
<div class="cover-box">
<div class="row">
    <div class="col-xs-8 col-md-4 cover-img">
        <a href="https://www.amazon.com/dp/0300251688"><img src="../images/cover.jpg" alt="Buy Today!"></a>
    </div>
    
    <div class="col-xs-12 col-md-8 cover-text-box">
            <h2> 
                Causal Inference: 
                <br><span style="font-style: italic; font-weight:bold; font-size: 20px;">The Mixtape.</span>
            </h2> 
            
        <div class="cover-text">
            <p>Buy the print version today:</p>
            
            <div class="chips">
                <a href="https://www.amazon.com/dp/0300251688" class="app-chip"> 
                    <i class="fab fa-amazon" aria-hidden="true"></i> Buy from Amazon 
                </a>
    
                <a href="https://yalebooks.yale.edu/book/9780300251685/causal-inference" class="app-chip"> 
                    <i class="fas fa-book" aria-hidden="true"></i> Buy from Yale Press 
                </a>
            </div>
        </div>
    </div>
</div>
</div>
</div>
</div>
<div id="twoway-fixed-effects-with-differential-timing" class="section level2" number="9.6">
<h2>
<span class="header-section-number">9.6</span> Twoway Fixed Effects with Differential Timing<a class="anchor" aria-label="anchor" href="#twoway-fixed-effects-with-differential-timing"><i class="fas fa-link"></i></a>
</h2>
<p>I have a bumper sticker on my car that says “I love Federalism (for the natural experiments)” (Figure <a href="ch8.html#fig:bumpersticker">9.16</a>). I made these bumper stickers for my students to be funny, and to illustrate that the United States is a never-ending laboratory. Because of state federalism, each US state has been given considerable discretion to govern itself with policies and reforms. Yet, because it is a union of states, US researchers have access to many data sets that have been harmonized across states, making it even more useful for causal inference.</p>
<div class="figure">
<span id="fig:bumpersticker"></span>
<img src="graphics/bumpersticker.jpg" alt="A bumper sticker for nerds." width="100%"><p class="caption">
Figure 9.16: A bumper sticker for nerds.
</p>
</div>
<p><span class="citation">Goodman-Bacon (<a href="references.html#ref-Bacon2019" role="doc-biblioref">2019</a>)</span> calls the staggered assignment of treatments across geographic units over time the “differential timing” of treatment. What he means is unlike the simple <span class="math inline">\(2\times 2\)</span> that we discussed earlier (e.g., New Jersey and Pennsylvania), where treatment units were all treated at the same time, the more common situation is one where geographic units receive treatments at different points in time. And this happens in the United States because each area (state, municipality) will adopt a policy when it wants to, for its own reasons. As a result, the adoption of some treatment will tend to be differentially timed across units.</p>
<p>This introduction of differential timing means there are basically two types of DD designs. There is the <span class="math inline">\(2\times 2\)</span> DD we’ve been discussing wherein a single unit or a group of units all receive some treatment at the same point in time, like Snow’s cholera study or <span class="citation">Card and Krueger (<a href="references.html#ref-Card1994" role="doc-biblioref">1994</a>)</span>. And then there is the DD with differential timing in which groups receive treatment at different points in time, like <span class="citation">Cheng and Hoekstra (<a href="references.html#ref-Cheng2013" role="doc-biblioref">2013</a>)</span>. We have a very good understanding of the <span class="math inline">\(2\times 2\)</span> design, how it works, why it works, when it works, and when it does not work. But we did not until <span class="citation">Goodman-Bacon (<a href="references.html#ref-Bacon2019" role="doc-biblioref">2019</a>)</span> have as good an understanding of the DD design with differential timing. So let’s get down to business and discuss that now by reminding ourselves of the <span class="math inline">\(2\times 2\)</span> DD that we introduced earlier.
<span class="math display">\[\begin{align}
\widehat{\delta}^{2\times 2}_{kU} =
\bigg(\overline{y}_k^{\text{post}(k)} - \overline{y}_k^{\text{pre}(k)} \bigg) - \bigg(\overline{y}_U^{\text{post}(k)} - \overline{y}_U^{\text{pre}(k)} \bigg )
\end{align}\]</span>
where <span class="math inline">\(k\)</span> is the treatment group, <span class="math inline">\(U\)</span> is the never-treated group, and everything else is self-explanatory. Since this involves sample means, we can calculate the differences manually. Or we can estimate it with the following regression:
<span class="math display">\[\begin{align}
y_{it} =\beta D_{i}+\tau \mathop{\mathrm{Post}}_{t}+\delta (D_i \times \mathop{\mathrm{Post}}_t)+X_{it}+\varepsilon_{it}
\end{align}\]</span></p>
<p>But a more common situation you’ll encounter will be a DD design with differential timing. And while the decomposition is a bit complicated, the regression equation itself is straightforward:
<span class="math display">\[\begin{align}
y_{it} =\alpha_0 + \delta D_{it} + X_{it} + \alpha_i + \alpha_t + \epsilon_{it}
\end{align}\]</span>
When researchers estimate this regression these days, they usually use the linear fixed-effects model that I discussed in the previous panel chapter. These linear panel models have gotten the nickname “twoway fixed effects” because they include both time fixed effects and unit fixed effects. Since this is such a popular estimator, it’s important we understand exactly what it is doing and what is it not.</p>
<div id="bacon-decomposition-theorem" class="section level3" number="9.6.1">
<h3>
<span class="header-section-number">9.6.1</span> Bacon Decomposition theorem<a class="anchor" aria-label="anchor" href="#bacon-decomposition-theorem"><i class="fas fa-link"></i></a>
</h3>
<p><span class="citation">Goodman-Bacon (<a href="references.html#ref-Bacon2019" role="doc-biblioref">2019</a>)</span> provides a helpful decomposition of the twoway fixed effects estimate of <span class="math inline">\(\widehat{\delta}\)</span>. Given this is the go-to model for implementing differential timing designs, I have found his decomposition useful. But as there are some other decompositions of twoway fixed effects estimators, such as another important paper by <span class="citation">Chaisemartin and D’Haultfœuille (<a href="references.html#ref-deChaisemartin2019" role="doc-biblioref">2019</a>)</span>, I’ll call it the Bacon decomposition for the sake of branding.</p>
<p>The punchline of the Bacon decomposition theorem is that the twoway fixed effects estimator is a weighted average of all potential <span class="math inline">\(2\times 2\)</span> DD estimates where weights are both based on group sizes and variance in treatment. Under the assumption of variance weighted common trends (VWCT) and time invariant treatment effects, the variance weighted ATT is a weighted average of all possible ATTs. And under more restrictive assumptions, that estimate perfectly matches the ATT. But that is not true when there are time-varying treatment effects, as time-varying treatment effects in a differential timing design estimated with twoway fixed effects can generate a bias. As such, twoway fixed-effects models may be severely biased, which is echoed in <span class="citation">Chaisemartin and D’Haultfœuille (<a href="references.html#ref-deChaisemartin2019" role="doc-biblioref">2019</a>)</span>.</p>
<p>To make this concrete, let’s start with a simple example. Assume in this design that there are three groups: an early treatment group <span class="math inline">\((k)\)</span>, a group treated later <span class="math inline">\((l)\)</span>, and a group that is never treated <span class="math inline">\((U)\)</span>. Groups <span class="math inline">\(k\)</span> and <span class="math inline">\(l\)</span> are similar in that they are both treated but they differ in that <span class="math inline">\(k\)</span> is treated earlier than <span class="math inline">\(l\)</span>.</p>
<p>Let’s say there are 5 periods, and <span class="math inline">\(k\)</span> is treated in period 2. Then it spends 40% of its time under treatment, or 0.4. But let’s say <span class="math inline">\(l\)</span> is treated in period 4. Then it spends 80% of its time treated, or 0.8. I represent this time spent in treatment for a group as <span class="math inline">\(\overline{D}_k = 0.4\)</span> and <span class="math inline">\(\overline{D}_l = 0.8\)</span>. This is important, because the length of time a group spends in treatment determines its treatment variance, which in turn affects the weight that <span class="math inline">\(2\times 2\)</span> plays in the final adding up of the DD parameter itself. And rather than write out <span class="math inline">\(2\times 2\)</span> DD estimator every time, we will just represent each <span class="math inline">\(2\times 2\)</span> as <span class="math inline">\(\widehat{\delta}_{ab}^{2\times 2,j}\)</span> where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are the treatment groups, and <span class="math inline">\(j\)</span> is the index notation for any treatment group. Thus if we wanted to know the <span class="math inline">\(2\times 2\)</span> for group <span class="math inline">\(k\)</span> compared to group <span class="math inline">\(U\)</span>, we would write <span class="math inline">\(\widehat{\delta}_{kU}^{2\times 2,k}\)</span> or, maybe to save space, just <span class="math inline">\(\widehat{\delta}_{kU}^{k}\)</span>.</p>
<p>So, let’s get started. First, in a single differential timing design, how many <span class="math inline">\(2\times 2\)</span>s are there anyway? Turns out there are a lot. To see, let’s make a toy example. Let’s say there are three timing groups (<span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>, and <span class="math inline">\(c\)</span>) and one untreated group <span class="math inline">\((U)\)</span>. Then there are 9 <span class="math inline">\(2\times 2\)</span> DDs. They are:</p>
<div class="inline-table"><table class="table table-sm"><tbody>
<tr class="odd">
<td align="center">a to b</td>
<td align="center">b to a</td>
<td align="center">c to a</td>
</tr>
<tr class="even">
<td align="center">a to c</td>
<td align="center">b to c</td>
<td align="center">c to b</td>
</tr>
<tr class="odd">
<td align="center">a to U</td>
<td align="center">b to U</td>
<td align="center">c to U</td>
</tr>
</tbody></table></div>
<p>See how it works? Okay, then let’s return to our simpler example where there are two timing groups <span class="math inline">\(k\)</span> and <span class="math inline">\(l\)</span> and one never-treated group. Groups <span class="math inline">\(k\)</span> and <span class="math inline">\(l\)</span> will get treated at time periods <span class="math inline">\(t^*_k\)</span> and <span class="math inline">\(t^*_l\)</span>. The earlier period before anyone is treated will be called the “pre” period, the period between <span class="math inline">\(k\)</span> and <span class="math inline">\(l\)</span> treated is called the “mid” period, and the period after <span class="math inline">\(l\)</span> is treated is called the “post” period. This will be <em>much</em> easier to understand with some simple graphs. Let’s look at Figure <a href="ch8.html#fig:bacon1">9.17</a>. Recall the definition of a <span class="math inline">\(2\times 2\)</span> DD is
<span class="math display">\[
\widehat{\delta}^{2\times 2}_{kU} = \bigg (\overline{y}_k^{\text{post}(k)} - \overline{y}_k^{\text{pre}(k)} \bigg ) - \bigg (\overline{y}_U^{\text{post}(k)} - \overline{y}_U^{\text{pre}(k)} \bigg )
\]</span>
where <span class="math inline">\(k\)</span> and <span class="math inline">\(U\)</span> are just place-holders for any of the groups used in a <span class="math inline">\(2\times 2\)</span>.</p>
<div class="figure">
<span id="fig:bacon1"></span>
<img src="graphics/bacon_goodman_1.jpg" alt="Four $2 \times 2$ DDs [@Bacon2019]. Reprinted with permission from authors." width="100%"><p class="caption">
Figure 9.17: Four <span class="math inline">\(2 \times 2\)</span> DDs <span class="citation">(Goodman-Bacon <a href="references.html#ref-Bacon2019" role="doc-biblioref">2019</a>)</span>. Reprinted with permission from authors.
</p>
</div>
<p>Substituting the information in each of the four panels of Figure <a href="ch8.html#fig:bacon1">9.17</a> into the equation will enable you to calculate what each specific <span class="math inline">\(2\times 2\)</span> is. But we can really just summarize these into three really important <span class="math inline">\(2\times 2\)</span>s, which are:
<span class="math display">\[\begin{align}
\widehat{\delta}^{2\times 2}_{kU} &amp;=\bigg ( \overline{y}_k^{\text{post}(k)} - \overline{y}_k^{\text{pre}(k)} \bigg ) - \bigg ( \overline{y}_U^{\text{post}(k)} - \overline{y}_U^{\text{pre}(k)} \bigg )
\\
\widehat{\delta}^{2\times 2}_{kl} &amp;=\bigg ( \overline{y}_k^{mid(k,l)} - \overline{y}_k^{\text{pre}(k)} \bigg ) - \bigg ( \overline{y}_l^{mid(k,l)} - \overline{y}_l^{\text{pre}(k)} \bigg )
\\
\widehat{\delta}^{2\times 2}_{lk} &amp;=\bigg ( \overline{y}_l^{\text{post}(l)} - \overline{y}_l^{mid(k,l)} \bigg ) - \bigg ( \overline{y}_k^{\text{post}(l)} - \overline{y}_k^{mid(k,l)} \bigg )
\end{align}\]</span>
where the first <span class="math inline">\(2\times 2\)</span> is any timing group compared to the untreated group (<span class="math inline">\(k\)</span> or <span class="math inline">\(l\)</span>), the second is a group compared to the yet-to-be-treated timing group, and the last is the eventually-treated group compared to the already-treated controls.</p>
<p>With this notation in mind, the DD parameter estimate can be decomposed as follows: <span class="math inline">\(\widehat{\delta}^{DD}\)</span>
<span class="math display">\[\begin{align}
\widehat{\delta}^{DD} = \sum_{k \neq U} s_{kU}\widehat{\delta}_{kU}^{2\times 2} + \sum_{k \neq U} \sum_{l&gt;k} s_{kl} \bigg [ \mu_{kl}\widehat{\delta}_{kl}^{2\times 2,k} + (1-\mu_{kl}) \widehat{\delta}_{kl}^{2\times 2,l} \bigg]
\end{align}\]</span>
where the first <span class="math inline">\(2\times 2\)</span> is the <span class="math inline">\(k\)</span> compared to <span class="math inline">\(U\)</span> and the <span class="math inline">\(l\)</span> compared to <span class="math inline">\(U\)</span> (combined to make the equation shorter).<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;All of this decomposition comes from applying the Frisch-Waugh theorem to the underlying twoway fixed effects estimator.&lt;/p&gt;"><sup>149</sup></a> So what are these weights exactly?
<span class="math display">\[\begin{align}
s_{ku} &amp;=\dfrac{ n_k n_u \overline{D}_k (1- \overline{D}_k ) }{ \widehat{Var} ( \tilde{D}_{it} )} \\
s_{kl} &amp;=\dfrac{ n_k n_l (\overline{D}_k - \overline{D}_{l} ) ( 1- ( \overline{D}_k - \overline{D}_{l} )) }{\widehat{Var}(\tilde{D}_{it})} \\
\mu_{kl} &amp;=\dfrac{1 - \overline{D}_k }{1 - ( \overline{D}_k - \overline{D}_{l} )}
\end{align}\]</span>
where <span class="math inline">\(n\)</span> refers to sample sizes, <span class="math inline">\(\overline{D}_k (1- \overline{D}_k )\)</span> <span class="math inline">\((\overline{D}_k - \overline{D}_{l} ) ( 1- ( \overline{D}_k - \overline{D}_{l} ))\)</span> expressions refer to variance of treatment, and the final equation is the same for two timing groups.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;A more recent version of &lt;span class="citation"&gt;Goodman-Bacon (&lt;a href="references.html#ref-Bacon2019" role="doc-biblioref"&gt;2019&lt;/a&gt;)&lt;/span&gt; rewrites this weighting but they are numerically the same, and for these purposes, I prefer the weighting scheme discussed in an earlier version of the paper. See &lt;span class="citation"&gt;Goodman-Bacon (&lt;a href="references.html#ref-Bacon2019" role="doc-biblioref"&gt;2019&lt;/a&gt;)&lt;/span&gt; for the equivalence between his two weighting descriptions.&lt;/p&gt;'><sup>150</sup></a></p>
<p>Two things immediately pop out of these weights that I’d like to bring to your attention. First, notice how “group” variation matters, as opposed to unit-level variation. The Bacon decomposition shows that it’s group variation that twoway fixed effects is using to calculate that parameter you’re seeking. The more states that adopted a law at the same time, the bigger they influence that final aggregate estimate itself.</p>
<p>The other thing that matters in these weights is <em>within-group</em> treatment variance. To appreciate the subtlety of what’s implied, ask yourself—how long does a group have to be treated in order to maximize its treatment variance? Define <span class="math inline">\(X=D(1-D)=D-D^2\)</span>, take the derivative of <span class="math inline">\(V\)</span> with respect to <span class="math inline">\(\overline{D}\)</span>, set <span class="math inline">\(\dfrac{d V}{d \overline{D}}\)</span> equal to zero, and solve for <span class="math inline">\(\overline{D}*\)</span>. Treatment variance is maximized when <span class="math inline">\(\overline{D}=0.5\)</span>. Let’s look at three values of <span class="math inline">\(\overline{D}\)</span> to illustrate this. <span class="math display">\[\begin{gather}
\overline{D}=0.1; 0.1 \times 0.9 = 0.09 \\
\overline{D}=0.4; 0.4 \times 0.6 =0.24 \\
\overline{D}=0.5; 0.5 \times 0.5 = 0.25\end{gather}\]</span> So what are we learning from this, exactly? Well, what we are learning is that being treated in the <em>middle</em> of the panel actually directly influences the numerical value you get when twoway fixed effects are used to estimate the ATT. That therefore means lengthening or shortening the panel can actually change the point estimate purely by changing group treatment variance and nothing more. Isn’t that kind of strange though? What criteria would we even use to determine the best length?</p>
<p>But what about the “treated on treated weights,” or the <span class="math inline">\(s_{kl}\)</span> weight. That doesn’t have a <span class="math inline">\(\overline{D}(1-\overline{D})\)</span> expression. Rather, it has a <span class="math inline">\((\overline{D}_k - \overline{D}_l)(1-(\overline{D}_k - \overline{D}_l)\)</span> expression. So the “middle” isn’t super clear. That’s because it isn’t the middle of treatment for a single group, but rather it’s the middle of the panel for the <em>difference</em> in treatment variance. For instance, let’s say <span class="math inline">\(k\)</span> spends 67% of time treated and <span class="math inline">\(l\)</span> spends 15% of time treated. Then <span class="math inline">\(\overline{D}_k - \overline{D}_l = 0.52\)</span> and therefore <span class="math inline">\(0.52 \times 0.48 = 0.2496\)</span>, which as we showed is very nearly the max value of the variance as is possible (e.g., 0.25). Think about this for a moment—twoway fixed effects with differential timing weights the 2$$2s comparing the two ultimate treatment groups more if the gap in treatment time is close to 0.5.</p>
</div>
<div id="expressing-the-decomposition-in-potential-outcomes" class="section level3" number="9.6.2">
<h3>
<span class="header-section-number">9.6.2</span> Expressing the decomposition in potential outcomes<a class="anchor" aria-label="anchor" href="#expressing-the-decomposition-in-potential-outcomes"><i class="fas fa-link"></i></a>
</h3>
<p>Up to now, we just showed what was inside the DD parameter estimate when using twoway fixed effects: it was nothing more than an “adding up” of all possible <span class="math inline">\(2\times 2\)</span>s weighted by group shares and treatment variance. But that only tells us what DD is numerically; it does not tell us whether the parameter estimate maps onto a meaningful average treatment effect. To do that, we need to take those sample averages and then use the switching equations replace them with potential outcomes. This is key to moving from numbers to estimates of causal effects.</p>
<p>Bacon’s decomposition theorem expresses the DD coefficient in terms of sample average, making it straightforward to substitute with potential outcomes using a modified switching equation. With a little creative manipulation, this will be revelatory. First, let’s define any year-specific ATT as
<span class="math display">\[\begin{align}
  ATT_k(\tau)=E\big[Y^1_{it}-Y^0_{it}  \mathop{\mathrm{\,\vert\,}}k, t=\tau\big]
\end{align}\]</span>
Next, let’s define it over a time window <span class="math inline">\(W\)</span> (e.g., a post-treatment window)
<span class="math display">\[\begin{align}
  ATT_k(\tau)=E\big[Y^1_{it}-Y^0_{it}  \mathop{\mathrm{\,\vert\,}}k,\tau\in W\big]
\end{align}\]</span>
Finally, let’s define differences in average potential outcomes overtime as:
<span class="math display">\[\begin{align}
  \Delta Y^h_k(W_1,W_0) =
  E\big[Y^h_{it}  \mathop{\mathrm{\,\vert\,}}k, W_1\big]-
  E\big[Y^h_{it}  \mathop{\mathrm{\,\vert\,}}k, W_0\big]
\end{align}\]</span>
for <span class="math inline">\(h=0\)</span> (i.e., <span class="math inline">\(Y^0\)</span>) or <span class="math inline">\(h=1\)</span> (i.e., <span class="math inline">\(Y^1\)</span>)</p>
<p>With trends, differences in mean potential outcomes is non-zero. You can see that in Figure <a href="ch8.html#fig:bacon2">9.18</a>.</p>
<div class="figure">
<span id="fig:bacon2"></span>
<img src="graphics/plots-test.jpg" alt="Changing average potential outcomes" width="100%"><p class="caption">
Figure 9.18: Changing average potential outcomes
</p>
</div>
<p>We’ll return to this, but I just wanted to point it out to you so that it would be concrete in your mind when we return to it later.</p>
<p>We can move now from the <span class="math inline">\(2\times 2\)</span>s that we decomposed earlier directly into the ATT, which is ultimately the main thing we want to know. We covered this earlier in the chapter, but review it again here to maintain progress on my argument. I will first write down the <span class="math inline">\(2\times 2\)</span> expression, use the switching equation to introduce potential outcome notation, and through a little manipulation, find some ATT expression.
<span class="math display">\[\begin{align}
\widehat{\delta}^{2\times 2}_{kU}
&amp;=\bigg (E\big[Y_j  \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Post}}\big]-
E\big[Y_j  \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Pre}}\big] \bigg ) -
\bigg( E\big[Y_u  \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Post}}\big]-
E\big[Y_u  \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Pre}}\big]\bigg)\\
&amp;=\bigg ( \underbrace{E\big[Y^1_j  \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Post}}\big] - E\big[Y^0_j]  \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Pre}}\big]\bigg)-
\bigg(E\big[Y^0_u  \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Post}}\big]-
E\big[Y^0_u  \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Pre}}\big]}_{\text{Switching equation}} \bigg)\\
&amp;+ \underbrace{E\big[Y_j^0  \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Post}}\big] - E\big[Y^0_j  \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Post}}\big]}_{\text{Adding zero}}\\
&amp;=\underbrace{E\big[Y^1_j  \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Post}}\big] - E\big[Y^0_j  \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Post}}\big]}_{\text{ATT}} \\
&amp;+\bigg [ \underbrace{E\big[Y^0_j  \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Post}}\big] - E\big[Y^0_j  \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Pre}}\big]\bigg]-
\bigg [E\big[Y^0_U  \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Post}}\big] -
E\big[Y_U^0  \mathop{\mathrm{\,\vert\,}}\mathop{\mathrm{Pre}}\big] }_{\text{Non-parallel trends bias in $2\times 2$ case}} \bigg ]
\end{align}\]</span>
This can be rewritten even more compactly as:
<span class="math display">\[\begin{align}
\widehat{\delta}^{2\times 2}_{kU} = ATT_{\mathop{\mathrm{Post}},j} + \underbrace{\Delta Y^0_{\mathop{\mathrm{Post}},\mathop{\mathrm{Pre}},j} -
\Delta Y^0_{\mathop{\mathrm{Post}},\mathop{\mathrm{Pre}}, U}}_{\text{Selection bias!}}
\end{align}\]</span>
The <span class="math inline">\(2\times 2\)</span> DD can be expressed as the sum of the ATT itself <em>plus</em> a parallel trends assumption, and without parallel trends, the estimator is biased. Ask yourself—which of these two differences in the parallel trends assumption is counterfactual, <span class="math inline">\(\Delta Y^0_{\mathop{\mathrm{Post}},\mathop{\mathrm{Pre}},j}\)</span> or <span class="math inline">\(\Delta Y^0_{\mathop{\mathrm{Post}},\mathop{\mathrm{Pre}}, U}\)</span>? Which one is observed, in other words, and which one is not observed? Look and see if you can figure it out from this drawing in Figure <a href="ch8.html#fig:bacon4">9.19</a>.</p>
<div class="figure">
<span id="fig:bacon4"></span>
<img src="graphics/nonparallel_counterfactual_trends.jpg" alt="Visualization of parallel trends." width="100%"><p class="caption">
Figure 9.19: Visualization of parallel trends.
</p>
</div>
<p>Only if these are parallel—the counterfactual trend <em>and</em> the observable trend—does the selection bias term zero out and ATT is identified. But let’s keep looking within the decomposition, as we aren’t done. The other two <span class="math inline">\(2\times 2\)</span>s need to be defined since they appear in Bacon’s decomposition also. And they are:
<span class="math display">\[\begin{align}
\widehat{\delta}^{2\times 2}_{kU} &amp;= ATT_k{\mathop{\mathrm{Post}}} + \Delta Y^0_k(\mathop{\mathrm{Post}}(k),\mathop{\mathrm{Pre}}(k)) - \Delta Y^0_U(\mathop{\mathrm{Post}}(k),\mathop{\mathrm{Pre}})
\\
\widehat{\delta}^{2\times 2}_{kl} &amp;= ATT_k(MID) + \Delta Y^0_k(MID,\mathop{\mathrm{Pre}}) - \Delta Y^0_l(MID, \mathop{\mathrm{Pre}})
\end{align}\]</span>
These look the same because you’re always comparing the treated unit with an untreated unit (though in the second case it’s just that they haven’t been treated <em>yet</em>).</p>
<p>But what about the <span class="math inline">\(2\times 2\)</span> that compared the late groups to the already-treated earlier groups? With a lot of substitutions like we did we get:
<span class="math display">\[\begin{align}
\widehat{\delta}^{2\times 2}_{lk} &amp;=
ATT_{l,\mathop{\mathrm{Post}}(l)} \nonumber
\\
&amp;+ \underbrace{\Delta Y^0_l(\mathop{\mathrm{Post}}(l),MID) - \Delta Y^0_k (\mathop{\mathrm{Post}}(l), MID)}_{\text{Parallel-trends bias}} \nonumber
\\
&amp; - \underbrace{(ATT_k(\mathop{\mathrm{Post}}) - ATT_k(Mid))}_{\text{Heterogeneity in time bias!}}
\end{align}\]</span>
I find it interesting our earlier decomposition of the simple difference in means into <span class="math inline">\(ATE\)</span> <span class="math inline">\(+\)</span> selection bias <span class="math inline">\(+\)</span> heterogeneity treatment effects bias resembles the decomposition of the late to early <span class="math inline">\(2\times 2\)</span> DD.</p>
<p>The first line is the <span class="math inline">\(ATT\)</span> that we desperately hope to identify. The selection bias zeroes out insofar as <span class="math inline">\(Y^0\)</span> for <span class="math inline">\(k\)</span> and <span class="math inline">\(l\)</span> has the same parallel trends from <span class="math inline">\(mid\)</span> to <span class="math inline">\(post\)</span> period. And the treatment effects bias in the third line zeroes out <em>so long as</em> there are constant treatment effects for a group <em>over time</em>. But if there is heterogeneity in time for a group, then the two <span class="math inline">\(ATT\)</span> terms will not be the same, and therefore will not zero out.</p>
<p>But we can sign the bias if we are willing to assume monotonicity, which means the <span class="math inline">\(mid\)</span> term is smaller in absolute value than the <span class="math inline">\(post\)</span> term. Under monotonicity, the interior of the parentheses in the third line is positive, and therefore the bias is negative. For positive ATT, this will bias the effects towards zero, and for negative ATT, it will cause the estimated ATT to become even more negative.</p>
<p>Let’s pause and collect these terms. The decomposition formula for DD is:
<span class="math display">\[\begin{align}
\widehat{\delta}^{DD} = \sum_{k \neq U} s_{kU}\widehat{\delta}_{kU}^{2\times 2} + \sum_{k \neq U} \sum_{l&gt;k} s_{kl} \bigg[ \mu_{kl}\widehat{\delta}_{kl}^{2\times 2,k} + (1-\mu_{kl}) \widehat{\delta}_{kl}^{2\times 2,l} \bigg]
\end{align}\]</span>
We will substitute the following three expressions into that formula.
<span class="math display">\[\begin{align}
\widehat{\delta}_{kU}^{2\times 2} &amp;= ATT_k(\mathop{\mathrm{Post}})+\Delta Y_l^0(\mathop{\mathrm{Post}},\mathop{\mathrm{Pre}})-
\Delta Y_U^0(\mathop{\mathrm{Post}},\mathop{\mathrm{Pre}})
\\
\widehat{\delta}_{kl}^{2\times 2,k} &amp;=ATT_k(  \mathop{\mathrm{\,\vert\,}})+\Delta Y_l^0(  \mathop{\mathrm{\,\vert\,}},\mathop{\mathrm{Pre}})-\Delta Y_l^0(  \mathop{\mathrm{\,\vert\,}}, \mathop{\mathrm{Pre}})
\\
\widehat{\delta}^{2\times 2,l}_{lk} &amp;=ATT_{l}\mathop{\mathrm{Post}}(l)+\Delta Y^0_l(\mathop{\mathrm{Post}}(l),  \mathop{\mathrm{\,\vert\,}})-\Delta Y^0_k (\mathop{\mathrm{Post}}(l),   \mathop{\mathrm{\,\vert\,}})
\\
&amp;- (ATT_k(\mathop{\mathrm{Post}})-ATT_k(  \mathop{\mathrm{\,\vert\,}}))
\end{align}\]</span>
Substituting all three terms into the decomposition formula is a bit overwhelming, so let’s simplify the notation. The estimated DD parameter is equal to: <span class="math display">\[p\lim\widehat{\delta}^{DD}_{n\to\infty} =
VWATT + VWCT - \Delta ATT\]</span> In the next few sections, I discuss each individual element of this expression.</p>
</div>
<div id="variance-weighted-att" class="section level3" number="9.6.3">
<h3>
<span class="header-section-number">9.6.3</span> Variance weighted ATT<a class="anchor" aria-label="anchor" href="#variance-weighted-att"><i class="fas fa-link"></i></a>
</h3>
<p>We begin by discussing the variance weighted average treatment effect on the treatment group, or <span class="math inline">\(VWATT\)</span>. Its unpacked expression is:
<span class="math display">\[\begin{align}
VWATT &amp;=\sum_{k\neq U}\sigma_{kU}ATT_k(\mathop{\mathrm{Post}}(k))
\\
&amp;+\sum_{k \neq U} \sum_{l&gt;k} \sigma_{kl} \bigg [ \mu_{kl} ATT_k (  \mathop{\mathrm{\,\vert\,}})+ (1-\mu_{kl}) ATT_{l} (POST(l)) \bigg ]
\end{align}\]</span>
where <span class="math inline">\(\sigma\)</span> is like <span class="math inline">\(s\)</span>, only population terms not samples. Notice that the VWATT simply contains the three ATTs identified above, each of which was weighted by the weights contained in the decomposition formula. While these weights sum to one, that weighting is irrelevant if the ATT are identical.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Heterogeneity in ATT across &lt;span class="math inline"&gt;\(k\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; is not the source of any biases. Only heterogeneity over time for &lt;span class="math inline"&gt;\(k\)&lt;/span&gt; or &lt;span class="math inline"&gt;\(l\)&lt;/span&gt;’s ATT introduces bias. We will discuss this in more detail later.&lt;/p&gt;'><sup>151</sup></a></p>
<p>When I learned that the DD coefficient was a weighted average of all individual <span class="math inline">\(2\times 2\)</span>s, I was not terribly surprised. I may not have intuitively known that the weights were based on group shares and treatment variance, but I figured it was probably a weighted average nonetheless. I did not have that same experience, though, when I worked through the other two terms. I now turn to the other two terms: the VWCT and the <span class="math inline">\(\Delta ATT\)</span>.</p>
</div>
<div id="variance-weighted-common-trends" class="section level3" number="9.6.4">
<h3>
<span class="header-section-number">9.6.4</span> Variance weighted common trends<a class="anchor" aria-label="anchor" href="#variance-weighted-common-trends"><i class="fas fa-link"></i></a>
</h3>
<p>VWCT stands for variance weighted common trends. This is just the collection of non-parallel-trends biases we previously wrote out, but notice—identification requires <em>variance weighted</em> common trends to hold, which is actually a bit weaker than we thought before with identical trends. You get this with identical trends, but what <span class="citation">Goodman-Bacon (<a href="references.html#ref-Bacon2019" role="doc-biblioref">2019</a>)</span> shows us is that <em>technically</em> you don’t need identical trends because the weights can make it hold even if we don’t have exact parallel trends. Unfortunately, this is a bit of a pain to write out, but since it’s important, I will.
<span class="math display">\[\begin{align}
VWCT &amp;= \sum_{k \neq U} \sigma_{kU} \bigg [ \Delta Y_k^0(\mathop{\mathrm{Post}}(k),\mathop{\mathrm{Pre}}) - \Delta Y_U^0(\mathop{\mathrm{Post}}(k),\mathop{\mathrm{Pre}}) \bigg ] \nonumber
\\
&amp;+ \sum_{k \neq U} \sum_{l&gt;k} \sigma_{kl} \bigg [ \mu_{kl} \{ \Delta Y^0_k(Mid,\mathop{\mathrm{Pre}}(k)) - \Delta Y_l^0(  \mathop{\mathrm{\,\vert\,}},\mathop{\mathrm{Pre}}(k))\} \nonumber
\\
&amp;+(1-\mu_{kl}) \{ \Delta Y^0_l(\mathop{\mathrm{Post}}(l),   \mathop{\mathrm{\,\vert\,}}) - \Delta Y^0_k(\mathop{\mathrm{Post}}(l),   \mathop{\mathrm{\,\vert\,}}) \} \bigg ]
\end{align}\]</span>
Notice that the VWCT term simply collects all the non-parallel-trend biases from the three <span class="math inline">\(2\times 2\)</span>s. One of the novelties, though, is that the non-parallel-trend biases are also weighted by the same weights used in the VATT.</p>
<p>This is actually a new insight. On the one hand, there are a lot of terms we need to be zero. On the other hand, it’s ironically a <em>weaker</em> identifying assumption strictly identical common trends as the weights can technically correct for unequal trends. VWCT will zero out with exact parallel trends and in those situations where the weights adjust the trends to zero out. This is good news (sort of).</p>
</div>
<div id="att-heterogeneity-within-time-bias" class="section level3" number="9.6.5">
<h3>
<span class="header-section-number">9.6.5</span> ATT heterogeneity within time bias<a class="anchor" aria-label="anchor" href="#att-heterogeneity-within-time-bias"><i class="fas fa-link"></i></a>
</h3>
<p>When we decomposed the simple difference in mean outcomes into the sum of the ATE, selection bias, and heterogeneous treatment effects bias, it really wasn’t a huge headache. That was because if the ATT differed from the ATU, then the simple difference in mean outcomes became the sum of ATT and selection bias, which was still an interesting parameter. But in the Bacon decomposition, ATT heterogeneity over time introduces bias that is not so benign. Let’s look at what happens when there is time-variant within-group treatment effects.
<span class="math display">\[\begin{align}
\Delta ATT = \sum_{k \neq U} \sum_{l&gt;k} (1 - \mu_{kl}) \Big[ ATT_k(\mathop{\mathrm{Post}}(l) - ATT_k(  \mathop{\mathrm{\,\vert\,}})) \Big]
\end{align}\]</span>
Heterogeneity in the ATT has two interpretations: you can have heterogeneous treatment effects <em>across</em> groups, and you can have heterogeneous treatment effects <em>within</em> groups over time. The <span class="math inline">\(\Delta ATT\)</span> is concerned with the latter only. The first case would be heterogeneity <em>across units</em> but not within groups. When there is heterogeneity across groups, then the VWATT is simply the average over group-specific ATTs weighted by a function of sample shares and treatment variance. There is no bias from this kind of heterogeneity.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Scattering the weights against the individual &lt;span class="math inline"&gt;\(2\times 2\)&lt;/span&gt;s can help reveal if the overall coefficient is driven by a few different &lt;span class="math inline"&gt;\(2\times 2\)&lt;/span&gt;s with large weights.&lt;/p&gt;'><sup>152</sup></a></p>
<p>But it’s the second case—when <span class="math inline">\(ATT\)</span> is constant across units but heterogeneous within groups over time—that things get a little worrisome. Time-varying treatment effects, even if they are identical across units, generate cross-group heterogeneity because of the differing post-treatment windows, and the fact that earlier-treated groups are serving as controls for later-treated groups. Let’s consider a case where the counterfactual outcomes are identical, but the treatment effect is a linear break in the trend (Figure <a href="ch8.html#fig:bacon5">9.20</a>). For instance, <span class="math inline">\(Y^1_{it} = Y^0_{it} + \theta (t-t^*_1+1)\)</span> similar to <span class="citation">Meer and West (<a href="references.html#ref-Meer2016" role="doc-biblioref">2016</a>)</span>.</p>
<div class="figure">
<span id="fig:bacon5"></span>
<img src="graphics/bacon_figure3.jpg" alt="Within-group heterogeneity in the $ATT$. Reprinted from @Bacon2019" width="100%"><p class="caption">
Figure 9.20: Within-group heterogeneity in the <span class="math inline">\(ATT\)</span>. Reprinted from <span class="citation">Goodman-Bacon (<a href="references.html#ref-Bacon2019" role="doc-biblioref">2019</a>)</span>
</p>
</div>
<p>Notice how the first <span class="math inline">\(2\times 2\)</span> uses the later group as its control in the middle period, <em>but</em> in the late period, the later-treated group is using the earlier treated as its control. When is this a problem?</p>
<p>It’s a problem if there are a lot of those <span class="math inline">\(2\times 2\)</span>s or if their weights are large. If they are negligible portions of the estimate, then even if it exists, then given their weights are small (as group shares are also an important piece of the weighting not just the variance in treatment) the bias may be small. But let’s say that doesn’t hold. Then what is going on? The effect is biased because the control group is experiencing a trend in outcomes (e.g., heterogeneous treatment effects), and this bias feeds through to the later <span class="math inline">\(2\times 2\)</span> according to the size of the weights, <span class="math inline">\((1-\mu_{kl})\)</span>. We will need to correct for this if our plan is to stick with the twoway fixed effects estimator.</p>
<p>Now it’s time to use what we’ve learned. Let’s look at an interesting and important paper by <span class="citation">Cheng and Hoekstra (<a href="references.html#ref-Cheng2013" role="doc-biblioref">2013</a>)</span> to both learn more about a DD paper and replicate it using event studies and the Bacon decomposition.</p>
</div>
<div id="castle-doctrine-statutes-and-homicides" class="section level3" number="9.6.6">
<h3>
<span class="header-section-number">9.6.6</span> Castle-doctrine statutes and homicides<a class="anchor" aria-label="anchor" href="#castle-doctrine-statutes-and-homicides"><i class="fas fa-link"></i></a>
</h3>
<p><span class="citation">Cheng and Hoekstra (<a href="references.html#ref-Cheng2013" role="doc-biblioref">2013</a>)</span> evaluated the impact that a gun reform had on violence and to illustrate various principles and practices regarding differential timing. I’d like to discuss those principles in the context of this paper. This next section will discuss, extend, and replicate various parts of this study.</p>
<p>Trayvon Benjamin Martin was a 17-year-old African-American young man when George Zimmerman shot and killed him in Sanford, Florida, on February 26, 2012. Martin was walking home alone from a convenience store when Zimmerman spotted him, followed him from a distance, and reported him to the police. He said he found Martin’s behavior “suspicious,” and though police officers urged Zimmerman to stay back, Zimmerman stalked and eventually provoked Martin. An altercation occurred and Zimmerman fatally shot Martin. Zimmerman claimed self-defense and was nonetheless charged with Martin’s death. A jury acquitted him of second-degree murder and of manslaughter.</p>
<p>Zimmerman’s actions were interpreted by the jury to be legal because in 2005, Florida reformed when and where lethal self-defense could be used. Whereas once lethal self-defense was only legal inside the home, a new law, “Stand Your Ground,” had extended that right to other public places. Between 2000 and 2010, twenty-one states explicitly expanded the castle-doctrine statute by extending the places outside the home where lethal force could be legally used.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;These laws are called castle-doctrine statutes because the home—where lethal self-defense had been protected—is considered one’s castle.&lt;/p&gt;"><sup>153</sup></a> These states had removed a long-standing tradition in the common law that placed the duty to retreat from danger on the victim. After these reforms, though, victims no longer had a duty to retreat in public places if they felt threatened; they could retaliate in lethal self-defense.</p>
<p>Other changes were also made. In some states, individuals who used lethal force outside the home were <em>assumed</em> to be reasonably afraid. Thus, a prosecutor would have to prove fear was not reasonable, allegedly an almost impossible task. Civil liability for those acting under these expansions was also removed. As civil liability is a lower threshold of guilt than criminal guilt, this effectively removed the remaining constraint that might keep someone from using lethal force outside the home.</p>
<p>From an economic perspective, these reforms lowered the cost of killing someone. One could use lethal self-defense in situations from which they had previously been barred. And as there was no civil liability, the expected cost of killing someone was now lower. Thus, insofar as people are sensitive to incentives, then depending on the elasticities of lethal self-defense with respect to cost, we expect an increase in lethal violence for the marginal victim. The reforms may have, in other words, caused homicides to rise.</p>
<p>One can divide lethal force into true and false positives. The true positive use of lethal force would be those situations in which, had the person not used lethal force, he or she would have been murdered. Thus, the true positive case of lethal force is simply a transfer of one life (the offender) for another (the defender). This is tragic, but official statistics would not record an net increase in homicides relative to the counterfactual—only which person had been killed. But a false positive causes a net increase in homicides relative to the counterfactual. Some arguments can escalate unnecessarily, and yet under common law, the duty to retreat would have defused the situation before it spilled over into lethal force. Now, though, under these castle-doctrine reforms, that safety valve is removed, and thus a killing occurs that would not have in counterfactual, leading to a net increase in homicides.</p>
<p>But that is not the only possible impact of the reforms—deterrence of violence is also a possibility under these reforms. In <span class="citation">Lott and Mustard (<a href="references.html#ref-Lott1997" role="doc-biblioref">1997</a>)</span>, the authors found that concealed-carry laws reduced violence. They suggested this was caused by deterrence—thinking someone may be carrying a concealed weapon, the rational criminal is deterred from committing a crime. Deterrence dates back to <span class="citation">Becker (<a href="references.html#ref-Becker1968" role="doc-biblioref">1968</a>)</span> and Jeremy Bentham before him. Expanding the arenas where lethal force could be used could also deter crime. Since this theoretical possibility depends crucially on key elasticities, which may in fact be zero, deterrence from expanding where guns can be used to kill someone is ultimately an empirical question.</p>
<p><span class="citation">Cheng and Hoekstra (<a href="references.html#ref-Cheng2013" role="doc-biblioref">2013</a>)</span> chose a difference-in-differences design for their project where the castle doctrine law was the treatment and timing was differential across states. Their estimatingequation was
<span class="math display">\[ 
Y_{it}=\alpha + \delta D_{it} + \gamma X_{it} + \sigma_i + \tau_t + \varepsilon_{it}
\]</span>
where <span class="math inline">\(D_{it}\)</span> is the treatment parameter. They estimated this equation using a standard twoway fixed effects model as well as count models. Ordinarily, the treatment parameter will be a 0 or 1, but in <span class="citation">Cheng and Hoekstra (<a href="references.html#ref-Cheng2013" role="doc-biblioref">2013</a>)</span>, it’s a variable ranging from 0 to 1, because some states get the law change mid-year. So if they got the law in July, then <span class="math inline">\(D_{it}\)</span> equals 0 before the year of adoption, 0.5 in the year of adoption and 1 thereafter. The <span class="math inline">\(X_{it}\)</span> variable included a particular kind of control that they called “region-by-year fixed effects,” which was a vector of dummies for the census region to which the state belonged interacted with each year fixed effect. This was done so that explicit counterfactuals were forced to come from within the same census region.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;This would violate SUTVA insofar as gun violence spills over to a neighboring state when the own state passes a reform.&lt;/p&gt;"><sup>154</sup></a> As the results are not dramatically different between their twoway fixed effects and count models, I will tend to emphasize results from the twoway fixed effects.</p>
<p>The data they used is somewhat standard in crime studies. They used the FBI Uniform Crime Reports Summary Part I files from 2000 to 2010. The FBI Uniform Crime Reports is a harmonized data set on eight “index” crimes collected from voluntarily participating police agencies across the country. Participation is high and the data goes back many decades, making it attractive for many contemporary questions regarding the crime policy. Crimes were converted into rates, or “offenses per 100,000 population.”</p>
<p><span class="citation">Cheng and Hoekstra (<a href="references.html#ref-Cheng2013" role="doc-biblioref">2013</a>)</span> rhetorically open their study with a series of simple placebos to check whether the reforms were spuriously correlated with crime trends more generally. Since oftentimes many crimes are correlated because of unobserved factors, this has some appeal, as it rules out the possibility that the laws were simply being adopted in areas where crime rates were already rising. For their falsifications they chose motor vehicle thefts and larcenies, neither of which, they reasoned, should be credibly connected to lowering the cost of using lethal force in public.</p>
<p>There are so many regression coefficients in Table <a href="ch8.html#tab:castle-false">9.5</a> because applied microeconomists like to report results under increasingly restrictive models. In this case, each column is a new regression with additional controls such as additional fixed-effects specifications, time-varying controls, a one-year lead to check on the pre-treatment differences in outcomes, and state-specific trends. As you can see, many of these coefficients are very small, and because they are small, even large standard errors yield a range of estimates that are still not very large.</p>
<p>Next they look at what they consider to be crimes that might be deterred if policy created a credible threat of lethal retaliation in public: burglary, robbery, and aggravated assault.</p>
<p>Insofar as castle doctrine has a deterrence effect, then we would expect a <em>negative</em> effect of the law on offenses. But all of the regressions shown in Table <a href="ch8.html#tab:castle-deterrence">9.6</a> are actually positive, and very few are significant even still. So the authors conclude they cannot detect any deterrence—which does not mean it didn’t happen; just that they cannot reject the null for large effects.</p>
<p>Now they move to their main results, which is interesting because it’s much more common for authors to lead with their main results. But the rhetoric of this paper is somewhat original in that respect. By this point, the reader has seen a lot of null effects from the laws and may be wondering, “What’s going on? This law isn’t spurious and isn’t causing deterrence. Why am I reading this paper?”</p>
<p>The first thing the authors did was show a series of figures showing the <em>raw data</em> on homicides for treatment and control states. This is always a challenge when working with differential timing, though. For instance, approximately twenty states adopted a castle-doctrine law from 2005 to 2010, but <em>not at the same time</em>. So how are you going to show this visually? What is the pre-treatment period, for instance, for the <em>control group</em> when there is differential timing? If one state adopts</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:castle-false">Table 9.5: </span> Falsification Tests: The effect of castle doctrine laws on larceny and motor vehicle theft.</caption>
<thead><tr class="header">
<th align="left">OLS – Weighted by State Population</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left"></td>
<td>(1)</td>
<td>(2)</td>
<td>(3)</td>
<td>(4)</td>
<td>(5)</td>
<td>(6)</td>
</tr>
<tr class="even">
<td align="left"><strong>Panel A. Larceny</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td align="left"><em>Log(Larceny Rate)</em></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td align="left">Castle Doctrine</td>
<td>0.00300</td>
<td><span class="math inline">\(-0.00600\)</span></td>
<td><span class="math inline">\(-0.00910\)</span></td>
<td><span class="math inline">\(-0.0858\)</span></td>
<td><span class="math inline">\(-0.00401\)</span></td>
<td><span class="math inline">\(-0.00284\)</span></td>
</tr>
<tr class="odd">
<td align="left">Law</td>
<td>(0.0161)</td>
<td>(0.0147)</td>
<td>(0.0139)</td>
<td>(0.0139)</td>
<td>(0.0128)</td>
<td>(0.0180)</td>
</tr>
<tr class="even">
<td align="left">0 to 2 years before adoption of castle doctrine law</td>
<td></td>
<td></td>
<td></td>
<td>0.00112</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td></td>
<td></td>
<td></td>
<td>(0.0105)</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td align="left">Observation</td>
<td>550</td>
<td>550</td>
<td>550</td>
<td>550</td>
<td>550</td>
<td>550</td>
</tr>
<tr class="odd">
<td align="left"><strong>Panel B. Motor</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td align="left"><em>Log(Motor Vehicle Theft Rate)</em></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td align="left">Castle Doctrine</td>
<td>0.0517</td>
<td><span class="math inline">\(-0.0389\)</span></td>
<td><span class="math inline">\(-0.0252\)</span></td>
<td><span class="math inline">\(-0.0294\)</span></td>
<td><span class="math inline">\(-0.0165\)</span></td>
<td><span class="math inline">\(-0.00708\)</span></td>
</tr>
<tr class="even">
<td align="left">Law</td>
<td>(0.0563)</td>
<td>(0.448)</td>
<td>(0.0396)</td>
<td>(0.0469)</td>
<td>(0.0354)</td>
<td>(0.0372)</td>
</tr>
<tr class="odd">
<td align="left">0 to 2 years before adoption of castle doctrine law</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td><span class="math inline">\(-0.00896\)</span></td>
<td></td>
</tr>
<tr class="even">
<td align="left"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>(0.0216)</td>
<td></td>
</tr>
<tr class="odd">
<td align="left">Observation</td>
<td>550</td>
<td>550</td>
<td>550</td>
<td>550</td>
<td>550</td>
<td>550</td>
</tr>
<tr class="even">
<td align="left">State and year fixed effects</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="odd">
<td align="left">Region-by-year fixed effects</td>
<td></td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="even">
<td align="left">Time-varying controls</td>
<td></td>
<td></td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="odd">
<td align="left">Controls for larceny or motor theft</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Yes</td>
<td></td>
</tr>
<tr class="even">
<td align="left">State-specific linear time trends</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Yes</td>
</tr>
</tbody>
</table></div>
<p class="footnote">
Each column in each panel represents a separate regression. The unit of observation is state-year. Robust standard errors are clustered at the state level. Time-varying controls include policing and incarceration rates, welfare and public assistance spending, median income, poverty rate, unemployment rate, and demographics. <span class="math inline">\(^{*}\)</span> Significant at the 10 percent level. <span class="math inline">\(^{**}\)</span> Significant at the 5 percent level. <span class="math inline">\(^{***}\)</span> Significant at the 1 percent level.
</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:castle-deterrence">Table 9.6: </span> The deterrence effects of castle-doctrine laws: Burglary, robbery, and aggravated assault.</caption>
<thead><tr class="header">
<th align="left">OLS – Weighted by State Population</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left"></td>
<td>(1)</td>
<td>(2)</td>
<td>(3)</td>
<td>(4)</td>
<td>(5)</td>
<td>(6)</td>
</tr>
<tr class="even">
<td align="left"><strong>Panel A. Burglary</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td align="left"><em>Log(Burglary Rate)</em></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td align="left">Castle-doctrine law</td>
<td>0.0780<span class="math inline">\(^{***}\)</span>
</td>
<td>0.0290</td>
<td>0.0223</td>
<td>0.0181</td>
<td>0.0327<span class="math inline">\(^*\)</span>
</td>
<td>0.0237</td>
</tr>
<tr class="odd">
<td align="left">0 to 2 years before adoption of castle-doctrine law</td>
<td>(0.0255)</td>
<td>(0.0236)</td>
<td>(0.0223)</td>
<td>(0.0265)</td>
<td>(0.0165)</td>
<td>(0.0207)</td>
</tr>
<tr class="even">
<td align="left"></td>
<td></td>
<td></td>
<td></td>
<td><span class="math inline">\(-0.009606\)</span></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td></td>
<td></td>
<td></td>
<td>(0.0133)</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td align="left"><strong>Panel B. Robbery</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td align="left"><em>Log(Robery Rate)</em></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td align="left">Castle-doctrine law</td>
<td>0.0408</td>
<td>0.0344</td>
<td>0.0262</td>
<td>0.0197</td>
<td>0.0376<span class="math inline">\(^{**}\)</span>
</td>
<td>0.0515<span class="math inline">\(^*\)</span>
</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td>(0.0254)</td>
<td>(0.0224)</td>
<td>(0.0229)</td>
<td>(0.0257)</td>
<td>(0.0181)</td>
<td>(0.0274)</td>
</tr>
<tr class="even">
<td align="left">0 to 2 years before adoption of castle-doctrine law</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td><span class="math inline">\(-0.0138\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>(0.0153)</td>
<td></td>
</tr>
<tr class="even">
<td align="left"><strong>Panel C. Aggravated</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td align="left"><em>Log(Aggrevated Assault Rate)</em></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td align="left">Castle-doctrine law</td>
<td>0.0434</td>
<td>0.0397</td>
<td>0.0372</td>
<td>0.0330</td>
<td>0.0424</td>
<td>0.0414</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td>(0.0387)</td>
<td>(0.0407)</td>
<td>(0.0319)</td>
<td>(0.0367)</td>
<td>(0.0291)</td>
<td>(0.0285)</td>
</tr>
<tr class="even">
<td align="left">0 to 2 years before adoption of castle-doctrine law</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td><span class="math inline">\(-0.00897\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>(0.0216)</td>
<td></td>
</tr>
<tr class="even">
<td align="left">Observation</td>
<td>550</td>
<td>550</td>
<td>550</td>
<td>550</td>
<td>550</td>
<td>550</td>
</tr>
<tr class="odd">
<td align="left">State and year fixed effects</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="even">
<td align="left">Region-by-year fixed effects</td>
<td></td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="odd">
<td align="left">Time-varying controls</td>
<td></td>
<td></td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="even">
<td align="left">Controls for larceny or motor theft</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Yes</td>
<td></td>
</tr>
<tr class="odd">
<td align="left">State-specific linear time trends</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Yes</td>
</tr>
</tbody>
</table></div>
<p class="footnote">
Each column in each panel represents a separate regression. The unit of observation is state-year. Robust standard errors are clustered at the state level. Time-varying controls include policing and incarceration rates, welfare and public assistance spending, median income, poverty rate, unemployment rate, and demographics. <span class="math inline">\(^{*}\)</span> Significant at the 10 percent level. <span class="math inline">\(^{**}\)</span> Significant at the 5 percent level. <span class="math inline">\(^{***}\)</span> Significant at the 1 percent level.
</p>
<p>in 2005, but another in 2006, then what precisely is the pre- and post-treatment for the control group? So that’s a bit of a challenge, and yet if you stick with our guiding principle that causal inference studies desperately need data visualization of the main effects, your job is to solve it with creativity and honesty to make beautiful figures. <span class="citation">Cheng and Hoekstra (<a href="references.html#ref-Cheng2013" role="doc-biblioref">2013</a>)</span> could’ve presented regression coefficients on leads and lags, as that is very commonly done, but knowing these authors firsthand, their preference is to give the reader pictures of the raw data to be as transparent as possible. Therefore, they showed multiple figures where each figure was a “treatment group” compared to all the “never-treated” units. Figure <a href="ch8.html#fig:Cheng3">9.21</a> shows the Florida case.</p>
<div class="figure">
<span id="fig:Cheng3"></span>
<img src="graphics/cheng5.jpg" alt="Raw data of log homicides per 100,000 for Florida vs never treated control states." width="100%"><p class="caption">
Figure 9.21: Raw data of log homicides per 100,000 for Florida vs never treated control states.
</p>
</div>
<p>Notice that before the passage of the law, the offenses are fairly flat for treatment and control. Obviously, as I’ve emphasized, this is not a direct <em>test</em> of the parallel-trends assumption. Parallel trends in the pre-treatment period are neither necessary nor sufficient. The identifying assumption, recall, is that of variance-weighted common trends, which are entirely based on parallel counterfactual trends, not pre-treatment trends. But researchers use parallel pre-treatment trends like a hunch that the counterfactual trends would have been parallel. In one sense, parallel pre-treatment rules out some obvious spurious factors that we should be worried about, such as the law adoption happening around the timing of a change, even if that’s simply nothing more than seemingly spurious factors like rising homicides. But that’s clearly not happening here—homicides weren’t diverging from controls pre-treatment. They were following a similar trajectory before Florida passed its law and <em>only then</em> did the trends converge. Notice that after 2005, which is when the law occurs, there’s a sizable jump in homicides. There are additional figures like this, but they all have this set up—they show a treatment <em>group</em> over time compared to the same “never-treated” group.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:castle-homicide">Table 9.7: </span> The Effect of Castle Doctrine Laws on Homicide</caption>
<tbody>
<tr class="odd">
<td align="left"><strong>Panel A. Homicide</strong></td>
<td align="right"><strong>Log(Homicide Rates)</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td align="left"><strong>OLS–Weights</strong></td>
<td align="right"><strong>1</strong></td>
<td><strong>2</strong></td>
<td><strong>3</strong></td>
<td><strong>4</strong></td>
<td><strong>5</strong></td>
<td><strong>6</strong></td>
</tr>
<tr class="odd">
<td align="left">Castle Doctrine Law</td>
<td align="right">0.0801<span class="math inline">\(^{**}\)</span>
</td>
<td>0.0946<span class="math inline">\(^{***}\)</span>
</td>
<td>0.0937<span class="math inline">\(^{***}\)</span>
</td>
<td>0.0955<span class="math inline">\(^{**}\)</span>
</td>
<td>0.0985<span class="math inline">\(^{***}\)</span>
</td>
<td>0.100<span class="math inline">\(^{**}\)</span>
</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right">(0.0342)</td>
<td>(0.0279)</td>
<td>(0.0290)</td>
<td>(0.0367)</td>
<td>(0.0299)</td>
<td>(0.0388)</td>
</tr>
<tr class="odd">
<td align="left">0 to 2 years before adoption of castle doctrine law</td>
<td align="right"></td>
<td></td>
<td></td>
<td></td>
<td>0.00398</td>
<td></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right"></td>
<td></td>
<td></td>
<td></td>
<td>(0.0222)</td>
<td></td>
</tr>
<tr class="odd">
<td align="left">Observation</td>
<td align="right">550</td>
<td>550</td>
<td>550</td>
<td>550</td>
<td>550</td>
<td>550</td>
</tr>
<tr class="even">
<td align="left">State and Year Fixed Effects</td>
<td align="right">Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="odd">
<td align="left">Region-by-year Fixed</td>
<td align="right"></td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="even">
<td align="left">Effects</td>
<td align="right"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td align="left">Time-Varying Controls</td>
<td align="right"></td>
<td></td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="even">
<td align="left">Controls for Larceny or Motor Theft</td>
<td align="right"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Yes</td>
</tr>
<tr class="odd">
<td align="left">State-specific Linear Time Trends</td>
<td align="right"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Yes</td>
</tr>
</tbody>
</table></div>
<p class="footnote">
Each column in each panel represents a separate regression. The unit of observation is state-year. Robust standard errors are clustered at the state level. Time-varying controls include policing and incarceration rates, welfare and public assistance spending, median income, poverty rate, unemployment rate, and demographics. <span class="math inline">\(^{**}\)</span> Significant at the 5 percent level. <span class="math inline">\(^{***}\)</span> Significant at the 1 percent level.
</p>
<p>Insofar as the cost of committing lethal force has fallen, then we expect to see more of it, which implies a positive coefficient on the estimated <span class="math inline">\(\delta\)</span> term assuming the heterogeneity bias we discussed earlier doesn’t cause the twoway fixed effects estimated coefficient to flip signs. It should be different from zero both statistically and in a meaningful magnitude. They present four separate types of specifications—three using OLS, one using negative binomial. But I will only report the weighted OLS regressions for the sake of space.</p>
<p>There’s a lot of information in Table <a href="ch8.html#tab:castle-homicide">9.7</a>, so let’s be sure not to get lost. First, all coefficients are positive and similar in magnitude—between 8% and 10% increases in homicides. Second, three of the four panels are almost entirely significant. It appears that the bulk of their evidence suggests the castle-doctrine statute caused an increase in homicides around 8%.</p>
<p>Not satisfied, the authors implemented a kind of randomization inference-based test. Specifically, they moved the eleven-year panel back in time covering 1960–2009 and estimated forty placebo “effects” of passing castle doctrine one to forty years earlier. When they did this, they found that the average effect from this exercise was essentially zero. Those results are summarized here. It appears there is something statistically unusual about the actual treatment profile compared to the placebo profiles, because the actual profile yields effect sizes larger than all but one case in any of the placebo regressions run.</p>
<div class="inline-table"><table class="table table-sm">
<caption>Randomization inference averages <span class="citation">(Cheng and Hoekstra <a href="references.html#ref-Cheng2013" role="doc-biblioref">2013</a>)</span>.</caption>
<thead><tr class="header">
<th align="left">Method</th>
<th align="center">Average estimate</th>
<th align="center">Estimates larger than actual estimate</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Weighted OLS</td>
<td align="center"><span class="math inline">\(-0.003\)</span></td>
<td align="center">0/40</td>
</tr>
<tr class="even">
<td align="left">Unweighted OLS</td>
<td align="center">0.001</td>
<td align="center">1/40</td>
</tr>
<tr class="odd">
<td align="left">Negative binomial</td>
<td align="center">0.001</td>
<td align="center">0/40</td>
</tr>
</tbody>
</table></div>
<p><span class="citation">Cheng and Hoekstra (<a href="references.html#ref-Cheng2013" role="doc-biblioref">2013</a>)</span> found no evidence that castle-doctrine laws deter violent offenses, but they did find that it increased homicides. An 8% net increase in homicide rates translates to around six hundred additional homicides per year across the twenty-one adopting states. Thinking back to to the killing of Trayvon Martin by George Zimmerman, one is left to wonder whether Trayvon might still be alive had Florida not passed Stand Your Ground. This kind of counterfactual reasoning can drive you crazy, because it is unanswerable—we simply don’t know, cannot know, and never will know the answer to counterfactual questions. The fundamental problem of causal inference states that we need to know what would have happened that fateful night without Stand Your Ground and compare that with what happened with Stand Your Ground to know what can and cannot be placed at the feet of that law. What we do know is that under certain assumptions related to the DD design, homicides were on net around 8%–10% higher than they would’ve been when compared against explicit counterfactuals. And while that doesn’t answer every question, it suggests that a nontrivial number of deaths can be blamed on laws similar to Stand Your Ground.</p>
</div>
<div id="replicating-cheng2013-sort-of" class="section level3" number="9.6.7">
<h3>
<span class="header-section-number">9.6.7</span> Replicating <span class="citation">Cheng and Hoekstra (<a href="references.html#ref-Cheng2013" role="doc-biblioref">2013</a>)</span>, sort of<a class="anchor" aria-label="anchor" href="#replicating-cheng2013-sort-of"><i class="fas fa-link"></i></a>
</h3>
<p>Now that we’ve discussed <span class="citation">Cheng and Hoekstra (<a href="references.html#ref-Cheng2013" role="doc-biblioref">2013</a>)</span>, I want to replicate it, or at least do some work on their data set to illustrate certain things that we’ve discussed, like event studies and the Bacon decomposition. This analysis will be slightly different from what they did, though, because their policy variable was on the interval <span class="math inline">\([0,1]\)</span> rather than being a pure dummy. That’s because they carefully defined their policy variable according to the month in which the law was passed (e.g., June) divided by a total of 12 months. So if a state passed the last in June, then they would assign a 0.5 in the first year, and a 1 thereafter. While there’s nothing wrong with that approach, I am going to use a dummy because it makes the event studies a bit easier to visualize, and the Bacon decomposition only works with dummy policy variables.</p>
<p>First, I will replicate his main homicide results from Panel A, column 6, of Figure <a href="ch8.html#fig:Cheng3">9.21</a>.</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/castle_1.do"><code>castle_1.do</code></a></em></p>
<div class="sourceCode" id="cb93"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb93-1"><a href="ch8.html#cb93-1" aria-hidden="true"></a><span class="kw">use</span> https:<span class="co">//github.com/scunning1975/mixtape/raw/master/castle.dta, clear</span></span>
<span id="cb93-2"><a href="ch8.html#cb93-2" aria-hidden="true"></a><span class="kw">set</span> <span class="dv">scheme</span> cleanplots</span>
<span id="cb93-3"><a href="ch8.html#cb93-3" aria-hidden="true"></a>* <span class="kw">ssc</span> install bacondecomp</span>
<span id="cb93-4"><a href="ch8.html#cb93-4" aria-hidden="true"></a></span>
<span id="cb93-5"><a href="ch8.html#cb93-5" aria-hidden="true"></a>* <span class="kw">define</span> <span class="kw">global</span> macros</span>
<span id="cb93-6"><a href="ch8.html#cb93-6" aria-hidden="true"></a><span class="kw">global</span> crime1 jhcitizen_c jhpolice_c murder homicide  robbery assault burglary larceny motor robbery_gun_r </span>
<span id="cb93-7"><a href="ch8.html#cb93-7" aria-hidden="true"></a><span class="kw">global</span> demo blackm_15_24 whitem_15_24 blackm_25_44 whitem_25_44 <span class="co">//demographics</span></span>
<span id="cb93-8"><a href="ch8.html#cb93-8" aria-hidden="true"></a><span class="kw">global</span> lintrend trend_1-trend_51 <span class="co">//state linear trend</span></span>
<span id="cb93-9"><a href="ch8.html#cb93-9" aria-hidden="true"></a><span class="kw">global</span> region r20001-r20104  <span class="co">//region-quarter fixed effects</span></span>
<span id="cb93-10"><a href="ch8.html#cb93-10" aria-hidden="true"></a><span class="kw">global</span> exocrime l_larceny l_motor <span class="co">// exogenous crime rates</span></span>
<span id="cb93-11"><a href="ch8.html#cb93-11" aria-hidden="true"></a><span class="kw">global</span> spending l_exp_subsidy l_exp_pubwelfare</span>
<span id="cb93-12"><a href="ch8.html#cb93-12" aria-hidden="true"></a><span class="kw">global</span> xvar l_police unemployrt poverty l_income l_prisoner l_lagprisoner <span class="ot">$demo</span> <span class="ot">$spending</span></span>
<span id="cb93-13"><a href="ch8.html#cb93-13" aria-hidden="true"></a></span>
<span id="cb93-14"><a href="ch8.html#cb93-14" aria-hidden="true"></a><span class="kw">label</span> <span class="kw">variable</span> <span class="kw">post</span> <span class="st">"Year of treatment"</span></span>
<span id="cb93-15"><a href="ch8.html#cb93-15" aria-hidden="true"></a><span class="kw">xi</span>: <span class="kw">xtreg</span> l_homicide i.<span class="fu">year</span> <span class="ot">$region</span> <span class="ot">$xvar</span> <span class="ot">$lintrend</span> <span class="kw">post</span> [<span class="kw">aweight</span>=popwt], <span class="kw">fe</span> <span class="kw">vce</span>(<span class="kw">cluster</span> sid)</span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/castle_1.R"><code>castle_1.R</code></a></em></p>
<div class="sourceCode" id="cb94"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">bacondecomp</span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://haven.tidyverse.org">haven</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/sgaure/lfe">lfe</a></span><span class="op">)</span>

<span class="va">read_data</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">df</span><span class="op">)</span>
<span class="op">{</span>
  <span class="va">full_path</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"https://raw.github.com/scunning1975/mixtape/master/"</span>, 
                     <span class="va">df</span>, sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span>
  <span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/read_dta.html">read_dta</a></span><span class="op">(</span><span class="va">full_path</span><span class="op">)</span>
  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">df</span><span class="op">)</span>
<span class="op">}</span>

<span class="va">castle</span> <span class="op">&lt;-</span> <span class="fu">read_data</span><span class="op">(</span><span class="st">"castle.dta"</span><span class="op">)</span>

<span class="co">#--- global variables</span>
<span class="va">crime1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"jhcitizen_c"</span>, <span class="st">"jhpolice_c"</span>, 
            <span class="st">"murder"</span>, <span class="st">"homicide"</span>, 
            <span class="st">"robbery"</span>, <span class="st">"assault"</span>, <span class="st">"burglary"</span>,
            <span class="st">"larceny"</span>, <span class="st">"motor"</span>, <span class="st">"robbery_gun_r"</span><span class="op">)</span>

<span class="va">demo</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"emo"</span>, <span class="st">"blackm_15_24"</span>, <span class="st">"whitem_15_24"</span>, 
          <span class="st">"blackm_25_44"</span>, <span class="st">"whitem_25_44"</span><span class="op">)</span>

<span class="co"># variables dropped to prevent colinearity</span>
<span class="va">dropped_vars</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"r20004"</span>, <span class="st">"r20014"</span>,
                  <span class="st">"r20024"</span>, <span class="st">"r20034"</span>,
                  <span class="st">"r20044"</span>, <span class="st">"r20054"</span>,
                  <span class="st">"r20064"</span>, <span class="st">"r20074"</span>,
                  <span class="st">"r20084"</span>, <span class="st">"r20094"</span>,
                  <span class="st">"r20101"</span>, <span class="st">"r20102"</span>, <span class="st">"r20103"</span>,
                  <span class="st">"r20104"</span>, <span class="st">"trend_9"</span>, <span class="st">"trend_46"</span>,
                  <span class="st">"trend_49"</span>, <span class="st">"trend_50"</span>, <span class="st">"trend_51"</span>
<span class="op">)</span>

<span class="va">lintrend</span> <span class="op">&lt;-</span> <span class="va">castle</span> <span class="op">%&gt;%</span>
    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="fu"><a href="https://tidyselect.r-lib.org/reference/starts_with.html">starts_with</a></span><span class="op">(</span><span class="st">"trend"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="va">colnames</span> <span class="op">%&gt;%</span> 
  <span class="co"># remove due to colinearity</span>
  <span class="fu"><a href="https://rdrr.io/r/base/subset.html">subset</a></span><span class="op">(</span><span class="va">.</span>,<span class="op">!</span> <span class="va">.</span> <span class="op">%in%</span> <span class="va">dropped_vars</span><span class="op">)</span> 

<span class="va">region</span> <span class="op">&lt;-</span> <span class="va">castle</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="fu"><a href="https://tidyselect.r-lib.org/reference/starts_with.html">starts_with</a></span><span class="op">(</span><span class="st">"r20"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="va">colnames</span> <span class="op">%&gt;%</span> 
  <span class="co"># remove due to colinearity</span>
  <span class="fu"><a href="https://rdrr.io/r/base/subset.html">subset</a></span><span class="op">(</span><span class="va">.</span>,<span class="op">!</span> <span class="va">.</span> <span class="op">%in%</span> <span class="va">dropped_vars</span><span class="op">)</span> 


<span class="va">exocrime</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"l_lacerny"</span>, <span class="st">"l_motor"</span><span class="op">)</span>
<span class="va">spending</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"l_exp_subsidy"</span>, <span class="st">"l_exp_pubwelfare"</span><span class="op">)</span>


<span class="va">xvar</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span>
  <span class="st">"blackm_15_24"</span>, <span class="st">"whitem_15_24"</span>, <span class="st">"blackm_25_44"</span>, <span class="st">"whitem_25_44"</span>,
  <span class="st">"l_exp_subsidy"</span>, <span class="st">"l_exp_pubwelfare"</span>,
  <span class="st">"l_police"</span>, <span class="st">"unemployrt"</span>, <span class="st">"poverty"</span>, 
  <span class="st">"l_income"</span>, <span class="st">"l_prisoner"</span>, <span class="st">"l_lagprisoner"</span>
<span class="op">)</span>

<span class="va">law</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"cdl"</span><span class="op">)</span>

<span class="va">dd_formula</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/formula.html">as.formula</a></span><span class="op">(</span>
  <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"l_homicide ~ "</span>,
        <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span>
          <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="va">xvar</span>, collapse <span class="op">=</span> <span class="st">" + "</span><span class="op">)</span>,
          <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="va">region</span>, collapse <span class="op">=</span> <span class="st">" + "</span><span class="op">)</span>,
          <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="va">lintrend</span>, collapse <span class="op">=</span> <span class="st">" + "</span><span class="op">)</span>,
          <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"post"</span>, collapse <span class="op">=</span> <span class="st">" + "</span><span class="op">)</span>, sep <span class="op">=</span> <span class="st">" + "</span><span class="op">)</span>,
        <span class="st">"| year + sid | 0 | sid"</span>
  <span class="op">)</span>
<span class="op">)</span>

<span class="co">#Fixed effect regression using post as treatment variable </span>
<span class="va">dd_reg</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/lfe/man/felm.html">felm</a></span><span class="op">(</span><span class="va">dd_formula</span>, weights <span class="op">=</span> <span class="va">castle</span><span class="op">$</span><span class="va">popwt</span>, data <span class="op">=</span> <span class="va">castle</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">dd_reg</span><span class="op">)</span></code></pre></div>
<p>Here we see the main result that castle doctrine expansions led to an approximately 10% increase in homicides. And if we use the post-dummy, which is essentially equal to 0 unless the state had fully covered castle doctrine expansions, then the effect is more like 7.6%.</p>
<p>But now, I’d like to go beyond their study to implement an event study. First, we need to define pre-treatment leads and lags. To do this, we use a “time_til” variable, which is the number of years until or after the state received the treatment. Using this variable, we then create the leads (which will be the years prior to treatment) and lags (the years post-treatment).</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/castle_2.do"><code>castle_2.do</code></a></em></p>
<div class="sourceCode" id="cb95"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb95-1"><a href="ch8.html#cb95-1" aria-hidden="true"></a>* Event study regression with the <span class="fu">year</span> <span class="kw">of</span> treatment (lag0) <span class="kw">as</span> the omitted category.</span>
<span id="cb95-2"><a href="ch8.html#cb95-2" aria-hidden="true"></a><span class="kw">xi</span>: <span class="kw">xtreg</span> l_homicide  i.<span class="fu">year</span> <span class="ot">$region</span> lead9 lead8 lead7 lead6 lead5 lead4 lead3 lead2 lead1 lag1-lag5 [<span class="kw">aweight</span>=popwt], <span class="kw">fe</span> <span class="kw">vce</span>(<span class="kw">cluster</span> sid)</span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/castle_2.R"><code>castle_2.R</code></a></em></p>
<div class="sourceCode" id="cb96"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">castle</span> <span class="op">&lt;-</span> <span class="va">castle</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>
    time_til <span class="op">=</span> <span class="va">year</span> <span class="op">-</span> <span class="va">treatment_date</span>,
    lead1 <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">time_til</span> <span class="op">==</span> <span class="op">-</span><span class="fl">1</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span>,
    lead2 <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">time_til</span> <span class="op">==</span> <span class="op">-</span><span class="fl">2</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span>,
    lead3 <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">time_til</span> <span class="op">==</span> <span class="op">-</span><span class="fl">3</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span>,
    lead4 <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">time_til</span> <span class="op">==</span> <span class="op">-</span><span class="fl">4</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span>,
    lead5 <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">time_til</span> <span class="op">==</span> <span class="op">-</span><span class="fl">5</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span>,
    lead6 <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">time_til</span> <span class="op">==</span> <span class="op">-</span><span class="fl">6</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span>,
    lead7 <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">time_til</span> <span class="op">==</span> <span class="op">-</span><span class="fl">7</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span>,
    lead8 <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">time_til</span> <span class="op">==</span> <span class="op">-</span><span class="fl">8</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span>,
    lead9 <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">time_til</span> <span class="op">==</span> <span class="op">-</span><span class="fl">9</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span>,
    
    lag0 <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">time_til</span> <span class="op">==</span> <span class="fl">0</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span>,
    lag1 <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">time_til</span> <span class="op">==</span> <span class="fl">1</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span>,
    lag2 <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">time_til</span> <span class="op">==</span> <span class="fl">2</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span>,
    lag3 <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">time_til</span> <span class="op">==</span> <span class="fl">3</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span>,
    lag4 <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">time_til</span> <span class="op">==</span> <span class="fl">4</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span>,
    lag5 <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">time_til</span> <span class="op">==</span> <span class="fl">5</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span>
  <span class="op">)</span>

<span class="va">event_study_formula</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/formula.html">as.formula</a></span><span class="op">(</span>
  <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"l_homicide ~ + "</span>,
        <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span>
          <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="va">region</span>, collapse <span class="op">=</span> <span class="st">" + "</span><span class="op">)</span>,
          <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"lead"</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">9</span>, sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span>, collapse <span class="op">=</span> <span class="st">" + "</span><span class="op">)</span>,
          <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"lag"</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">5</span>, sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span>, collapse <span class="op">=</span> <span class="st">" + "</span><span class="op">)</span>, sep <span class="op">=</span> <span class="st">" + "</span><span class="op">)</span>,
        <span class="st">"| year + state | 0 | sid"</span>
  <span class="op">)</span>,
<span class="op">)</span>

<span class="va">event_study_reg</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/lfe/man/felm.html">felm</a></span><span class="op">(</span><span class="va">event_study_formula</span>, weights <span class="op">=</span> <span class="va">castle</span><span class="op">$</span><span class="va">popwt</span>, data <span class="op">=</span> <span class="va">castle</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">event_study_reg</span><span class="op">)</span></code></pre></div>
<p>Our omitted category is the year of treatment, so all coefficients are with respect to that year. You can see from the coefficients on the leads that they are not statistically different from zero prior to treatment, except for leads 8 and 9, which may be because there are only three states with eight years prior to treatment, and one state with nine years prior to treatment. But in the years prior to treatment, leads 1 to 6 are equal to zero and statistically insignificant, although they do technically have large confidence intervals. The lags, on the other hand, are all positive and not too dissimilar from one another except for lag 5, which is around 17%.</p>
<p>Now it is customary to plot these event studies, so let’s do that now. I am going to show you an easy way and a longer way to do this. The longer way gives you ultimately more control over what exactly you want the event study to look like, but for a fast and dirty method, the easier way will suffice. For the easier way, you will need to install a program in Stata called <code>coefplot</code>, written by Ben Jann, author of <code>estout</code>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Ben Jann is a valuable contributor to the Stata community for creating several community ado packages, such as -estout- for making tables and -coefplot- for making pictures of regression coefficients.&lt;/p&gt;"><sup>155</sup></a></p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/castle_3.do"><code>castle_3.do</code></a></em></p>
<div class="sourceCode" id="cb97"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb97-1"><a href="ch8.html#cb97-1" aria-hidden="true"></a>* Plot the coefficients <span class="kw">using</span> coefplot</span>
<span id="cb97-2"><a href="ch8.html#cb97-2" aria-hidden="true"></a>* <span class="kw">ssc</span> install coefplot</span>
<span id="cb97-3"><a href="ch8.html#cb97-3" aria-hidden="true"></a></span>
<span id="cb97-4"><a href="ch8.html#cb97-4" aria-hidden="true"></a>coefplot, <span class="kw">keep</span>(lead9 lead8 lead7 lead6 lead5 lead4 lead3 lead2 lead1 lag1 lag2 lag3 lag4 lag5) <span class="kw">xlabel</span>(, angle(vertical)) <span class="kw">yline</span>(0) <span class="bn">xline</span>(9.5) vertical <span class="bn">msymbol</span>(D) mfcolor(<span class="bn">white</span>) ciopts(lwidth(*3) lcolor(*.6)) <span class="bn">mlabel</span> <span class="kw">format</span>(%9.3f) <span class="bn">mlabposition</span>(12) <span class="bn">mlabgap</span>(*2) <span class="bn">title</span>(Log Murder Rate) </span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/castle_3.R"><code>castle_3.R</code></a></em></p>
<div class="sourceCode" id="cb98"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># order of the coefficients for the plot</span>
<span class="va">plot_order</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"lead9"</span>, <span class="st">"lead8"</span>, <span class="st">"lead7"</span>, 
                <span class="st">"lead6"</span>, <span class="st">"lead5"</span>, <span class="st">"lead4"</span>, <span class="st">"lead3"</span>, 
                <span class="st">"lead2"</span>, <span class="st">"lead1"</span>, <span class="st">"lag1"</span>, 
                <span class="st">"lag2"</span>, <span class="st">"lag3"</span>, <span class="st">"lag4"</span>, <span class="st">"lag5"</span><span class="op">)</span>

<span class="co"># grab the clustered standard errors</span>
<span class="co"># and average coefficient estimates</span>
<span class="co"># from the regression, label them accordingly</span>
<span class="co"># add a zero'th lag for plotting purposes</span>
<span class="va">leadslags_plot</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble</a></span><span class="op">(</span>
  sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">event_study_reg</span><span class="op">$</span><span class="va">cse</span><span class="op">[</span><span class="va">plot_order</span><span class="op">]</span>, <span class="fl">0</span><span class="op">)</span>,
  mean <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">event_study_reg</span><span class="op">)</span><span class="op">[</span><span class="va">plot_order</span><span class="op">]</span>, <span class="fl">0</span><span class="op">)</span>,
  label <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">9</span>,<span class="op">-</span><span class="fl">8</span>,<span class="op">-</span><span class="fl">7</span>,<span class="op">-</span><span class="fl">6</span>, <span class="op">-</span><span class="fl">5</span>, <span class="op">-</span><span class="fl">4</span>, <span class="op">-</span><span class="fl">3</span>, <span class="op">-</span><span class="fl">2</span>, <span class="op">-</span><span class="fl">1</span>, <span class="fl">1</span>,<span class="fl">2</span>,<span class="fl">3</span>,<span class="fl">4</span>,<span class="fl">5</span>, <span class="fl">0</span><span class="op">)</span>
<span class="op">)</span>

<span class="co"># This version has a point-range at each</span>
<span class="co"># estimated lead or lag</span>
<span class="co"># comes down to stylistic preference at the</span>
<span class="co"># end of the day!</span>
<span class="va">leadslags_plot</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">label</span>, y <span class="op">=</span> <span class="va">mean</span>,
             ymin <span class="op">=</span> <span class="va">mean</span><span class="op">-</span><span class="fl">1.96</span><span class="op">*</span><span class="va">sd</span>, 
             ymax <span class="op">=</span> <span class="va">mean</span><span class="op">+</span><span class="fl">1.96</span><span class="op">*</span><span class="va">sd</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_hline</a></span><span class="op">(</span>yintercept <span class="op">=</span> <span class="fl">0.035169444</span>, color <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_linerange.html">geom_pointrange</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="st">"Years before and after castle doctrine expansion"</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"log(Homicide Rate)"</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_hline</a></span><span class="op">(</span>yintercept <span class="op">=</span> <span class="fl">0</span>,
             linetype <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_vline</a></span><span class="op">(</span>xintercept <span class="op">=</span> <span class="fl">0</span>,
             linetype <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span></code></pre></div>
<div class="figure">
<span id="fig:event-cheng1"></span>
<img src="graphics/event_cheng1.jpg" alt="Homicide event study plots using coefplot [@Cheng2013]." width="100%"><p class="caption">
Figure 9.22: Homicide event study plots using coefplot <span class="citation">(Cheng and Hoekstra <a href="references.html#ref-Cheng2013" role="doc-biblioref">2013</a>)</span>.
</p>
</div>
<p>Let’s look now at what this command created. As you can see in Figure <a href="ch8.html#fig:event-cheng1">9.22</a>, eight to nine years prior to treatment, treatment states have significantly lower levels of homicides, but as there are so few states that even have these values (one with <span class="math inline">\(-9\)</span> and three with <span class="math inline">\(-8\)</span>), we may want to disregard the relevance of these negative effects if for no other reason than that there are so few units in the dummy and we know from earlier that that can lead to very high overrejection rates <span class="citation">(MacKinnon and Webb <a href="references.html#ref-MacKinnon2017" role="doc-biblioref">2017</a>)</span>. Instead, notice that for the six years prior to treatment, there is virtually no difference between the treatment states and the control states.</p>
<p>But, after the year of treatment, that changes. Log murders begin rising, which is consistent with our post dummy that imposed zeros on all pre-treatment leads and required that the average effect post-treatment be a constant.</p>
<p>I promised to show you how to make this graph in a way that gave more flexibility, but you should be warned, this is a bit more cumbersome.</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/castle_4.do"><code>castle_4.do</code></a></em></p>
<div class="sourceCode" id="cb99"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb99-1"><a href="ch8.html#cb99-1" aria-hidden="true"></a><span class="kw">xi</span>: <span class="kw">xtreg</span> l_homicide  i.<span class="fu">year</span> <span class="ot">$region</span> <span class="ot">$xvar</span> <span class="ot">$lintrend</span> <span class="kw">post</span> [<span class="kw">aweight</span>=popwt], <span class="kw">fe</span> <span class="kw">vce</span>(<span class="kw">cluster</span> sid)</span>
<span id="cb99-2"><a href="ch8.html#cb99-2" aria-hidden="true"></a></span>
<span id="cb99-3"><a href="ch8.html#cb99-3" aria-hidden="true"></a><span class="kw">local</span> DDL = _b[<span class="kw">post</span>]</span>
<span id="cb99-4"><a href="ch8.html#cb99-4" aria-hidden="true"></a><span class="kw">local</span> DD : <span class="kw">display</span> %03.2f _b[<span class="kw">post</span>]</span>
<span id="cb99-5"><a href="ch8.html#cb99-5" aria-hidden="true"></a><span class="kw">local</span> DDSE : <span class="kw">display</span> %03.2f _se[<span class="kw">post</span>]</span>
<span id="cb99-6"><a href="ch8.html#cb99-6" aria-hidden="true"></a><span class="kw">local</span> DD1 = -0.10</span>
<span id="cb99-7"><a href="ch8.html#cb99-7" aria-hidden="true"></a></span>
<span id="cb99-8"><a href="ch8.html#cb99-8" aria-hidden="true"></a><span class="kw">xi</span>: <span class="kw">xtreg</span> l_homicide  i.<span class="fu">year</span> <span class="ot">$region</span> lead9 lead8 lead7 lead6 lead5 lead4 lead3 lead2 lead1 lag1-lag5 [<span class="kw">aweight</span>=popwt], <span class="kw">fe</span> <span class="kw">vce</span>(<span class="kw">cluster</span> sid)</span>
<span id="cb99-9"><a href="ch8.html#cb99-9" aria-hidden="true"></a></span>
<span id="cb99-10"><a href="ch8.html#cb99-10" aria-hidden="true"></a>outreg2 <span class="kw">using</span> <span class="st">"./eventstudy_levels.xls"</span>, <span class="kw">replace</span> <span class="kw">keep</span>(lead9 lead8 lead7 lead6 lead5 lead4 lead3 lead2 lead1 lag1-lag5) noparen noaster addstat(DD, <span class="ot">`DD'</span>, DDSE, <span class="ot">`DDSE'</span>)</span>
<span id="cb99-11"><a href="ch8.html#cb99-11" aria-hidden="true"></a></span>
<span id="cb99-12"><a href="ch8.html#cb99-12" aria-hidden="true"></a></span>
<span id="cb99-13"><a href="ch8.html#cb99-13" aria-hidden="true"></a>*Pull <span class="kw">in</span> the ES Coefs</span>
<span id="cb99-14"><a href="ch8.html#cb99-14" aria-hidden="true"></a><span class="kw">xmluse</span> <span class="st">"./eventstudy_levels.xls"</span>, <span class="kw">clear</span> cells(A3:B32) first</span>
<span id="cb99-15"><a href="ch8.html#cb99-15" aria-hidden="true"></a><span class="kw">replace</span> VARIABLES = <span class="fu">subinstr</span>(VARIABLES,<span class="st">"lead"</span>,<span class="st">""</span>,.) </span>
<span id="cb99-16"><a href="ch8.html#cb99-16" aria-hidden="true"></a><span class="kw">replace</span> VARIABLES = <span class="fu">subinstr</span>(VARIABLES,<span class="st">"lag"</span>,<span class="st">""</span>,.)  </span>
<span id="cb99-17"><a href="ch8.html#cb99-17" aria-hidden="true"></a><span class="kw">quietly</span> <span class="kw">destring</span> <span class="dt">_all</span>, <span class="kw">replace</span> ignore(<span class="st">","</span>)</span>
<span id="cb99-18"><a href="ch8.html#cb99-18" aria-hidden="true"></a><span class="kw">replace</span> VARIABLES = -9 <span class="kw">in</span> 2</span>
<span id="cb99-19"><a href="ch8.html#cb99-19" aria-hidden="true"></a><span class="kw">replace</span> VARIABLES = -8 <span class="kw">in</span> 4</span>
<span id="cb99-20"><a href="ch8.html#cb99-20" aria-hidden="true"></a><span class="kw">replace</span> VARIABLES = -7 <span class="kw">in</span> 6</span>
<span id="cb99-21"><a href="ch8.html#cb99-21" aria-hidden="true"></a><span class="kw">replace</span> VARIABLES = -6 <span class="kw">in</span> 8</span>
<span id="cb99-22"><a href="ch8.html#cb99-22" aria-hidden="true"></a><span class="kw">replace</span> VARIABLES = -5 <span class="kw">in</span> 10</span>
<span id="cb99-23"><a href="ch8.html#cb99-23" aria-hidden="true"></a><span class="kw">replace</span> VARIABLES = -4 <span class="kw">in</span> 12</span>
<span id="cb99-24"><a href="ch8.html#cb99-24" aria-hidden="true"></a><span class="kw">replace</span> VARIABLES = -3 <span class="kw">in</span> 14</span>
<span id="cb99-25"><a href="ch8.html#cb99-25" aria-hidden="true"></a><span class="kw">replace</span> VARIABLES = -2 <span class="kw">in</span> 16</span>
<span id="cb99-26"><a href="ch8.html#cb99-26" aria-hidden="true"></a><span class="kw">replace</span> VARIABLES = -1 <span class="kw">in</span> 18</span>
<span id="cb99-27"><a href="ch8.html#cb99-27" aria-hidden="true"></a><span class="kw">replace</span> VARIABLES = 1 <span class="kw">in</span> 20</span>
<span id="cb99-28"><a href="ch8.html#cb99-28" aria-hidden="true"></a><span class="kw">replace</span> VARIABLES = 2 <span class="kw">in</span> 22</span>
<span id="cb99-29"><a href="ch8.html#cb99-29" aria-hidden="true"></a><span class="kw">replace</span> VARIABLES = 3 <span class="kw">in</span> 24</span>
<span id="cb99-30"><a href="ch8.html#cb99-30" aria-hidden="true"></a><span class="kw">replace</span> VARIABLES = 4 <span class="kw">in</span> 26</span>
<span id="cb99-31"><a href="ch8.html#cb99-31" aria-hidden="true"></a><span class="kw">replace</span> VARIABLES = 5 <span class="kw">in</span> 28</span>
<span id="cb99-32"><a href="ch8.html#cb99-32" aria-hidden="true"></a><span class="kw">drop</span> <span class="kw">in</span> 1</span>
<span id="cb99-33"><a href="ch8.html#cb99-33" aria-hidden="true"></a><span class="kw">compress</span></span>
<span id="cb99-34"><a href="ch8.html#cb99-34" aria-hidden="true"></a><span class="kw">quietly</span> <span class="kw">destring</span> <span class="dt">_all</span>, <span class="kw">replace</span> ignore(<span class="st">","</span>)</span>
<span id="cb99-35"><a href="ch8.html#cb99-35" aria-hidden="true"></a><span class="kw">compress</span></span>
<span id="cb99-36"><a href="ch8.html#cb99-36" aria-hidden="true"></a></span>
<span id="cb99-37"><a href="ch8.html#cb99-37" aria-hidden="true"></a></span>
<span id="cb99-38"><a href="ch8.html#cb99-38" aria-hidden="true"></a></span>
<span id="cb99-39"><a href="ch8.html#cb99-39" aria-hidden="true"></a><span class="kw">ren</span> VARIABLES <span class="fu">exp</span></span>
<span id="cb99-40"><a href="ch8.html#cb99-40" aria-hidden="true"></a><span class="kw">gen</span> b = <span class="fu">exp</span>&lt;.</span>
<span id="cb99-41"><a href="ch8.html#cb99-41" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">exp</span> = -9 <span class="kw">in</span> 2 </span>
<span id="cb99-42"><a href="ch8.html#cb99-42" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">exp</span> = -8 <span class="kw">in</span> 4</span>
<span id="cb99-43"><a href="ch8.html#cb99-43" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">exp</span> = -7 <span class="kw">in</span> 6</span>
<span id="cb99-44"><a href="ch8.html#cb99-44" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">exp</span> = -6 <span class="kw">in</span> 8</span>
<span id="cb99-45"><a href="ch8.html#cb99-45" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">exp</span> = -5 <span class="kw">in</span> 10 </span>
<span id="cb99-46"><a href="ch8.html#cb99-46" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">exp</span> = -4 <span class="kw">in</span> 12</span>
<span id="cb99-47"><a href="ch8.html#cb99-47" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">exp</span> = -3 <span class="kw">in</span> 14</span>
<span id="cb99-48"><a href="ch8.html#cb99-48" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">exp</span> = -2 <span class="kw">in</span> 16</span>
<span id="cb99-49"><a href="ch8.html#cb99-49" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">exp</span> = -1 <span class="kw">in</span> 18</span>
<span id="cb99-50"><a href="ch8.html#cb99-50" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">exp</span> = 1 <span class="kw">in</span> 20</span>
<span id="cb99-51"><a href="ch8.html#cb99-51" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">exp</span> = 2 <span class="kw">in</span> 22</span>
<span id="cb99-52"><a href="ch8.html#cb99-52" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">exp</span> = 3 <span class="kw">in</span> 24</span>
<span id="cb99-53"><a href="ch8.html#cb99-53" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">exp</span> = 4 <span class="kw">in</span> 26</span>
<span id="cb99-54"><a href="ch8.html#cb99-54" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">exp</span> = 5 <span class="kw">in</span> 28</span>
<span id="cb99-55"><a href="ch8.html#cb99-55" aria-hidden="true"></a></span>
<span id="cb99-56"><a href="ch8.html#cb99-56" aria-hidden="true"></a>* Expand the dataset <span class="kw">by</span> one <span class="kw">more</span> observation so <span class="kw">as</span> to <span class="kw">include</span> the comparison <span class="fu">year</span></span>
<span id="cb99-57"><a href="ch8.html#cb99-57" aria-hidden="true"></a><span class="kw">local</span> <span class="kw">obs</span> =_N+1</span>
<span id="cb99-58"><a href="ch8.html#cb99-58" aria-hidden="true"></a><span class="kw">set</span> <span class="kw">obs</span> <span class="ot">`obs'</span></span>
<span id="cb99-59"><a href="ch8.html#cb99-59" aria-hidden="true"></a><span class="kw">for</span> <span class="kw">var</span> <span class="dt">_all</span>: <span class="kw">replace</span> X = 0 <span class="kw">in</span> <span class="ot">`obs'</span></span>
<span id="cb99-60"><a href="ch8.html#cb99-60" aria-hidden="true"></a><span class="kw">replace</span> b = 1 <span class="kw">in</span> <span class="ot">`obs'</span></span>
<span id="cb99-61"><a href="ch8.html#cb99-61" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">exp</span> = 0 <span class="kw">in</span> <span class="ot">`obs'</span></span>
<span id="cb99-62"><a href="ch8.html#cb99-62" aria-hidden="true"></a><span class="kw">keep</span> <span class="fu">exp</span> l_homicide b </span>
<span id="cb99-63"><a href="ch8.html#cb99-63" aria-hidden="true"></a><span class="kw">set</span> <span class="kw">obs</span> 30</span>
<span id="cb99-64"><a href="ch8.html#cb99-64" aria-hidden="true"></a><span class="kw">foreach</span> x <span class="kw">of</span> <span class="kw">varlist</span> <span class="fu">exp</span> l_homicide b {</span>
<span id="cb99-65"><a href="ch8.html#cb99-65" aria-hidden="true"></a>    <span class="kw">replace</span> <span class="ot">`x'</span>=0 <span class="kw">in</span> 30</span>
<span id="cb99-66"><a href="ch8.html#cb99-66" aria-hidden="true"></a>    }</span>
<span id="cb99-67"><a href="ch8.html#cb99-67" aria-hidden="true"></a><span class="kw">reshape</span> <span class="kw">wide</span> l_homicide, i(<span class="fu">exp</span>) j(b)</span>
<span id="cb99-68"><a href="ch8.html#cb99-68" aria-hidden="true"></a></span>
<span id="cb99-69"><a href="ch8.html#cb99-69" aria-hidden="true"></a></span>
<span id="cb99-70"><a href="ch8.html#cb99-70" aria-hidden="true"></a>* Create the confidence intervals</span>
<span id="cb99-71"><a href="ch8.html#cb99-71" aria-hidden="true"></a>cap <span class="kw">drop</span> *lb* *ub*</span>
<span id="cb99-72"><a href="ch8.html#cb99-72" aria-hidden="true"></a><span class="kw">gen</span> lb = l_homicide1 - 1.96*l_homicide0 </span>
<span id="cb99-73"><a href="ch8.html#cb99-73" aria-hidden="true"></a><span class="kw">gen</span> ub = l_homicide1 + 1.96*l_homicide0 </span>
<span id="cb99-74"><a href="ch8.html#cb99-74" aria-hidden="true"></a></span>
<span id="cb99-75"><a href="ch8.html#cb99-75" aria-hidden="true"></a></span>
<span id="cb99-76"><a href="ch8.html#cb99-76" aria-hidden="true"></a>* Create the picture</span>
<span id="cb99-77"><a href="ch8.html#cb99-77" aria-hidden="true"></a><span class="kw">set</span> <span class="dv">scheme</span> s2color</span>
<span id="cb99-78"><a href="ch8.html#cb99-78" aria-hidden="true"></a>#delimit ;</span>
<span id="cb99-79"><a href="ch8.html#cb99-79" aria-hidden="true"></a><span class="kw">twoway</span> (<span class="kw">scatter</span> l_homicide1 ub lb <span class="fu">exp</span> , </span>
<span id="cb99-80"><a href="ch8.html#cb99-80" aria-hidden="true"></a>        lpattern(solid <span class="kw">dash</span> <span class="kw">dash</span> <span class="bn">dot</span> <span class="bn">dot</span> solid solid) </span>
<span id="cb99-81"><a href="ch8.html#cb99-81" aria-hidden="true"></a>        lcolor(<span class="bn">gray</span> <span class="bn">gray</span> <span class="bn">gray</span> <span class="kw">red</span> <span class="bn">blue</span>) </span>
<span id="cb99-82"><a href="ch8.html#cb99-82" aria-hidden="true"></a>        lwidth(thick medium medium medium medium thick thick)</span>
<span id="cb99-83"><a href="ch8.html#cb99-83" aria-hidden="true"></a>        <span class="bn">msymbol</span>(i i i i i i i i i i i i i i i) msize(medlarge medlarge)</span>
<span id="cb99-84"><a href="ch8.html#cb99-84" aria-hidden="true"></a>        mcolor(<span class="bn">gray</span> <span class="bn">black</span> <span class="bn">gray</span> <span class="bn">gray</span> <span class="kw">red</span> <span class="bn">blue</span>) </span>
<span id="cb99-85"><a href="ch8.html#cb99-85" aria-hidden="true"></a>        c(<span class="ot">l</span> <span class="ot">l</span> <span class="ot">l</span> <span class="ot">l</span> <span class="ot">l</span> <span class="ot">l</span> <span class="ot">l</span> <span class="ot">l</span> <span class="ot">l</span> <span class="ot">l</span> <span class="ot">l</span> <span class="ot">l</span> <span class="ot">l</span> <span class="ot">l</span> <span class="ot">l</span>) </span>
<span id="cb99-86"><a href="ch8.html#cb99-86" aria-hidden="true"></a>        cmissing(n n n n n n n n n n n n n n n n) </span>
<span id="cb99-87"><a href="ch8.html#cb99-87" aria-hidden="true"></a>        <span class="bn">xline</span>(0, lcolor(<span class="bn">black</span>) lpattern(solid))</span>
<span id="cb99-88"><a href="ch8.html#cb99-88" aria-hidden="true"></a>        <span class="kw">yline</span>(0, lcolor(<span class="bn">black</span>)) </span>
<span id="cb99-89"><a href="ch8.html#cb99-89" aria-hidden="true"></a>        <span class="kw">xlabel</span>(-9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 , <span class="bn">labsize</span>(medium))</span>
<span id="cb99-90"><a href="ch8.html#cb99-90" aria-hidden="true"></a>        <span class="kw">ylabel</span>(, <span class="bn">nogrid</span> <span class="bn">labsize</span>(medium))</span>
<span id="cb99-91"><a href="ch8.html#cb99-91" aria-hidden="true"></a>        <span class="bn">xsize</span>(7.5) <span class="bn">ysize</span>(5.5)           </span>
<span id="cb99-92"><a href="ch8.html#cb99-92" aria-hidden="true"></a>        <span class="bn">legend</span>(<span class="kw">off</span>)</span>
<span id="cb99-93"><a href="ch8.html#cb99-93" aria-hidden="true"></a>        <span class="bn">xtitle</span>(<span class="st">"Years before and after castle doctrine expansion"</span>, <span class="kw">size</span>(medium))</span>
<span id="cb99-94"><a href="ch8.html#cb99-94" aria-hidden="true"></a>        <span class="bn">ytitle</span>(<span class="st">"Log Murders "</span>, <span class="kw">size</span>(medium))</span>
<span id="cb99-95"><a href="ch8.html#cb99-95" aria-hidden="true"></a>        graphregion(fcolor(<span class="bn">white</span>) <span class="kw">color</span>(<span class="bn">white</span>) icolor(<span class="bn">white</span>) <span class="bn">margin</span>(<span class="bn">zero</span>))</span>
<span id="cb99-96"><a href="ch8.html#cb99-96" aria-hidden="true"></a>        <span class="kw">yline</span>(<span class="ot">`DDL'</span>, lcolor(<span class="kw">red</span>) lwidth(thick)) <span class="bn">text</span>(<span class="ot">`DD1'</span> -0.10 <span class="st">"DD Coefficient = `DD' (s.e. = `DDSE')"</span>)</span>
<span id="cb99-97"><a href="ch8.html#cb99-97" aria-hidden="true"></a>        )</span>
<span id="cb99-98"><a href="ch8.html#cb99-98" aria-hidden="true"></a>        ;</span>
<span id="cb99-99"><a href="ch8.html#cb99-99" aria-hidden="true"></a></span>
<span id="cb99-100"><a href="ch8.html#cb99-100" aria-hidden="true"></a>#delimit <span class="kw">cr</span>;</span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/castle_4.R"><code>castle_4.R</code></a></em></p>
<div class="sourceCode" id="cb100"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># This version includes</span>
<span class="co"># an interval that traces the confidence intervals</span>
<span class="co"># of your coefficients</span>
<span class="va">leadslags_plot</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">label</span>, y <span class="op">=</span> <span class="va">mean</span>,
             ymin <span class="op">=</span> <span class="va">mean</span><span class="op">-</span><span class="fl">1.96</span><span class="op">*</span><span class="va">sd</span>, 
             ymax <span class="op">=</span> <span class="va">mean</span><span class="op">+</span><span class="fl">1.96</span><span class="op">*</span><span class="va">sd</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="co"># this creates a red horizontal line</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_hline</a></span><span class="op">(</span>yintercept <span class="op">=</span> <span class="fl">0.035169444</span>, color <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_ribbon.html">geom_ribbon</a></span><span class="op">(</span>alpha <span class="op">=</span> <span class="fl">0.2</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="co"># Important to have informative axes labels!</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="st">"Years before and after castle doctrine expansion"</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"log(Homicide Rate)"</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_hline</a></span><span class="op">(</span>yintercept <span class="op">=</span> <span class="fl">0</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_vline</a></span><span class="op">(</span>xintercept <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></code></pre></div>
<p>You can see the figure that this creates in Figure <a href="ch8.html#fig:event-cheng2">9.23</a>. The difference between coefplot and this twoway command connects the event-study coefficients with lines, whereas coefplot displayed them as coefficients hanging in the air. Neither is right or wrong; I merely wanted you to see the differences for your own sake and to have code that you might experiment with and adapt to your own needs.</p>
<p>But the thing about this graph is that the leads are imbalanced. There’s only one state, for instance, in the ninth lead, and there’s only three in the eighth lead. So I’d like you to do two modifications to this. First, I’d like you to replace the sixth lead so that it is now equal to leads 6–9. In other words, we will force these late adopters to have the same coefficient as those with six years until treatment. When you do that, you should get Figure <a href="ch8.html#fig:event-cheng3">9.24</a>.</p>
<p>Next, let’s balance the event study by dropping the states who only show up in the seventh, eighth, and ninth leads.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Alex Bartik once recommended this to me.&lt;/p&gt;"><sup>156</sup></a> When you do this, you should get Figure <a href="ch8.html#fig:event-cheng4">9.25</a>.</p>
<div class="figure">
<span id="fig:event-cheng2"></span>
<img src="graphics/event_cheng2.jpg" alt="Homicide event study plots created manually with -twoway- [@Cheng2013]." width="100%"><p class="caption">
Figure 9.23: Homicide event study plots created manually with -twoway- <span class="citation">(Cheng and Hoekstra <a href="references.html#ref-Cheng2013" role="doc-biblioref">2013</a>)</span>.
</p>
</div>
<div class="figure">
<span id="fig:event-cheng3"></span>
<img src="graphics/event_cheng3.jpg" alt="Homicide event study plots using -twoway- [@Cheng2013]." width="100%"><p class="caption">
Figure 9.24: Homicide event study plots using -twoway- <span class="citation">(Cheng and Hoekstra <a href="references.html#ref-Cheng2013" role="doc-biblioref">2013</a>)</span>.
</p>
</div>
<div class="figure">
<span id="fig:event-cheng4"></span>
<img src="graphics/event_cheng4.jpg" alt="Homicide event study plots using -twoway- [@Cheng2013]." width="100%"><p class="caption">
Figure 9.25: Homicide event study plots using -twoway- <span class="citation">(Cheng and Hoekstra <a href="references.html#ref-Cheng2013" role="doc-biblioref">2013</a>)</span>.
</p>
</div>
<p>If nothing else, exploring these different specifications and cuts of the data can help you understand just how confident you should be that prior to treatment, treatment and control states genuinely were pretty similar. And if they weren’t similar, it behooves the researcher to at minimum provide some insight to others as to why the treatment and control groups were dissimilar in levels. Because after all—if they were different in levels, then it’s entirely plausible they would be different in their counterfactual trends too because why else are they different in the first place <span class="citation">(Kahn-Lang and Lang <a href="references.html#ref-KahnLang2019" role="doc-biblioref">2019</a>)</span>.</p>
</div>
<div id="bacon-decomposition" class="section level3" number="9.6.8">
<h3>
<span class="header-section-number">9.6.8</span> Bacon decomposition<a class="anchor" aria-label="anchor" href="#bacon-decomposition"><i class="fas fa-link"></i></a>
</h3>
<p>Recall that we run into trouble using the twoway fixed-effects model in a DD framework insofar as there are heterogeneous treatment effects over time. But the problem here only occurs with those <span class="math inline">\(2\times 2\)</span>s that use late-treated units compared to early-treated units. If there are few such cases, then the issue is much less problematic depending on the magnitudes of the weights and the size of the DD coefficients themselves. What we are now going to do is simply evaluate the frequency with which this issue occurs using the Bacon decomposition. Recall that the Bacon decomposition decomposes the twoway fixed effects estimator of the DD parameter into weighted averages of individual <span class="math inline">\(2\times 2\)</span>s across the four types of <span class="math inline">\(2\times 2\)</span>s possible. The Bacon decomposition uses a binary treatment variable, so we will reestimate the effect of castle-doctrine statutes on logged homicide rates by coding a state as “treated” if any portion of the year it had a castle-doctrine amendment. We will work with the special case of no covariates for simplicity, though note that the decomposition works with the inclusion of covariates as well <span class="citation">(Goodman-Bacon <a href="references.html#ref-Bacon2019" role="doc-biblioref">2019</a>)</span>. Stata users will need to download -ddtiming- from Thomas Goldring’s website, which I’ve included in the first line.</p>
<p>First, let’s estimate the actual model itself using a post dummy equaling one if the state was covered by a castle-doctrine statute that year. Here we find a smaller effect than many of Cheng and Hoekstra’s estimates because we do not include their state-year interaction fixed effects strategy among other things. But this is just for illustrative purposes, so let’s move to the Bacon decomposition itself. We can decompose the parameter estimate into the three different types of <span class="math inline">\(2\times 2\)</span>s, which I’ve reproduced in Table <a href="ch8.html#tab:bacon-regression">9.8</a></p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th align="left">Dependent Variable</th>
<th align="center">Log(homicide rate)</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Castle-doctrine law</td>
<td align="center">0.069</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center">(0.034)</td>
</tr>
</tbody>
</table></div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:bacon-regression">Table 9.8: </span> Bacon Decomposition Example.</caption>
<thead><tr class="header">
<th align="left">DD Comparison</th>
<th></th>
<th></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Earlier T vs. Later C</td>
<td>0.077</td>
<td><span class="math inline">\(-0.029\)</span></td>
</tr>
<tr class="even">
<td align="left">Later T vs. Earlier C</td>
<td>0.024</td>
<td>0.046</td>
</tr>
<tr class="odd">
<td align="left">T vs. Never treated</td>
<td>0.899</td>
<td>0.078</td>
</tr>
</tbody>
</table></div>
<p>Taking these weights, let’s just double check that they do indeed add up to the regression estimate we just obtained using our twoway fixed-effects estimator.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;This result is different from Cheng and Hoekstra’s because it does not include the region by year fixed effects. I exclude them for simplicity.&lt;/p&gt;"><sup>157</sup></a>
<span class="math display">\[\begin{align}
di (0.077 * -0.029) + (0.024 * 0.046) + (0.899 * 0.078) = 0.069
\end{align}\]</span></p>
<p>That is our main estimate, and thus confirms what we’ve been building on, which is that the DD parameter estimate from a twoway fixed-effects estimator is simply a weighted average over the different types of <span class="math inline">\(2\times 2\)</span>s in any differential design. Furthermore, we can see in the Bacon decomposition that most of the 0.069 parameter estimate is coming from comparing the treatment states to a group of never-treated states. The average DD estimate for that group is 0.078 with a weight of 0.899. So even though there is a later to early <span class="math inline">\(2\times 2\)</span> in the mix, as there always will be with any differential timing, it is small in terms of influence and ultimately pulls down the estimate.</p>
<p>But let’s now visualize this as distributing the weights against the DD estimates, which is a useful exercise. The horizontal line in Figure <a href="ch8.html#fig:bacon-weights">9.26</a> shows the average DD estimate we obtained from our fixed-effects regression of 0.069. But then what are these other graphics? Let’s review.</p>
<div class="figure">
<span id="fig:bacon-weights"></span>
<img src="graphics/bacon_weights.jpg" alt="@Bacon2019 decomposition of DD into weights and single $2 \times 2$s." width="100%"><p class="caption">
Figure 9.26: <span class="citation">Goodman-Bacon (<a href="references.html#ref-Bacon2019" role="doc-biblioref">2019</a>)</span> decomposition of DD into weights and single <span class="math inline">\(2 \times 2\)</span>s.
</p>
</div>
<p>Each icon in the graphic represents a single <span class="math inline">\(2\times 2\)</span> DD. The horizontal axis shows the weight itself, whereas the vertical axis shows the magnitude of that particular <span class="math inline">\(2\times 2\)</span>. Icons further to the right therefore will be more influential in the final average DD than those closer to zero.</p>
<p>There are three kinds of icons here: an early to late group comparison (represented with a light <span class="math inline">\(\times\)</span>), a late to early (dark <span class="math inline">\(\times\)</span>), and a treatment compared to a never-treated (dark triangle). You can see that the dark triangles are all above zero, meaning that each of these <span class="math inline">\(2\times 2\)</span>s (which correspond to a particular set of states getting the treatment in the same year) is positive. Now they are spread out somewhat—two groups are on the horizontal line, but the rest are higher. What appears to be the case, though, is that the group with the largest weight is really pulling down the parameter estimate and bringing it closer to the 0.069 that we find in the regression.</p>
</div>
<div id="the-future-of-dd" class="section level3" number="9.6.9">
<h3>
<span class="header-section-number">9.6.9</span> The future of DD<a class="anchor" aria-label="anchor" href="#the-future-of-dd"><i class="fas fa-link"></i></a>
</h3>
<p>The Bacon decomposition is an important phase in our understanding of the DD design when implemented using the twoway fixed effects linear model. Prior to this decomposition, we had only a metaphorical understanding of the necessary conditions for identifying causal effects using differential timing with a twoway fixed-effects estimator. We thought that since the <span class="math inline">\(2\times 2\)</span> required parallel trends, that that “sort of” must be what’s going on with differential timing too. And we weren’t too far off—there is a version of parallel trends in the identifying assumptions of DD using twoway fixed effects with differential timing. But what <span class="citation">Goodman-Bacon (<a href="references.html#ref-Bacon2019" role="doc-biblioref">2019</a>)</span> also showed is that the weights themselves drove the numerical estimates too, and that while some of it was intuitive (e.g., group shares being influential) others were not (e.g., variance in treatment being influential).</p>
<p>The Bacon decomposition also highlighted some of the unique challenges we face with differential timing. Perhaps no other problem is better highlighted in the diagnostics of the Bacon decomposition than the problematic “late to early” <span class="math inline">\(2\times 2\)</span> for instance. Given any heterogeneity bias, the late to early <span class="math inline">\(2\times 2\)</span> introduces biases <em>even with variance weighted common trends holding</em>! So, where to now?</p>
<p>From 2018 to 2020, there has been an explosion of work on the DD design. Much of it is unpublished, and there has yet to appear any real consensus among applied people as to how to handle it. Here I would like to outline what I believe could be a map as you attempt to navigate the future of DD. I have attempted to divide this new work into three categories: weighting, selective choice of “good” <span class="math inline">\(2 \times 2\)</span>s, and matrix completion.</p>
<p>What we know now is that there are two fundamental problems with the DD design. First, there is the issue of weighting itself. The twoway fixed-effects estimator weights the individual <span class="math inline">\(2\times 2\)</span>s in ways that do not make a ton of theoretical sense. For instance, why do we think that groups at the middle of the panel should be weighted more than those at the end? There’s no theoretical reason we should believe that. But as <span class="citation">Goodman-Bacon (<a href="references.html#ref-Bacon2019" role="doc-biblioref">2019</a>)</span> revealed, that’s precisely what twoway fixed effects does. And this is weird because you can change your results simply by adding or subtracting years to the panel—not just because this changes the <span class="math inline">\(2\times 2\)</span>, but also because it changes the variance in treatment itself! So that’s weird.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;This is less an issue with event study designs because the variance of treatment indicator is the same for everyone.&lt;/p&gt;"><sup>158</sup></a></p>
<p>But this is not really the fatal problem, you might say, with twoway fixed-effects estimates of a DD design. The bigger issue was what we saw in the Bacon decomposition—you will inevitably use past treated units as controls for future treated units, or what I called the “late to early <span class="math inline">\(2\times 2\)</span>.” This happens both in the event study and in the designs modeling the average treatment effect with a dummy variable. Insofar as it takes more than one period for the treatment to be fully incorporated, then insofar as there’s substantial weight given to the late to early <span class="math inline">\(2\times 2\)</span>s, the existence of heterogeneous treatment effects skews the parameter away from the ATT—maybe even flipping signs!<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;In all seriousness, it is practically modal in applied papers that utilize a DD design to imagine that dynamic treatment effects are at least plausible ex ante, if not expected. This kind of “dynamic treatment effects” is usually believed as a realistic description of what we think could happen in any policy environment. As such, the biases associated with panel fixed effects model with twoway fixed effects is, to be blunt, scary. Rarely have I seen a study wherein the treatment was merely a one-period shift in size. Even in the &lt;span class="citation"&gt;Miller et al. (&lt;a href="references.html#ref-Miller2019" role="doc-biblioref"&gt;2019&lt;/a&gt;)&lt;/span&gt; paper, the effect of ACA-led Medicaid expansions was a gradual reduction in annual mortality over time. Figure &lt;a href="ch8.html#fig:miller4"&gt;9.7&lt;/a&gt; really is probably a &lt;em&gt;typical&lt;/em&gt; kind of event study, not an exceptional one.&lt;/p&gt;'><sup>159</sup></a></p>
<p>Whereas the weird weighting associated with twoway fixed effects is an issue, it’s something you can at least check into because the Bacon decomposition allows you to separate out the <span class="math inline">\(2\times 2\)</span> average DD values from their weights. Thus, if your results are changing by adding years because your underlying <span class="math inline">\(2\times 2\)</span>s are changing, you simply need to investigate it in the Bacon decomposition. The weights and the <span class="math inline">\(2\times 2\)</span>s, in other words, are things that can be directly calculated, which can be a source of insight into why twoway fixed effects estimator is finding what it finds.</p>
<p>But the second issue is a different beast altogether. And one way to think of the emerging literature is that many authors are attempting to solve the problem that some of these <span class="math inline">\(2\times 2\)</span>s (e.g., the late to early <span class="math inline">\(2\times 2\)</span>) are problematic. Insofar as they are problematic, can we improve over our static twoway fixed-effects model? Let’s take a few of these issues up with examples from the growing literature.</p>
<p>Another solution to the weird weighting twoway fixed-effects problem has been provided by <span class="citation">Callaway and Sant’Anna (<a href="references.html#ref-Callaway2019" role="doc-biblioref">2019</a>)</span>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Sant’Anna has been particularly active in this area in producing elegant econometric solutions to some of these DD problems.&lt;/p&gt;"><sup>160</sup></a> <span class="citation">Callaway and Sant’Anna (<a href="references.html#ref-Callaway2019" role="doc-biblioref">2019</a>)</span> approach the DD framework very differently than <span class="citation">Goodman-Bacon (<a href="references.html#ref-Bacon2019" role="doc-biblioref">2019</a>)</span>. <span class="citation">Callaway and Sant’Anna (<a href="references.html#ref-Callaway2019" role="doc-biblioref">2019</a>)</span> use an approach that allows them to estimate what they call the group-time average treatment effect, which is just the ATT for a given group at any point in time. Assuming parallel trends conditional on time-invariant covariates and overlap in a propensity score, which I’ll discuss below, you can calculate group ATT by time (relative time like in an event study or absolute time). One unique part of these authors’ approach is that it is non-parametric as opposed to regression-based. For instance, under their identifying assumptions, their nonparametric estimator for a group ATT by time is:
<span class="math display">\[\begin{align}
ATT(g,t)=E\left[\left(\dfrac{G_g}{E[G_g]}-
\dfrac{\dfrac{p_g(X)C}{1-p_g(X)}}{E \bigg [ \dfrac{p_g(X)C}{1-p_g(X)}\bigg]}\right)
(Y_t-Y_{g-1})\right]
\end{align}\]</span>
where the weights, <span class="math inline">\(p\)</span>, are propensity scores, <span class="math inline">\(G\)</span> is a binary variable that is equal to 1 if an individual is first treated in period <span class="math inline">\(g\)</span>, and <span class="math inline">\(C\)</span> is a binary variable equal to one for individuals in the control group. Notice there is no time index, so these <span class="math inline">\(C\)</span> units are the never-treated group. If you’re still with me, you should find the weights straightforward. Take observations from the control group as well as group <span class="math inline">\(g\)</span>, and omit the other groups. Then weight up those observations from the control group that have characteristics similar to those frequently found in group <span class="math inline">\(g\)</span> and weight down observations from the control group that have characteristics rarely found in group <span class="math inline">\(g\)</span>. This kind of reweighting procedure guarantees that the covariates of group <span class="math inline">\(g\)</span> and the control group are balanced. You can see principles from earlier chapters making their way into this DD estimation—namely, balance on covariates to create exchangeable units on observables.</p>
<p>But because we are calculating group-specific ATT by time, you end up with a lot of treatment effect parameters. The authors address this by showing how one can take all of these treatment effects and collapse them into more interpretable parameters, such as a larger ATT. All of this is done without running a regression, and therefore avoids some of the unique issues created in doing so.</p>
<p>One simple solution might be to estimate your event-study model and simply take the mean over all lags using a linear combination of all point estimates <span class="citation">(Borusyak and Jaravel <a href="references.html#ref-Borusyak2018" role="doc-biblioref">2018</a>)</span>. Using this method, we in fact find considerably larger effects or nearly twice the size as we get from the simpler static twoway fixed-effects model. This is perhaps an improvement because weights can be large on the long-run effects due to large effects from group shares. So if you want a summary measure, it’s better to estimate the event study and then average them afterthe fact.</p>
<p>Another great example of a paper wrestling with the biases brought up by heterogeneous treatment effects is <span class="citation">Sun and Abraham (<a href="references.html#ref-Abraham2019" role="doc-biblioref">2020</a>)</span>. This paper is primarily motivated by problems created in event studies, but you can see some of the issues brought up in <span class="citation">Goodman-Bacon (<a href="references.html#ref-Bacon2019" role="doc-biblioref">2019</a>)</span>. In an event study with differential timing, as we discussed earlier, leads and lags are often used to measure dynamics in the treatment itself. But these can produce causally uninterpretable results because they will assign non-convex weights to cohort-specific treatment effects. Similar to <span class="citation">Callaway and Sant’Anna (<a href="references.html#ref-Callaway2019" role="doc-biblioref">2019</a>)</span>, they propose estimating a group-specific dynamic effect and from those calculate a group specific estimate.</p>
<p>The way I organize these papers in my mind is around the idea of heterogeneity in time, the use of twoway fixed effects, and differential timing. The theoretical insight from all these papers is the coefficients on the static twoway fixed-effects leads and lags will be unintelligible if there is heterogeneity in treatment effects over time. In this sense, we are back in the world that <span class="citation">Goodman-Bacon (<a href="references.html#ref-Bacon2019" role="doc-biblioref">2019</a>)</span> revealed, in which heterogeneity treatment effect biases create real challenges for the DD design using twoway fixed effects.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;One improvement over the binary treatment approach to estimating the treatment effect is when using an event study, the variance of treatment issues are moot.&lt;/p&gt;"><sup>161</sup></a></p>
<p>Their alternative is estimate a “saturated” model to ensure that the heterogeneous problem never occurs in the first place. The proposed alternative estimation technique is to use an interacted specification that is saturated in relative time indicators as well as cohort indicators. The treatment effect associated with this design is called the interaction-weighted estimator, and using it, the DD parameter is equivalent to the difference between the average change in outcomes for a given cohort in those periods prior to treatment and the average changes for those units that had not been treated at the time interval. Additionally, this method uses the never-treated units as controls, and thereby avoids the hairy problems noted in <span class="citation">Goodman-Bacon (<a href="references.html#ref-Bacon2019" role="doc-biblioref">2019</a>)</span> when computing later to early <span class="math inline">\(2\times 2\)</span>s.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;But in selecting only the never-treated as controls, the approach may have limited value for those situations where the number of units in the never-treated pool is extremely small.&lt;/p&gt;"><sup>162</sup></a></p>
<p>Another paper that attempts to circumvent the weirdness of the regression-based method when there are numerous late to early <span class="math inline">\(2\times 2\)</span>s is <span class="citation">Cengiz et al. (<a href="references.html#ref-Cengiz2019" role="doc-biblioref">2019</a>)</span>. This is bound to be a classic study in labor for its exhaustive search for detectable repercussions of the minimum wage on low-paying jobs. The authors ultimately find little evidence to support any concern, but how do they come to this conclusion?</p>
<p><span class="citation">Cengiz et al. (<a href="references.html#ref-Cengiz2019" role="doc-biblioref">2019</a>)</span> take a careful approach by creating separate samples. The authors want to know the impact of minimum-wage changes on low-wage jobs across 138 state-level minimum-wage changes from 1979 to 2016. The authors in an appendix note the problems with aggregating individual DD estimates into a single parameter, and so tackle the problem incrementally by creating 138 separate data sets associated with a minimum-wage event. Each sample has both treatment groups and control groups, but not all units are used as controls. Rather, only units that were not treated within the sample window are allowed to be controls. Insofar as a control is not treated during the sample window associated with a treatment unit, it can be by this criteria used as a control. These 138 estimates are then stacked to calculate average treatment effects. This is an alternative method to the twoway fixed-effects DD estimator because it uses a more stringent criteria for whether a unit can be considered a control. This in turn circumvents the heterogeneity problems that <span class="citation">Goodman-Bacon (<a href="references.html#ref-Bacon2019" role="doc-biblioref">2019</a>)</span> notes because <span class="citation">Cengiz et al. (<a href="references.html#ref-Cengiz2019" role="doc-biblioref">2019</a>)</span> essentially create 138 DD situations in which controls are always “never-treated” for the duration of time under consideration.</p>
<p>But the last methodology I will discuss that has emerged in the last couple of years is a radical departure from the regression-based methodology altogether. Rather than use a twoway fixed-effects estimator to estimate treatment effects with differential timing, <span class="citation">Athey et al. (<a href="references.html#ref-Athey2018" role="doc-biblioref">2018</a>)</span> propose a machine-learning-based methodology called “matrix completion” for panel data. The estimator is exotic and bears some resemblance to matching imputation and synthetic control. Given the growing popularity of placing machine learning at the service of causal inference, I suspect that once Stata code for matrix completion is introduced, we will see this procedure used more broadly.</p>
<p>Matrix completion for panel data is a machine-learning-based approach to causal inference when one is working explicitly with panel data and differential timing. The application of matrix completion to causal inference has some intuitive appeal given one of the ways that Rubin has framed causality is as a missing data problem. Thus, if we are missing the matrix of counterfactuals, we might explore whether this method from computer science could assist us in recovering it. Imagine we could create two matrices of potential outcomes: a matrix of <span class="math inline">\(Y^0\)</span> potential outcomes for all panel units over time and <span class="math inline">\(Y^1\)</span>. Once treatment occurs, a unit switches from <span class="math inline">\(Y^0\)</span> to <span class="math inline">\(Y^1\)</span> under the switching equation, and therefore the missing data problem occurs. Missingness is simply another way of describing the fundamental problem of causal inference for there will never be a complete set of matrices enabling calculation of interesting treatment parameters given the switching equation only assigns one of them to reality.</p>
<p>Say we are interested in this treatment effect parameter:
<span class="math display">\[\begin{align}
\widehat{\delta_{ATT}} = \dfrac{1}{N_T} \sum \big(Y_{it}^1 - Z_{it}^0\big)
\end{align}\]</span>
where <span class="math inline">\(Y^1\)</span> are the observed outcomes in a panel unit at some post-treatment period, <span class="math inline">\(Z^0\)</span> is the estimated missing elements of the <span class="math inline">\(Y^0\)</span> matrix for the post-treatment period, and <span class="math inline">\(N_T\)</span> is the number of treatment units. Matrix completion uses the observed elements of the matrix’s realized values to predict the missing elements of the <span class="math inline">\(Y^0\)</span> matrix (missing due to being in the post-treatment period and therefore having switched from <span class="math inline">\(Y^0\)</span> to <span class="math inline">\(Y^1\)</span>).</p>
<p>Analytically, this imputation is done via something called regularization-based prediction. The objective in this approach is to optimally predict the missing elements by minimizing a convex function of the difference between the observed matrix of <span class="math inline">\(Y^0\)</span> and the unknown complete matrix <span class="math inline">\(Z^0\)</span> using nuclear norm regularization. Let <span class="math inline">\(\Omega\)</span> denote the row and column indices <span class="math inline">\((i,j)\)</span> of the observed entries of the outcomes, then the objective function can be written as
<span class="math display">\[\begin{align}
\widehat{Z^0}=\arg\min_{Z^0}
\sum_{(i,j) \in \Omega} \dfrac{(Y^0_{it} - Z^0_{it})^2}{|\Omega|}+\Lambda ||Z^0||
\end{align}\]</span>
where <span class="math inline">\(||Z^0||\)</span> is the nuclear norm (sum of singular values of <span class="math inline">\(Z0\)</span>). The regularization parameter <span class="math inline">\(\Lambda\)</span> is chosen using tenfold cross validation. <span class="citation">Athey et al. (<a href="references.html#ref-Athey2018" role="doc-biblioref">2018</a>)</span> show that this procedure outperforms other methods in terms of root mean squared prediction error.</p>
<p>Unfortunately, at present estimation using matrix completion is not available in Stata. R packages for it do exist, such as the gsynth package, but it has to be adapted for Stata users. And until it is created, I suspect adoption will lag.</p>
</div>
</div>
<div id="conclusion-7" class="section level2" number="9.7">
<h2>
<span class="header-section-number">9.7</span> Conclusion<a class="anchor" aria-label="anchor" href="#conclusion-7"><i class="fas fa-link"></i></a>
</h2>
<p>America’s institutionalized state federalism provides a constantly evolving laboratory for applied researchers seeking to evaluate the causal effects of laws and other interventions. It has for this reason probably become one of the most popular forms of identification among American researchers, if not the most common. A Google search of the phrase “difference-in-differences” brought up 45,000 hits. It is arguably the most common methodology you will use—more than IV or matching or even RDD, despite RDD’s greater perceived credibility. There is simply a never-ending flow of quasi-experiments being created by our decentralized data-generating process in the United States made even more advantageous by so many federal agencies being responsible for data collection, thus ensuring improved data quality and consistency.</p>
<p>But, what we have learned in this chapter is that while there is a current set of identifying assumptions and practices associated with the DD design, differential timing does introduce some thorny challenges that have long been misunderstood. Much of the future of DD appears to be mounting solutions to problems we are coming to understand better, such as the odd weighting of regression itself and problematic <span class="math inline">\(2\times 2\)</span> DDs that bias the aggregate ATT when heterogeneity in the treatment effects over time exists. Nevertheless, DD—and specifically, regression-based DD—is not going away. It is the single most popular design in the applied researcher’s toolkit and likely will be for many years to come. Thus it behooves the researcher to study this literature carefully so that they can better protect against various forms of bias.</p>
<div class="cover-box">
<div class="row">
    <div class="col-xs-8 col-md-4 cover-img">
        <a href="https://www.amazon.com/dp/0300251688"><img src="../images/cover.jpg" alt="Buy Today!"></a>
    </div>
    
    <div class="col-xs-12 col-md-8 cover-text-box">
            <h2> 
                Causal Inference: 
                <br><span style="font-style: italic; font-weight:bold; font-size: 20px;">The Mixtape.</span>
            </h2> 
            
        <div class="cover-text">
            <p>Buy the print version today:</p>
            
            <div class="chips">
                <a href="https://www.amazon.com/dp/0300251688" class="app-chip"> 
                    <i class="fab fa-amazon" aria-hidden="true"></i> Buy from Amazon 
                </a>
    
                <a href="https://yalebooks.yale.edu/book/9780300251685/causal-inference" class="app-chip"> 
                    <i class="fas fa-book" aria-hidden="true"></i> Buy from Yale Press 
                </a>
            </div>
        </div>
    </div>
</div>
</div>

</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="ch7.html"><span class="header-section-number">8</span> Panel Data</a></div>
<div class="next"><a href="ch9.html"><span class="header-section-number">10</span> Synthetic Control</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#ch8"><span class="header-section-number">9</span> Difference-in-Differences</a></li>
<li>
<a class="nav-link" href="#john-snows-cholera-hypothesis"><span class="header-section-number">9.1</span> John Snow’s Cholera Hypothesis</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#table-xii"><span class="header-section-number">9.1.1</span> Table XII</a></li></ul>
</li>
<li>
<a class="nav-link" href="#estimation-1"><span class="header-section-number">9.2</span> Estimation</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#a-simple-table"><span class="header-section-number">9.2.1</span> A simple table</a></li>
<li><a class="nav-link" href="#the-simple-2times-2-dd"><span class="header-section-number">9.2.2</span> The simple \(2\times 2\) DD</a></li>
<li><a class="nav-link" href="#dd-and-the-minimum-wage"><span class="header-section-number">9.2.3</span> DD and the Minimum Wage</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#inference-1"><span class="header-section-number">9.3</span> Inference</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#block-bootstrapping"><span class="header-section-number">9.3.1</span> Block bootstrapping</a></li>
<li><a class="nav-link" href="#aggregation"><span class="header-section-number">9.3.2</span> Aggregation</a></li>
<li><a class="nav-link" href="#clustering"><span class="header-section-number">9.3.3</span> Clustering</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#providing-evidence-for-parallel-trends-through-event-studies-and-parallel-leads"><span class="header-section-number">9.4</span> Providing Evidence for Parallel Trends Through Event Studies and Parallel Leads</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#a-redundant-rant-about-parallel-pre-treatment-dd-coefficients-because-im-worried-one-was-not-enough"><span class="header-section-number">9.4.1</span> A redundant rant about parallel pre-treatment DD coefficients (because I’m worried one was not enough)</a></li>
<li><a class="nav-link" href="#checking-the-pre-treatment-balance-between-treatment-and-control-groups"><span class="header-section-number">9.4.2</span> Checking the pre-treatment balance between treatment and control groups</a></li>
<li><a class="nav-link" href="#affordable-care-act-expanding-medicaid-and-population-mortality"><span class="header-section-number">9.4.3</span> Affordable Care Act, expanding Medicaid and population mortality</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#the-importance-of-placebos-in-dd"><span class="header-section-number">9.5</span> The Importance of Placebos in DD</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#triple-differences"><span class="header-section-number">9.5.1</span> Triple differences</a></li>
<li><a class="nav-link" href="#state-mandated-maternity-benefits"><span class="header-section-number">9.5.2</span> State-mandated maternity benefits</a></li>
<li><a class="nav-link" href="#abortion-legalization-and-long-term-gonorrhea-incidence"><span class="header-section-number">9.5.3</span> Abortion legalization and long-term gonorrhea incidence</a></li>
<li><a class="nav-link" href="#going-beyond-cunningham2013"><span class="header-section-number">9.5.4</span> Going beyond Cunningham and Cornwell (2013)</a></li>
<li><a class="nav-link" href="#placebos-as-critique"><span class="header-section-number">9.5.5</span> Placebos as critique</a></li>
<li><a class="nav-link" href="#compositional-change-within-repeated-cross-sections"><span class="header-section-number">9.5.6</span> Compositional change within repeated cross-sections</a></li>
<li><a class="nav-link" href="#final-thoughts"><span class="header-section-number">9.5.7</span> Final thoughts</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#twoway-fixed-effects-with-differential-timing"><span class="header-section-number">9.6</span> Twoway Fixed Effects with Differential Timing</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#bacon-decomposition-theorem"><span class="header-section-number">9.6.1</span> Bacon Decomposition theorem</a></li>
<li><a class="nav-link" href="#expressing-the-decomposition-in-potential-outcomes"><span class="header-section-number">9.6.2</span> Expressing the decomposition in potential outcomes</a></li>
<li><a class="nav-link" href="#variance-weighted-att"><span class="header-section-number">9.6.3</span> Variance weighted ATT</a></li>
<li><a class="nav-link" href="#variance-weighted-common-trends"><span class="header-section-number">9.6.4</span> Variance weighted common trends</a></li>
<li><a class="nav-link" href="#att-heterogeneity-within-time-bias"><span class="header-section-number">9.6.5</span> ATT heterogeneity within time bias</a></li>
<li><a class="nav-link" href="#castle-doctrine-statutes-and-homicides"><span class="header-section-number">9.6.6</span> Castle-doctrine statutes and homicides</a></li>
<li><a class="nav-link" href="#replicating-cheng2013-sort-of"><span class="header-section-number">9.6.7</span> Replicating Cheng and Hoekstra (2013), sort of</a></li>
<li><a class="nav-link" href="#bacon-decomposition"><span class="header-section-number">9.6.8</span> Bacon decomposition</a></li>
<li><a class="nav-link" href="#the-future-of-dd"><span class="header-section-number">9.6.9</span> The future of DD</a></li>
</ul>
</li>
<li><a class="nav-link" href="#conclusion-7"><span class="header-section-number">9.7</span> Conclusion</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/scunning1975/mixtape/blob/master/08-Difference_in_Differences.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/scunning1975/mixtape/edit/master/08-Difference_in_Differences.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong><span style="font-weight:bold">Causal Inference</span></strong>: <i>The Mixtape</i>" was written by Scott Cunningham. It was last built on 2020-12-19.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
