<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>5 Matching and Subclassification | Causal Inference</title>
<meta name="author" content="Scott Cunningham">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.6/header-attrs.js"></script><script src="libs/jquery-3.5.1/jquery-3.5.1.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.5.3/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.5.3/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.3.9000/tabs.js"></script><script src="libs/bs3compat-0.2.3.9000/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><style>
    @import url('https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,400;0,700;1,400;1,700&family=Roboto:ital,wght@0,700;1,300&display=swap');
    </style>
<script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS --><link rel="stylesheet" href="css/style.css">
<link rel="stylesheet" href="css/toc.css">
<link rel="stylesheet" href="css/causal_inference_style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="&lt;i&gt;The Mixtape&lt;/i&gt;"><span style="font-weight:bold">Causal Inference</span></a>:
        <small class="text-muted"><i>The Mixtape</i></small>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="ch1.html"><span class="header-section-number">2</span> Probability and Regression Review</a></li>
<li><a class="" href="ch2.html"><span class="header-section-number">3</span> Directed Acyclic Graphs</a></li>
<li><a class="" href="ch3.html"><span class="header-section-number">4</span> Potential Outcomes Causal Model</a></li>
<li><a class="active" href="ch4.html"><span class="header-section-number">5</span> Matching and Subclassification</a></li>
<li><a class="" href="ch5.html"><span class="header-section-number">6</span> Regression Discontinuity</a></li>
<li><a class="" href="ch6.html"><span class="header-section-number">7</span> Instrumental Variables</a></li>
<li><a class="" href="ch7.html"><span class="header-section-number">8</span> Panel Data</a></li>
<li><a class="" href="ch8.html"><span class="header-section-number">9</span> Difference-in-Differences</a></li>
<li><a class="" href="ch9.html"><span class="header-section-number">10</span> Synthetic Control</a></li>
<li><a class="" href="ch10.html"><span class="header-section-number">11</span> Conclusion</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/scunning1975/mixtape">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="ch4" class="section level1" number="5">
<h1>
<span class="header-section-number">5</span> Matching and Subclassification<a class="anchor" aria-label="anchor" href="#ch4"><i class="fas fa-link"></i></a>
</h1>
<div class="cover-box">
<div class="row">
    <div class="col-xs-8 col-md-4 cover-img">
        <a href="https://www.amazon.com/dp/0300251688"><img src="../images/cover.jpg" alt="Buy Today!"></a>
    </div>
    
    <div class="col-xs-12 col-md-8 cover-text-box">
            <h2> 
                Causal Inference: 
                <br><span style="font-style: italic; font-weight:bold; font-size: 20px;">The Mixtape.</span>
            </h2> 
            
        <div class="cover-text">
            <p>Buy the print version today:</p>
            
            <div class="chips">
                <a href="https://www.amazon.com/dp/0300251688" class="app-chip"> 
                    <i class="fab fa-amazon" aria-hidden="true"></i> Buy from Amazon 
                </a>
    
                <a href="https://yalebooks.yale.edu/book/9780300251685/causal-inference" class="app-chip"> 
                    <i class="fas fa-book" aria-hidden="true"></i> Buy from Yale Press 
                </a>
            </div>
        </div>
    </div>
</div>
</div>
<div id="subclassification" class="section level2" number="5.1">
<h2>
<span class="header-section-number">5.1</span> Subclassification<a class="anchor" aria-label="anchor" href="#subclassification"><i class="fas fa-link"></i></a>
</h2>
<p>One of the main things I wanted to cover in the chapter on directed acylical graphical models was the idea of the backdoor criterion. Specifically, insofar as there exists a conditioning strategy that will satisfy the backdoor criterion, then you can use that strategy to identify some causal effect. We now discuss three different kinds of conditioning strategies. They are subclassification, exact matching, and approximate matching.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Everything I know about matching I learned from the Northwestern causal inference workshops in lectures taught by the econometrician Alberto Abadie. I would like to acknowledge him as this chapter is heavily indebted to him and those lectures.&lt;/p&gt;"><sup>75</sup></a></p>
<p>Subclassification is a method of satisfying the backdoor criterion by weighting differences in means by strata-specific weights. These strata-specific weights will, in turn, adjust the differences in means so that their distribution by strata is the same as that of the counterfactual’s strata. This method implicitly achieves distributional <em>balance</em> between the treatment and control in terms of that known, observable confounder. This method was created by statisticians like <span class="citation">Cochran (<a href="references.html#ref-Cochran1968" role="doc-biblioref">1968</a>)</span>, who tried to analyze the causal effect of smoking on lung cancer, and while the methods today have moved beyond it, we include it because some of the techniques implicit in subclassification are present throughout the rest of the book.</p>
<p>One of the concepts threaded through this chapter is the conditional independence assumption, or <em>CIA</em>. Sometimes we know that randomization occurred only conditional on some observable characteristics. For instance, in <span class="citation">Krueger (<a href="references.html#ref-Krueger1999" role="doc-biblioref">1999</a>)</span>, Tennessee randomly assigned kindergarten students and their teachers to small classrooms, large classrooms, and large classrooms with an aide. But the state did this conditionally—specifically, schools were chosen, and then students were randomized. Krueger therefore estimated regression models that included a school fixed effect because he knew that the treatment assignment was only <em>conditionally</em> random.</p>
<p>This assumption is written as
<span class="math display">\[
(Y^1,Y^0) \perp \!\!\! \perp D\mid X
\]</span>
where again <span class="math inline">\(\perp \!\!\! \perp\)</span> is the notation for statistical independence and <span class="math inline">\(X\)</span> is the variable we are conditioning on. What this means is that the expected values of <span class="math inline">\(Y^1\)</span> and <span class="math inline">\(Y^0\)</span> are equal for treatment and control group <em>for each value of <span class="math inline">\(X\)</span></em>. Written out, this means:</p>
<p><span class="math display">\[\begin{align}
   E\big[Y^1\mid D=1,X\big]=E\big[Y^1\mid D=0,X\big]
   \\
   E\big[Y^0\mid D=1,X\big]=E\big[Y^0\mid D=0,X\big]
\end{align}\]</span></p>
<p>Let me link together some concepts. First, insofar as CIA is credible, then CIA means you have found a conditioning strategy that satisfies the backdoor criterion. Second, when treatment assignment had been conditional on observable variables, it is a situation of <em>selection on observables</em>. The variable <span class="math inline">\(X\)</span> can be thought of as an <span class="math inline">\(n\times k\)</span> matrix of covariates that satisfy the CIA as a whole.</p>
<div id="some-background" class="section level3" number="5.1.1">
<h3>
<span class="header-section-number">5.1.1</span> Some background<a class="anchor" aria-label="anchor" href="#some-background"><i class="fas fa-link"></i></a>
</h3>
<p>A major public health problem of the mid- to late twentieth century was the problem of rising lung cancer. For instance, the mortality rate per 100,000 from cancer of the lungs in males reached 80–100 per 100,000 by 1980 in Canada, England, and Wales. From 1860 to 1950, the incidence of lung cancer found in cadavers during autopsy grew from 0% to as high as 7%. The rate of lung cancer incidence appeared to be increasing.</p>
<p>Studies began emerging that suggested smoking was the cause since it was so highly correlated with incidence of lung cancer. For instance, studies found that the relationship between daily smoking and lung cancer in males was monotonically increasing in the number of cigarettes a male smoked per day. But some statisticians believed that scientists couldn’t draw a causal conclusion because it was possible that smoking was not independent of potential health outcomes. Specifically, perhaps the people who smoked cigarettes differed from non-smokers in ways that were directly related to the incidence of lung cancer. After all, no one is flipping coins when deciding to smoke.</p>
<p>Thinking about the simple difference in means decomposition from earlier, we know that contrasting the incidence of lung cancer between smokers and non-smokers will be biased in observational data if the independence assumption does not hold. And because smoking is endogenous—that is, people choose to smoke—it’s entirely possible that smokers differed from the non-smokers in ways that were directly related to the incidence of lung cancer.</p>
<p>Criticisms at the time came from such prominent statisticians as Joseph Berkson, Jerzy Neyman, and Ronald Fisher. They made several compelling arguments. First, they suggested that the correlation was spurious due to a non-random selection of subjects. Functional form complaints were also common. This had to do with people’s use of risk ratios and odds ratios. The association, they argued, was sensitive to those kinds of functional form choices, which is a fair criticism. The arguments were really not so different from the kinds of arguments you might see today when people are skeptical of a statistical association found in some observational data set.</p>
<p>Probably most damning, though, was the hypothesis that there existed an unobservable genetic element that both caused people to smoke and independently caused people to develop lung cancer. This confounder meant that smokers and non-smokers differed from one another in ways that were directly related to their potential outcomes, and thus independence did not hold. And there was plenty of evidence that the two groups were different. For instance, smokers were more extroverted than non-smokers, and they also differed in age, income, education, and so on.</p>
<p>The arguments against the smoking cause mounted. Other criticisms included that the magnitudes relating smoking and lung cancer were implausibly large. And again, the ever-present criticism of observational studies: there did not exist any experimental evidence that could incriminate smoking as a cause of lung cancer.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;But think about the hurdle that the last criticism actually creates. Just imagine the hypothetical experiment: a large sample of people, with diverse potential outcomes, are assigned to a treatment group (smoker) and control (non-smoker). These people must be dosed with their corresponding treatments long enough for us to observe lung cancer develop—so presumably years of heavy smoking. How could anyone ever run an experiment like that? Who in their right mind would participate!? Just to describe the idealized experiment is to admit it’s impossible. But how do we answer the causal question without independence (i.e., randomization)?&lt;/p&gt;"><sup>76</sup></a></p>
<p>The theory that smoking causes lung cancer is now accepted science. I wouldn’t be surprised if more people believe in a flat Earth than that smoking causes lung cancer. I can’t think of a more well-known and widely accepted causal theory, in fact. So how did Fisher and others fail to see it? Well, in Fisher’s defense, his arguments were based on sound causal logic. Smoking <em>was</em> endogenous. There <em>was</em> no experimental evidence. The two groups differed considerably on observables. And the decomposition of the simple difference in means shows that contrasts will be biased if there is selection bias. Nonetheless, Fisher was wrong, and his opponents were right. They just were right for the wrong reasons.</p>
<p>To motivate what we’re doing in subclassification, let’s work with <span class="citation">Cochran (<a href="references.html#ref-Cochran1968" role="doc-biblioref">1968</a>)</span>, which was a study trying to address strange patterns in smoking data by adjusting for a confounder. Cochran lays out mortality rates by country and smoking type (Table <a href="ch4.html#tab:cochran">5.1</a>).</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:cochran">Table 5.1: </span> Death rates per 1,000 person-years <span class="citation">(Cochran <a href="references.html#ref-Cochran1968" role="doc-biblioref">1968</a>)</span>
</caption>
<thead><tr class="header">
<th align="left">Smoking group</th>
<th align="center">Canada</th>
<th align="center">UK</th>
<th align="center">US</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Non-smokers</td>
<td align="center">20.2</td>
<td align="center">11.3</td>
<td align="center">13.5</td>
</tr>
<tr class="even">
<td align="left">Cigarettes</td>
<td align="center">20.5</td>
<td align="center">14.1</td>
<td align="center">13.5</td>
</tr>
<tr class="odd">
<td align="left">Cigars/pipes</td>
<td align="center">35.5</td>
<td align="center">20.7</td>
<td align="center">17.4</td>
</tr>
</tbody>
</table></div>
<p>As you can see, the highest death rate for Canadians is among the cigar and pipe smokers, which is considerably higher than for non-smokers or for those who smoke cigarettes. Similar patterns show up in both countries, though smaller in magnitude than what we see in Canada.</p>
<p>This table suggests that pipes and cigars are more dangerous than cigarette smoking, which, to a modern reader, sounds ridiculous. The reason it sounds ridiculous is because cigar and pipe smokers often do not inhale, and therefore there is less tar that accumulates in the lungs than with cigarettes. And insofar as it’s the tar that causes lung cancer, it stands to reason that we should see higher mortality rates among cigarette smokers.</p>
<p>But, recall the independence assumption. Do we really believe that:</p>
<p><span class="math display">\[\begin{align}
   E\big[Y^1\mid \text{Cigarette}\big] =
   E\big[Y^1\mid \text{Pipe}\big] =
   E\big[Y^1\mid \text{Cigar}\big]
   \\
   E\big[Y^0\mid \text{Cigarette}\big]=
   E\big[Y^0\mid \text{Pipe}\big] =
   E\big[Y^0\mid \text{Cigar}\big]
\end{align}\]</span></p>
<p>Is it the case that factors related to these three states of the world are truly independent to the factors that determine death rates? Well, let’s assume for the sake of argument that these independence assumptions held. What else would be true across these three groups? Well, if the mean potential outcomes are the same for each type of smoking category, then wouldn’t we expect the observable characteristics of the smokers themselves to be as well? This connection between the independence assumption and the characteristics of the groups is called <em>balance</em>. If the means of the covariates are the same for each group, then we say those covariates are balanced and the two groups are exchangeable with respect to those covariates.</p>
<p>One variable that appears to matter is the age of the person. Older people were more likely at this time to smoke cigars and pipes, and without stating the obvious, older people were more likely to die. In Table <a href="ch4.html#tab:cochran2">5.2</a> we can see the mean ages of the different groups.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:cochran2">Table 5.2: </span> Mean ages, years <span class="citation">(Cochran <a href="references.html#ref-Cochran1968" role="doc-biblioref">1968</a>)</span>.</caption>
<thead><tr class="header">
<th align="left">Smoking group</th>
<th align="center">Canada</th>
<th align="center">British</th>
<th align="center">US</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Non-smokers</td>
<td align="center">54.9</td>
<td align="center">49.1</td>
<td align="center">57.0</td>
</tr>
<tr class="even">
<td align="left">Cigarettes</td>
<td align="center">50.5</td>
<td align="center">49.8</td>
<td align="center">53.2</td>
</tr>
<tr class="odd">
<td align="left">Cigars/pipes</td>
<td align="center">65.9</td>
<td align="center">55.7</td>
<td align="center">59.7</td>
</tr>
</tbody>
</table></div>
<p>The high means for cigar and pipe smokers are probably not terribly surprising. Cigar and pipe smokers are typically older than cigarette smokers, or at least they were in 1968 when Cochran was writing. And since older people die at a higher rate (for reasons other than just smoking cigars), maybe the higher death rate for cigar smokers is because they’re older on average. Furthermore, maybe by the same logic, cigarette smoking has such a low mortality rate because cigarette smokers are younger on average. Note, using DAG notation, this simply means that we have the following DAG:</p>
<div class="inline-figure"><img src="causal_inference_mixtape_files/figure-html/unnamed-chunk-49-1.png" width="50%" style="display: block; margin: auto;"></div>
<p>where <span class="math inline">\(D\)</span> is smoking, <span class="math inline">\(Y\)</span> is mortality, and <span class="math inline">\(A\)</span> is age of the smoker. Insofar as CIA is violated, then we have a backdoor path that is open, which also means that we have omitted variable bias. But however we want to describe it, the common thing is that the distribution of age for each group will be different—which is what I mean by <em>covariate imbalance</em>. My first strategy for addressing this problem of covariate imbalance is to <em>condition</em> on age in such a way that the distribution of age is comparable for the treatment and control groups.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Interestingly, this issue of covariate balance weaves throughout nearly every identification strategy that we will discuss.&lt;/p&gt;"><sup>77</sup></a></p>
<p>So how does subclassification achieve covariate balance? Our first step is to divide age into strata: say, 20–40, 41–70, and 71 and older. Then we can calculate the mortality rate for some treatment group (cigarette smokers) by strata (here, that is age). Next, weight the mortality rate for the treatment group by a strata-specific (or age-specific) weight that corresponds to the control group. This gives us the age-adjusted mortality rate for the treatment group. Let’s explain with an example by looking at Table <a href="ch4.html#tab:cochran3">5.3</a>. Assume that age is the only relevant confounder between cigarette smoking and mortality.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;A truly hilarious assumption, but this is just illustrative.&lt;/p&gt;"><sup>78</sup></a></p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:cochran3">Table 5.3: </span> Subclassification example.</caption>
<thead><tr class="header">
<th></th>
<th align="center">Death sates Cigarette smokers</th>
<th align="center"># of Cigarette smokers</th>
<th align="center"># of Pipe or cigar smokers</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Age 20–40</td>
<td align="center">20</td>
<td align="center">65</td>
<td align="center">10</td>
</tr>
<tr class="even">
<td>Age 41–70</td>
<td align="center">40</td>
<td align="center">25</td>
<td align="center">25</td>
</tr>
<tr class="odd">
<td>Age <span class="math inline">\(\geq 71\)</span>
</td>
<td align="center">60</td>
<td align="center">10</td>
<td align="center">65</td>
</tr>
<tr class="even">
<td>Total</td>
<td align="center"></td>
<td align="center">100</td>
<td align="center">100</td>
</tr>
</tbody>
</table></div>
<p>What is the average death rate for pipe smokers without subclassification? It is the weighted average of the mortality rate column where each weight is equal to <span class="math inline">\(\dfrac{N_t}{N}\)</span>, and <span class="math inline">\(N_t\)</span> and <span class="math inline">\(N\)</span> are the number of people in each group and the total number of people, respectively. Here that would be
<span class="math display">\[
20 \times \dfrac{65}{100} + 40 \times \dfrac{25}{100} + 60 \times \dfrac{10}{100}=29.
\]</span>
That is, the mortality rate of smokers in the population is 29 per 100,000.</p>
<p>But notice that the age distribution of cigarette smokers is the exact opposite (by construction) of pipe and cigar smokers. Thus the age distribution is imbalanced. Subclassification simply adjusts the mortality rate for cigarette smokers so that it has the same age distribution as the comparison group. In other words, we would multiply each age-specific mortality rate by the proportion of individuals in that age strata for the comparison group. That would be
<span class="math display">\[
20 \times \dfrac{10}{100} + 40 \times \dfrac{25}{100} + 60 \times \dfrac{65}{100}=51
\]</span>
That is, when we adjust for the age distribution, the age-adjusted mortality rate for cigarette smokers (were they to have the same age distribution as pipe and cigar smokers) would be 51 per 100,000—almost twice as large as we got taking a simple naı̈ve calculation unadjusted for the age confounder.</p>
<p>Cochran uses a version of this subclassification method in his paper and recalculates the mortality rates for the three countries and the three smoking groups (see Table <a href="ch4.html#tab:cochran4">5.4</a>). As can be seen, once we adjust for the age distribution, cigarette smokers have the highest death rates among any group.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:cochran4">Table 5.4: </span> Adjusted mortality rates using 3 age groups <span class="citation">(Cochran <a href="references.html#ref-Cochran1968" role="doc-biblioref">1968</a>)</span>.</caption>
<thead><tr class="header">
<th align="left">Smoking group</th>
<th align="center">Canada</th>
<th align="center">UK</th>
<th align="center">US</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Non-smokers</td>
<td align="center">20.2</td>
<td align="center">11.3</td>
<td align="center">13.5</td>
</tr>
<tr class="even">
<td align="left">Cigarettes</td>
<td align="center">29.5</td>
<td align="center">14.8</td>
<td align="center">21.2</td>
</tr>
<tr class="odd">
<td align="left">Cigars/pipes</td>
<td align="center">19.8</td>
<td align="center">11.0</td>
<td align="center">13.7</td>
</tr>
</tbody>
</table></div>
<p>This kind of adjustment raises a question—which variable(s) should we use for adjustment? First, recall what we’ve emphasized repeatedly. Both the backdoor criterion and CIA tell us precisely what we need to do. We need to choose a set of variables that satisfy the backdoor criterion. If the backdoor criterion is met, then all backdoor paths are closed, and if all backdoor paths are closed, then CIA is achieved. We call such a variable the <em>covariate</em>. A covariate is usually a random variable assigned to the individual units prior to treatment. This is sometimes also called exogenous. Harkening back to our DAG chapter, this variable must not be a collider as well. A variable is exogenous with respect to <span class="math inline">\(D\)</span> if the value of <span class="math inline">\(X\)</span> does not depend on the value of <span class="math inline">\(D\)</span>. Oftentimes, though not always and not necessarily, this variable will be time-invariant, such as race. Thus, when trying to adjust for a confounder using subclassification, rely on a credible DAG to help guide the selection of variables. Remember—your goal is to meet the backdoor criterion.</p>
</div>
<div id="identifying-assumptions" class="section level3" number="5.1.2">
<h3>
<span class="header-section-number">5.1.2</span> Identifying assumptions<a class="anchor" aria-label="anchor" href="#identifying-assumptions"><i class="fas fa-link"></i></a>
</h3>
<p>Let me now formalize what we’ve learned. In order to estimate a causal effect when there is a confounder, we need (1) CIA and (2) the probability of treatment to be between 0 and 1 for each strata. More formally,</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\((Y^1,Y^0) \perp \!\!\! \perp D\mid X\)</span> (conditional independence)</p></li>
<li><p><span class="math inline">\(0&lt;Pr(D=1 \mid X) &lt;1\)</span> with probability one (common support)</p></li>
</ol>
<p>These two assumptions yield the following identity</p>
<p><span class="math display">\[\begin{align}
   E\big[Y^1-Y^0\mid X\big] &amp; = E\big[Y^1 - Y^0 \mid X,D=1\big]                     
   \\
            &amp; = E\big[Y^1\mid X,D=1\big] - E\big[Y^0\mid X,D=0\big] 
   \\
            &amp; = E\big[Y\mid X,D=1\big] - E\big[Y\mid X,D=0\big]     
\end{align}\]</span></p>
<p>where each value of <span class="math inline">\(Y\)</span> is determined by the switching equation. Given common support, we get the following estimator:</p>
<p><span class="math display">\[\begin{align}
   \widehat{\delta_{ATE}}= \int \Big(E\big[Y\mid X,D=1\big] - E\big[Y\mid X,D=0\big]\Big)d\Pr(X)
\end{align}\]</span></p>
<p>Whereas we need treatment to be conditionally independent of both potential outcomes to identify the ATE, we need only treatment to be conditionally independent of <span class="math inline">\(Y^0\)</span> to identify the ATT and the fact that there exist some units in the control group for each treatment strata. Note, the reason for the common support assumption is because we are weighting the data; without common support, we cannot calculate the relevant weights.</p>
</div>
<div id="subclassification-exercise-titanic-mathrmdata-set" class="section level3" number="5.1.3">
<h3>
<span class="header-section-number">5.1.3</span> Subclassification exercise: Titanic <span class="math inline">\(\mathrm{data\ set}\)</span><a class="anchor" aria-label="anchor" href="#subclassification-exercise-titanic-mathrmdata-set"><i class="fas fa-link"></i></a>
</h3>
<p>For what we are going to do next, I find it useful to move into actual data. We will use an interesting data set to help us better understand subclassification. As everyone knows, the <em>Titanic</em> ocean cruiser hit an iceberg and sank on its maiden voyage. Slightly more than 700 passengers and crew survived out of the 2,200 people on board. It was a horrible disaster. One of the things about it that was notable, though, was the role that wealth and norms played in passengers’ survival.</p>
<p>Imagine that we wanted to know whether or not being seated in first class made someone more likely to survive. Given that the cruiser contained a variety of levels for seating and that wealth was highly concentrated in the upper decks, it’s easy to see why wealth might have a leg up for survival. But the problem was that women and children were explicitly given priority for boarding the scarce lifeboats. If women and children were more likely to be seated in first class, then maybe differences in survival by first class is simply picking up the effect of that social norm. Perhaps a DAG might help us here, as a DAG can help us outline the sufficient conditions for identifying the causal effect of first class on survival.</p>
<div class="inline-figure"><img src="causal_inference_mixtape_files/figure-html/unnamed-chunk-50-1.png" width="50%" style="display: block; margin: auto;"></div>
<p>Now before we commence, let’s review what this DAG is telling us. This says that being a female made you more likely to be in first class but also made you more likely to survive because lifeboats were more likely to be allocated to women. Furthermore, being a child made you more likely to be in first class and made you more likely to survive. Finally, there are no other confounders, observed or unobserved.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;I’m sure you can think of others, though, in which case this DAG is misleading.&lt;/p&gt;"><sup>79</sup></a></p>
<p>Here we have one direct path (the causal effect) between first class (<span class="math inline">\(D\)</span>) and survival (<span class="math inline">\(Y\)</span>) and that’s <span class="math inline">\(D \rightarrow Y\)</span>. But, we have two backdoor paths. One travels through the variable Child (C): <span class="math inline">\(D \leftarrow C \rightarrow Y\)</span>; the other travels through the variable Woman (W): <span class="math inline">\(D \leftarrow W \rightarrow Y\)</span>. Fortunately for us, our data includes both age and gender, so it is possible to close each backdoor path and therefore satisfy the backdoor criterion. We will use subclassification to do that, but before we do, let’s calculate a naı̈ve simple difference in outcomes (SDO), which is just
<span class="math display">\[
E\big[Y\mid D=1\big] - E\big[Y\mid D=0\big]
\]</span>
for the sample.</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/titanic.do"><code>titanic.do</code></a></em></p>
<div class="sourceCode" id="cb33"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb33-1"><a href="ch4.html#cb33-1" aria-hidden="true"></a><span class="kw">use</span> https:<span class="co">//github.com/scunning1975/mixtape/raw/master/titanic.dta, clear</span></span>
<span id="cb33-2"><a href="ch4.html#cb33-2" aria-hidden="true"></a><span class="kw">gen</span> female=(sex==0)</span>
<span id="cb33-3"><a href="ch4.html#cb33-3" aria-hidden="true"></a><span class="kw">label</span> <span class="kw">variable</span> female <span class="st">"Female"</span></span>
<span id="cb33-4"><a href="ch4.html#cb33-4" aria-hidden="true"></a><span class="kw">gen</span> male=(sex==1)</span>
<span id="cb33-5"><a href="ch4.html#cb33-5" aria-hidden="true"></a><span class="kw">label</span> <span class="kw">variable</span> male <span class="st">"Male"</span></span>
<span id="cb33-6"><a href="ch4.html#cb33-6" aria-hidden="true"></a><span class="kw">gen</span>     <span class="fu">s</span>=1 <span class="kw">if</span> (female==1 &amp; age==1)</span>
<span id="cb33-7"><a href="ch4.html#cb33-7" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">s</span>=2 <span class="kw">if</span> (female==1 &amp; age==0)</span>
<span id="cb33-8"><a href="ch4.html#cb33-8" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">s</span>=3 <span class="kw">if</span> (female==0 &amp; age==1)</span>
<span id="cb33-9"><a href="ch4.html#cb33-9" aria-hidden="true"></a><span class="kw">replace</span> <span class="fu">s</span>=4 <span class="kw">if</span> (female==0 &amp; age==0)</span>
<span id="cb33-10"><a href="ch4.html#cb33-10" aria-hidden="true"></a><span class="kw">gen</span>     <span class="kw">d</span>=1 <span class="kw">if</span> <span class="kw">class</span>==1</span>
<span id="cb33-11"><a href="ch4.html#cb33-11" aria-hidden="true"></a><span class="kw">replace</span> <span class="kw">d</span>=0 <span class="kw">if</span> <span class="kw">class</span>!=1</span>
<span id="cb33-12"><a href="ch4.html#cb33-12" aria-hidden="true"></a><span class="kw">summarize</span> survived <span class="kw">if</span> <span class="kw">d</span>==1</span>
<span id="cb33-13"><a href="ch4.html#cb33-13" aria-hidden="true"></a><span class="kw">gen</span> ey1=<span class="fu">r</span>(<span class="kw">mean</span>)</span>
<span id="cb33-14"><a href="ch4.html#cb33-14" aria-hidden="true"></a><span class="kw">summarize</span> survived <span class="kw">if</span> <span class="kw">d</span>==0</span>
<span id="cb33-15"><a href="ch4.html#cb33-15" aria-hidden="true"></a><span class="kw">gen</span> ey0=<span class="fu">r</span>(<span class="kw">mean</span>)</span>
<span id="cb33-16"><a href="ch4.html#cb33-16" aria-hidden="true"></a><span class="kw">gen</span> sdo=ey1-ey0</span>
<span id="cb33-17"><a href="ch4.html#cb33-17" aria-hidden="true"></a>su sdo</span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/titanic.R"><code>titanic.R</code></a></em></p>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://haven.tidyverse.org">haven</a></span><span class="op">)</span>

<span class="va">read_data</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">df</span><span class="op">)</span>
<span class="op">{</span>
  <span class="va">full_path</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"https://raw.github.com/scunning1975/mixtape/master/"</span>, 
                     <span class="va">df</span>, sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span>
  <span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/read_dta.html">read_dta</a></span><span class="op">(</span><span class="va">full_path</span><span class="op">)</span>
  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">df</span><span class="op">)</span>
<span class="op">}</span>

<span class="va">titanic</span> <span class="op">&lt;-</span> <span class="fu">read_data</span><span class="op">(</span><span class="st">"titanic.dta"</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>d <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">class</span> <span class="op">==</span> <span class="fl">1</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span><span class="op">)</span>

<span class="va">ey1</span> <span class="op">&lt;-</span> <span class="va">titanic</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">d</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html">pull</a></span><span class="op">(</span><span class="va">survived</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">)</span>

<span class="va">ey0</span> <span class="op">&lt;-</span> <span class="va">titanic</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">d</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html">pull</a></span><span class="op">(</span><span class="va">survived</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">)</span>

<span class="va">sdo</span> <span class="op">&lt;-</span> <span class="va">ey1</span> <span class="op">-</span> <span class="va">ey0</span></code></pre></div>
<p>Using the data set on the <em>Titanic</em>, we calculate a simple difference in mean outcomes (SDO), which finds that being seated in first class raised the probability of survival by 35.4%. But note, since this does not adjust for observable confounders age and gender, it is a biased estimate of the ATE. So next we use subclassification weighting to control for these confounders. Here are the steps that will entail:</p>
<ol style="list-style-type: decimal">
<li><p>Stratify the data into four groups: young males, young females, old males, old females.</p></li>
<li><p>Calculate the difference in survival probabilities for each group.</p></li>
<li><p>Calculate the number of people in the non-first-class groups and divide by the total number of non-first-class population. These are our strata-specific weights.</p></li>
<li><p>Calculate the weighted average survival rate using the strata weights.</p></li>
</ol>
<p>Let’s review this with some code so that you can better understand what these four steps actually entail.</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/titanic_subclassification.do"><code>titanic_subclassification.do</code></a></em></p>
<div class="sourceCode" id="cb35"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb35-1"><a href="ch4.html#cb35-1" aria-hidden="true"></a>* Subclassification</span>
<span id="cb35-2"><a href="ch4.html#cb35-2" aria-hidden="true"></a>cap n <span class="kw">drop</span> ey1 ey0</span>
<span id="cb35-3"><a href="ch4.html#cb35-3" aria-hidden="true"></a>su survived <span class="kw">if</span> <span class="fu">s</span>==1 &amp; <span class="kw">d</span>==1</span>
<span id="cb35-4"><a href="ch4.html#cb35-4" aria-hidden="true"></a><span class="kw">gen</span> ey11=<span class="fu">r</span>(<span class="kw">mean</span>)</span>
<span id="cb35-5"><a href="ch4.html#cb35-5" aria-hidden="true"></a><span class="kw">label</span> <span class="kw">variable</span> ey11 <span class="st">"Average survival for male child in treatment"</span></span>
<span id="cb35-6"><a href="ch4.html#cb35-6" aria-hidden="true"></a>su survived <span class="kw">if</span> <span class="fu">s</span>==1 &amp; <span class="kw">d</span>==0</span>
<span id="cb35-7"><a href="ch4.html#cb35-7" aria-hidden="true"></a><span class="kw">gen</span> ey10=<span class="fu">r</span>(<span class="kw">mean</span>)</span>
<span id="cb35-8"><a href="ch4.html#cb35-8" aria-hidden="true"></a><span class="kw">label</span> <span class="kw">variable</span> ey10 <span class="st">"Average survival for male child in control"</span></span>
<span id="cb35-9"><a href="ch4.html#cb35-9" aria-hidden="true"></a><span class="kw">gen</span> diff1=ey11-ey10</span>
<span id="cb35-10"><a href="ch4.html#cb35-10" aria-hidden="true"></a><span class="kw">label</span> <span class="kw">variable</span> diff1 <span class="st">"Difference in survival for male children"</span></span>
<span id="cb35-11"><a href="ch4.html#cb35-11" aria-hidden="true"></a>su survived <span class="kw">if</span> <span class="fu">s</span>==2 &amp; <span class="kw">d</span>==1</span>
<span id="cb35-12"><a href="ch4.html#cb35-12" aria-hidden="true"></a><span class="kw">gen</span> ey21=<span class="fu">r</span>(<span class="kw">mean</span>)</span>
<span id="cb35-13"><a href="ch4.html#cb35-13" aria-hidden="true"></a>su survived <span class="kw">if</span> <span class="fu">s</span>==2 &amp; <span class="kw">d</span>==0</span>
<span id="cb35-14"><a href="ch4.html#cb35-14" aria-hidden="true"></a><span class="kw">gen</span> ey20=<span class="fu">r</span>(<span class="kw">mean</span>)</span>
<span id="cb35-15"><a href="ch4.html#cb35-15" aria-hidden="true"></a><span class="kw">gen</span> diff2=ey21-ey20</span>
<span id="cb35-16"><a href="ch4.html#cb35-16" aria-hidden="true"></a>su survived <span class="kw">if</span> <span class="fu">s</span>==3 &amp; <span class="kw">d</span>==1</span>
<span id="cb35-17"><a href="ch4.html#cb35-17" aria-hidden="true"></a><span class="kw">gen</span> ey31=<span class="fu">r</span>(<span class="kw">mean</span>)</span>
<span id="cb35-18"><a href="ch4.html#cb35-18" aria-hidden="true"></a>su survived <span class="kw">if</span> <span class="fu">s</span>==3 &amp; <span class="kw">d</span>==0</span>
<span id="cb35-19"><a href="ch4.html#cb35-19" aria-hidden="true"></a><span class="kw">gen</span> ey30=<span class="fu">r</span>(<span class="kw">mean</span>)</span>
<span id="cb35-20"><a href="ch4.html#cb35-20" aria-hidden="true"></a><span class="kw">gen</span> diff3=ey31-ey30</span>
<span id="cb35-21"><a href="ch4.html#cb35-21" aria-hidden="true"></a>su survived <span class="kw">if</span> <span class="fu">s</span>==4 &amp; <span class="kw">d</span>==1</span>
<span id="cb35-22"><a href="ch4.html#cb35-22" aria-hidden="true"></a><span class="kw">gen</span> ey41=<span class="fu">r</span>(<span class="kw">mean</span>)</span>
<span id="cb35-23"><a href="ch4.html#cb35-23" aria-hidden="true"></a>su survived <span class="kw">if</span> <span class="fu">s</span>==4 &amp; <span class="kw">d</span>==0</span>
<span id="cb35-24"><a href="ch4.html#cb35-24" aria-hidden="true"></a><span class="kw">gen</span> ey40=<span class="fu">r</span>(<span class="kw">mean</span>)</span>
<span id="cb35-25"><a href="ch4.html#cb35-25" aria-hidden="true"></a><span class="kw">gen</span> diff4=ey41-ey40</span>
<span id="cb35-26"><a href="ch4.html#cb35-26" aria-hidden="true"></a><span class="fu">count</span> <span class="kw">if</span> <span class="fu">s</span>==1 &amp; <span class="kw">d</span>==0</span>
<span id="cb35-27"><a href="ch4.html#cb35-27" aria-hidden="true"></a><span class="fu">count</span> <span class="kw">if</span> <span class="fu">s</span>==2 &amp; <span class="kw">d</span>==0</span>
<span id="cb35-28"><a href="ch4.html#cb35-28" aria-hidden="true"></a><span class="fu">count</span> <span class="kw">if</span> <span class="fu">s</span>==3 &amp; <span class="kw">d</span>==0</span>
<span id="cb35-29"><a href="ch4.html#cb35-29" aria-hidden="true"></a><span class="fu">count</span> <span class="kw">if</span> <span class="fu">s</span>==4 &amp; <span class="kw">d</span>==0</span>
<span id="cb35-30"><a href="ch4.html#cb35-30" aria-hidden="true"></a><span class="fu">count</span></span>
<span id="cb35-31"><a href="ch4.html#cb35-31" aria-hidden="true"></a><span class="kw">gen</span> wt1=425/2201</span>
<span id="cb35-32"><a href="ch4.html#cb35-32" aria-hidden="true"></a><span class="kw">gen</span> wt2=45/2201</span>
<span id="cb35-33"><a href="ch4.html#cb35-33" aria-hidden="true"></a><span class="kw">gen</span> wt3=1667/2201</span>
<span id="cb35-34"><a href="ch4.html#cb35-34" aria-hidden="true"></a><span class="kw">gen</span> wt4=64/2201</span>
<span id="cb35-35"><a href="ch4.html#cb35-35" aria-hidden="true"></a><span class="kw">gen</span> wate=diff1*wt1 + diff2*wt2 + diff3*wt3 + diff4*wt4</span>
<span id="cb35-36"><a href="ch4.html#cb35-36" aria-hidden="true"></a><span class="kw">sum</span> wate sdo</span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/titanic_subclassification.R"><code>titanic_subclassification.R</code></a></em></p>
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">stargazer</span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://magrittr.tidyverse.org">magrittr</a></span><span class="op">)</span> <span class="co"># for %$% pipes</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://haven.tidyverse.org">haven</a></span><span class="op">)</span>

<span class="va">titanic</span> <span class="op">&lt;-</span> <span class="fu">read_data</span><span class="op">(</span><span class="st">"titanic.dta"</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>d <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">class</span> <span class="op">==</span> <span class="fl">1</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span><span class="op">)</span>


<span class="va">titanic</span> <span class="op">%&lt;&gt;%</span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>s <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">sex</span> <span class="op">==</span> <span class="fl">0</span> <span class="op">&amp;</span> <span class="va">age</span> <span class="op">==</span> <span class="fl">1</span> <span class="op">~</span> <span class="fl">1</span>,
                       <span class="va">sex</span> <span class="op">==</span> <span class="fl">0</span> <span class="op">&amp;</span> <span class="va">age</span> <span class="op">==</span> <span class="fl">0</span> <span class="op">~</span> <span class="fl">2</span>,
                       <span class="va">sex</span> <span class="op">==</span> <span class="fl">1</span> <span class="op">&amp;</span> <span class="va">age</span> <span class="op">==</span> <span class="fl">1</span> <span class="op">~</span> <span class="fl">3</span>,
                       <span class="va">sex</span> <span class="op">==</span> <span class="fl">1</span> <span class="op">&amp;</span> <span class="va">age</span> <span class="op">==</span> <span class="fl">0</span> <span class="op">~</span> <span class="fl">4</span>,
                       <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span><span class="op">)</span>

<span class="va">ey11</span> <span class="op">&lt;-</span> <span class="va">titanic</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">s</span> <span class="op">==</span> <span class="fl">1</span> <span class="op">&amp;</span> <span class="va">d</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span> <span class="op">%$%</span>
  <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">survived</span><span class="op">)</span>

<span class="va">ey10</span> <span class="op">&lt;-</span> <span class="va">titanic</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">s</span> <span class="op">==</span> <span class="fl">1</span> <span class="op">&amp;</span> <span class="va">d</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span> <span class="op">%$%</span>
  <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">survived</span><span class="op">)</span>

<span class="va">ey21</span> <span class="op">&lt;-</span> <span class="va">titanic</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">s</span> <span class="op">==</span> <span class="fl">2</span> <span class="op">&amp;</span> <span class="va">d</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span> <span class="op">%$%</span>
  <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">survived</span><span class="op">)</span>

<span class="va">ey20</span> <span class="op">&lt;-</span> <span class="va">titanic</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">s</span> <span class="op">==</span> <span class="fl">2</span> <span class="op">&amp;</span> <span class="va">d</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span> <span class="op">%$%</span>
  <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">survived</span><span class="op">)</span>

<span class="va">ey31</span> <span class="op">&lt;-</span> <span class="va">titanic</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">s</span> <span class="op">==</span> <span class="fl">3</span> <span class="op">&amp;</span> <span class="va">d</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span> <span class="op">%$%</span>
  <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">survived</span><span class="op">)</span>

<span class="va">ey30</span> <span class="op">&lt;-</span> <span class="va">titanic</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">s</span> <span class="op">==</span> <span class="fl">3</span> <span class="op">&amp;</span> <span class="va">d</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span> <span class="op">%$%</span>
  <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">survived</span><span class="op">)</span>

<span class="va">ey41</span> <span class="op">&lt;-</span> <span class="va">titanic</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">s</span> <span class="op">==</span> <span class="fl">4</span> <span class="op">&amp;</span> <span class="va">d</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span> <span class="op">%$%</span>
  <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">survived</span><span class="op">)</span>

<span class="va">ey40</span> <span class="op">&lt;-</span> <span class="va">titanic</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">s</span> <span class="op">==</span> <span class="fl">4</span> <span class="op">&amp;</span> <span class="va">d</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span> <span class="op">%$%</span>
  <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">survived</span><span class="op">)</span>

<span class="va">diff1</span> <span class="op">=</span> <span class="va">ey11</span> <span class="op">-</span> <span class="va">ey10</span>
<span class="va">diff2</span> <span class="op">=</span> <span class="va">ey21</span> <span class="op">-</span> <span class="va">ey20</span>
<span class="va">diff3</span> <span class="op">=</span> <span class="va">ey31</span> <span class="op">-</span> <span class="va">ey30</span>
<span class="va">diff4</span> <span class="op">=</span> <span class="va">ey41</span> <span class="op">-</span> <span class="va">ey40</span>

<span class="va">obs</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">titanic</span><span class="op">)</span>

<span class="va">wt1</span> <span class="op">&lt;-</span> <span class="va">titanic</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">s</span> <span class="op">==</span> <span class="fl">1</span> <span class="op">&amp;</span> <span class="va">d</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span> <span class="op">%$%</span>
  <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">.</span><span class="op">)</span><span class="op">/</span><span class="va">obs</span>

<span class="va">wt2</span> <span class="op">&lt;-</span> <span class="va">titanic</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">s</span> <span class="op">==</span> <span class="fl">2</span> <span class="op">&amp;</span> <span class="va">d</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span> <span class="op">%$%</span>
  <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">.</span><span class="op">)</span><span class="op">/</span><span class="va">obs</span>

<span class="va">wt3</span> <span class="op">&lt;-</span> <span class="va">titanic</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">s</span> <span class="op">==</span> <span class="fl">3</span> <span class="op">&amp;</span> <span class="va">d</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span> <span class="op">%$%</span>
  <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">.</span><span class="op">)</span><span class="op">/</span><span class="va">obs</span>

<span class="va">wt4</span> <span class="op">&lt;-</span> <span class="va">titanic</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">s</span> <span class="op">==</span> <span class="fl">4</span> <span class="op">&amp;</span> <span class="va">d</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span> <span class="op">%$%</span>
  <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">.</span><span class="op">)</span><span class="op">/</span><span class="va">obs</span>

<span class="va">wate</span> <span class="op">=</span> <span class="va">diff1</span><span class="op">*</span><span class="va">wt1</span> <span class="op">+</span> <span class="va">diff2</span><span class="op">*</span><span class="va">wt2</span> <span class="op">+</span> <span class="va">diff3</span><span class="op">*</span><span class="va">wt3</span> <span class="op">+</span> <span class="va">diff4</span><span class="op">*</span><span class="va">wt4</span>

<span class="fu"><a href="https://rdrr.io/pkg/stargazer/man/stargazer.html">stargazer</a></span><span class="op">(</span><span class="va">wate</span>, <span class="va">sdo</span>, type <span class="op">=</span> <span class="st">"text"</span><span class="op">)</span></code></pre></div>
<p>Here we find that once we condition on the confounders gender and age, first-class seating has a much lower probability of survival associated with it (though frankly, still large). The weighted ATE is 16.1%, versus the SDO, which is 35.4%.</p>
</div>
<div id="curse-of-dimensionality" class="section level3" number="5.1.4">
<h3>
<span class="header-section-number">5.1.4</span> Curse of dimensionality<a class="anchor" aria-label="anchor" href="#curse-of-dimensionality"><i class="fas fa-link"></i></a>
</h3>
<p>Here we’ve been assuming two covariates, each of which has two possible set of values. But this was for convenience. Our data set, for instance, only came to us with two possible values for age—child and adult. But what if it had come to us with multiple values for age, like specific age? Then once we condition on individual age and gender, it’s entirely likely that we will not have the information necessary to calculate differences within strata, and therefore be unable to calculate the strata-specific weights that we need for subclassification.</p>
<p>For this next part, let’s assume that we have precise data on <em>Titanic</em> survivor ages. But because this will get incredibly laborious, let’s just focus on a few of them.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:cochran5">Table 5.5: </span> Subclassification example of Titanic survival for large <span class="math inline">\(K\)</span>
</caption>
<thead><tr class="header">
<th align="left">Age and Gender</th>
<th align="center">Survival Prob. 1st Class</th>
<th align="center">Controls</th>
<th align="center">Diff</th>
<th align="center"># of 1st Class</th>
<th align="center"># of Controls</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Male 11-yo</td>
<td align="center">1.0</td>
<td align="center">0</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="left">Male 12-yo</td>
<td align="center">–</td>
<td align="center">1</td>
<td align="center">–</td>
<td align="center">0</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="left">Male 13-yo</td>
<td align="center">1.0</td>
<td align="center">0</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="left">Male 14-yo</td>
<td align="center">–</td>
<td align="center">0.25</td>
<td align="center">–</td>
<td align="center">0</td>
<td align="center">4</td>
</tr>
<tr class="odd">
<td align="left">…</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table></div>
<p>Here we see an example of the common support assumption being violated. The common support assumption requires that for each strata, there exist observations in both the treatment and control group, but as you can see, there are not any 12-year-old male passengers in first class. Nor are there any 14-year-old male passengers in first class. And if we were to do this for every combination of age and gender, we would find that this problem was quite common. Thus, we cannot estimate the ATE using subclassification. The problem is that our stratifying variable has too many dimensions, and as a result, we have sparseness in some cells because the sample is too small.</p>
<p>But let’s say that the problem was always on the treatment group, not the control group. That is, let’s assume that there is always <em>someone</em> in the control group for a given combination of gender and age, but there isn’t always for the treatment group. Then we can calculate the ATT. Because as you see in this table, for those two strata, 11-year-olds and 13-year-olds, there are both treatment and control group values for the calculation. So long as there exist controls for a given treatment strata, we can calculate the ATT. The equation to do so can be compactly written as:</p>
<p><span class="math display">\[
\widehat{\delta}_{ATT} = \sum_{k=1}^K\Big(\overline{Y}^{1,k} - \overline{Y}^{0,k}\Big)\times \bigg( \dfrac{N^k_T}{N_T} \bigg )
\]</span></p>
<p>We’ve seen a problem that arises with subclassification—in a finite sample, subclassification becomes less feasible as the number of covariates grows, because as <span class="math inline">\(K\)</span> grows, the data becomes sparse. This is most likely caused by our sample being too small relative to the size of our covariate matrix. We will at some point be missing values, in other words, for those <span class="math inline">\(K\)</span> categories. Imagine if we tried to add a third strata, say, race (black and white). Then we’d have two age categories, two gender categories, and two race categories, giving us eight possibilities. In this small sample, we probably will end up with many cells having missing information. This is called the <em>curse of dimensionality</em>. If sparseness occurs, it means many cells may contain either only treatment units or only control units, but not both. If that happens, we can’t use subclassification, because we do not have common support. And therefore we are left searching for an alternative method to satisfy the backdoor criterion.</p>
</div>
</div>
<div id="exact-matching" class="section level2" number="5.2">
<h2>
<span class="header-section-number">5.2</span> Exact Matching<a class="anchor" aria-label="anchor" href="#exact-matching"><i class="fas fa-link"></i></a>
</h2>
<p>Subclassification uses the difference between treatment and control group units and achieves covariate balance by using the <span class="math inline">\(K\)</span> probability weights to weight the averages. It’s a simple method, but it has the aforementioned problem of the curse of dimensionality. And probably, that’s going to be an issue in any research you undertake because it may not be merely one variable you’re worried about but several—in which case, you’ll already be running into the curse. But the thing to emphasize here is that the subclassification method is using the raw data, but weighting it so as to achieve balance. We are weighting the differences, and then summing over those weighted differences.</p>
<p>But there are alternative approaches. For instance, what if we estimated <span class="math inline">\(\widehat{\delta}_{ATT}\)</span> by <em>imputing</em> the missing potential outcomes by conditioning on the confounding, observed covariate? Specifically, what if we filled in the missing potential outcome for each treatment unit using a control group unit that was “closest” to the treatment group unit for some <span class="math inline">\(X\)</span> confounder? This would give us estimates of all the counterfactuals from which we could simply take the average over the differences. As we will show, this will also achieve covariate balance. This method is called <em>matching</em>.</p>
<p>There are two broad types of matching that we will consider: exact matching and approximate matching. We will first start by describing exact matching. Much of what I am going to be discussing is based on <span class="citation">Abadie and Imbens (<a href="references.html#ref-Abadie2006" role="doc-biblioref">2006</a>)</span>.</p>
<p>A simple matching estimator is the following:</p>
<p><span class="math display">\[
\widehat{\delta}_{ATT} = \dfrac{1}{N_T} \sum_{D_i=1}(Y_i - Y_{j(i)})
\]</span></p>
<p>where <span class="math inline">\(Y_{j(i)}\)</span> is the <span class="math inline">\(j\)</span>th unit matched to the <span class="math inline">\(i\)</span>th unit based on the <span class="math inline">\(j\)</span>th being “closest to” the <span class="math inline">\(i\)</span>th unit for some <span class="math inline">\(X\)</span> covariate. For instance, let’s say that a unit in the treatment group has a covariate with a value of 2 and we find another unit in the control group (exactly one unit) with a covariate value of 2. Then we will impute the treatment unit’s missing counterfactual with the matched unit’s, and take a difference.</p>
<p>But, what if there’s more than one variable “closest to” the <span class="math inline">\(i\)</span>th unit? For instance, say that the same <span class="math inline">\(i\)</span>th unit has a covariate value of 2 and we find two <span class="math inline">\(j\)</span> units with a value of 2. What can we then do? Well, one option is to simply take the average of those two units’ <span class="math inline">\(Y\)</span> outcome value. But what if we found 3 close units? What if we found 4? And so on. However many matches <span class="math inline">\(M\)</span> that we find, we would assign the average outcome <span class="math inline">\(\left(\dfrac{1}{M} \right)\)</span> as the counterfactual for the treatment group unit.</p>
<p>Notationally, we can describe this estimator as</p>
<p><span class="math display">\[
   \widehat{\delta}_{ATT} = \dfrac{1}{N_T} \sum_{D_i=1} \bigg ( Y_i - \bigg [\dfrac{1}{M} \sum_{m=1}^M Y_{j_m(1)} \bigg ] \bigg )
\]</span></p>
<p>This estimator really isn’t too different from the one just before it; the main difference is that this one averages over several close matches as opposed to just picking one. This approach works well when we can find a number of good matches for each treatment group unit. We usually define <span class="math inline">\(M\)</span> to be small, like <span class="math inline">\(M=2\)</span>. If <em>M</em> is greater than 2, then we may simply randomly select two units to average outcomes over.</p>
<p>Those were both ATT estimators. You can tell that these are <span class="math inline">\(\widehat{\delta}_{ATT}\)</span> estimators because of the summing over the treatment group.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Notice the &lt;span class="math inline"&gt;\(D_i=1\)&lt;/span&gt; in the subscript of the summation operator.&lt;/p&gt;'><sup>80</sup></a> But we can also estimate the ATE. But note, when estimating the ATE, we are filling in both missing control group units like before and missing treatment group units. If observation <span class="math inline">\(i\)</span> is treated, in other words, then we need to fill in the missing <span class="math inline">\(Y^0_i\)</span> using the control matches, and if the observation <span class="math inline">\(i\)</span> is a control group unit, then we need to fill in the missing <span class="math inline">\(Y^1_i\)</span> using the treatment group matches. The estimator is below. It looks scarier than it really is. It’s actually a very compact, nicely-written-out estimator equation.</p>
<p><span class="math display">\[
   \widehat{\delta}_{ATE} = \dfrac{1}{N} \sum_{i=1}^N (2D_i - 1) \bigg [ Y_i - \bigg ( \dfrac{1}{M} \sum_{m=1}^M Y_{j_m(i)} \bigg ) \bigg ]
\]</span></p>
<p>The <span class="math inline">\(2D_i-1\)</span> is the nice little trick. When <span class="math inline">\(D_i=1\)</span>, then that leading term becomes a <span class="math inline">\(1\)</span>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;&lt;span class="math inline"&gt;\(2 \times 1 - 1 = 1.\)&lt;/span&gt;&lt;/p&gt;'><sup>81</sup></a> And when <span class="math inline">\(D_i=0\)</span>, then that leading term becomes a negative 1, and the outcomes reverse order so that the treatment observation can be imputed. Nice little mathematical form!</p>
<p>Let’s see this work in action by working with an example. Table <a href="ch4.html#tab:trainee1">5.6</a> shows two samples: a list of participants in a job trainings program and a list of non-participants, or non-trainees. The left-hand group is the treatment group and the right-hand group is the control group. The matching algorithm that we defined earlier will create a third group called the <em>matched sample</em>, consisting of each treatment group unit’s matched counterfactual. Here we will match on the age of the participant.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:trainee1">Table 5.6: </span> Training example with exact matching</caption>
<tbody>
<tr class="odd">
<td align="center"><strong>Trainees</strong></td>
<td></td>
<td></td>
<td align="center"><strong>Non-Trainees</strong></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td align="center"><strong>Unit</strong></td>
<td><strong>Age</strong></td>
<td><strong>Earnings</strong></td>
<td align="center"><strong>Unit</strong></td>
<td><strong>Age</strong></td>
<td><strong>Earnings</strong></td>
</tr>
<tr class="odd">
<td align="center">1</td>
<td>18</td>
<td>9500</td>
<td align="center">1</td>
<td>20</td>
<td>8500</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td>29</td>
<td>12250</td>
<td align="center">2</td>
<td>27</td>
<td>10075</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td>24</td>
<td>11000</td>
<td align="center">3</td>
<td>21</td>
<td>8725</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td>27</td>
<td>11750</td>
<td align="center">4</td>
<td>39</td>
<td>12775</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td>33</td>
<td>13250</td>
<td align="center">5</td>
<td>38</td>
<td>12550</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td>22</td>
<td>10500</td>
<td align="center">6</td>
<td>29</td>
<td>10525</td>
</tr>
<tr class="odd">
<td align="center">7</td>
<td>19</td>
<td>9750</td>
<td align="center">7</td>
<td>39</td>
<td>12775</td>
</tr>
<tr class="even">
<td align="center">8</td>
<td>20</td>
<td>10000</td>
<td align="center">8</td>
<td>33</td>
<td>11425</td>
</tr>
<tr class="odd">
<td align="center">9</td>
<td>21</td>
<td>10250</td>
<td align="center">9</td>
<td>24</td>
<td>9400</td>
</tr>
<tr class="even">
<td align="center">10</td>
<td>30</td>
<td>12500</td>
<td align="center">10</td>
<td>30</td>
<td>10750</td>
</tr>
<tr class="odd">
<td align="center"></td>
<td></td>
<td></td>
<td align="center">11</td>
<td>33</td>
<td>11425</td>
</tr>
<tr class="even">
<td align="center"></td>
<td></td>
<td></td>
<td align="center">12</td>
<td>36</td>
<td>12100</td>
</tr>
<tr class="odd">
<td align="center"></td>
<td></td>
<td></td>
<td align="center">13</td>
<td>22</td>
<td>8950</td>
</tr>
<tr class="even">
<td align="center"></td>
<td></td>
<td></td>
<td align="center">14</td>
<td>18</td>
<td>8050</td>
</tr>
<tr class="odd">
<td align="center"></td>
<td></td>
<td></td>
<td align="center">15</td>
<td>43</td>
<td>13675</td>
</tr>
<tr class="even">
<td align="center"></td>
<td></td>
<td></td>
<td align="center">16</td>
<td>39</td>
<td>12775</td>
</tr>
<tr class="odd">
<td align="center"></td>
<td></td>
<td></td>
<td align="center">17</td>
<td>19</td>
<td>8275</td>
</tr>
<tr class="even">
<td align="center"></td>
<td></td>
<td></td>
<td align="center">18</td>
<td>30</td>
<td>9000</td>
</tr>
<tr class="odd">
<td align="center"></td>
<td></td>
<td></td>
<td align="center">19</td>
<td>51</td>
<td>15475</td>
</tr>
<tr class="even">
<td align="center"></td>
<td></td>
<td></td>
<td align="center">20</td>
<td>48</td>
<td>14800</td>
</tr>
<tr class="odd">
<td align="center">Mean</td>
<td>24.3</td>
<td>$11,075</td>
<td align="center"></td>
<td>31.95</td>
<td>$11,101.25</td>
</tr>
</tbody>
</table></div>
<p>Before we do this, though, I want to show you how the ages of the trainees differ on average from the ages of the non-trainees. We can see that in Table <a href="ch4.html#tab:trainee1">5.6</a>—the average age of the participants is 24.3 years, and the average age of the non-participants is 31.95 years. Thus, the people in the control group are older, and since wages typically rise with age, we may suspect that part of the reason their average earnings are higher ($11,075 vs. $11,101) is because the control group is older. We say that the two groups are not <em>exchangeable</em> because the covariate is not <em>balanced</em>. Let’s look at the age distribution. To illustrate this, we need to download the data first. We will create two histograms—the distribution of age for treatment and non-trainee group—as well as summarize earnings for each group. That information is also displayed in Figure <a href="ch4.html#fig:trainee1">5.1</a>.</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/training_example.do"><code>training_example.do</code></a></em></p>
<div class="sourceCode" id="cb37"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb37-1"><a href="ch4.html#cb37-1" aria-hidden="true"></a><span class="kw">use</span> https:<span class="co">//github.com/scunning1975/mixtape/raw/master/training_example.dta, clear</span></span>
<span id="cb37-2"><a href="ch4.html#cb37-2" aria-hidden="true"></a><span class="kw">histogram</span> age_treat, <span class="bn">bin</span>(10) <span class="kw">frequency</span></span>
<span id="cb37-3"><a href="ch4.html#cb37-3" aria-hidden="true"></a><span class="kw">histogram</span> age_control, <span class="bn">bin</span>(10) <span class="kw">frequency</span></span>
<span id="cb37-4"><a href="ch4.html#cb37-4" aria-hidden="true"></a>su age_treat age_control</span>
<span id="cb37-5"><a href="ch4.html#cb37-5" aria-hidden="true"></a>su earnings_treat earnings_control</span>
<span id="cb37-6"><a href="ch4.html#cb37-6" aria-hidden="true"></a></span>
<span id="cb37-7"><a href="ch4.html#cb37-7" aria-hidden="true"></a><span class="kw">histogram</span> age_treat, <span class="bn">bin</span>(10) <span class="kw">frequency</span></span>
<span id="cb37-8"><a href="ch4.html#cb37-8" aria-hidden="true"></a><span class="kw">histogram</span> age_matched, <span class="bn">bin</span>(10) <span class="kw">frequency</span></span>
<span id="cb37-9"><a href="ch4.html#cb37-9" aria-hidden="true"></a>su age_treat age_control</span>
<span id="cb37-10"><a href="ch4.html#cb37-10" aria-hidden="true"></a>su earnings_matched earnings_matched</span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/training_example.R"><code>training_example.R</code></a></em></p>
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://haven.tidyverse.org">haven</a></span><span class="op">)</span>

<span class="va">read_data</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">df</span><span class="op">)</span>
<span class="op">{</span>
  <span class="va">full_path</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"https://raw.github.com/scunning1975/mixtape/master/"</span>, 
                     <span class="va">df</span>, sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span>
  <span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/read_dta.html">read_dta</a></span><span class="op">(</span><span class="va">full_path</span><span class="op">)</span>
  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">df</span><span class="op">)</span>
<span class="op">}</span>

<span class="va">training_example</span> <span class="op">&lt;-</span> <span class="fu">read_data</span><span class="op">(</span><span class="st">"training_example.dta"</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/slice.html">slice</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">20</span><span class="op">)</span>

<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">training_example</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">age_treat</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_histogram.html">stat_bin</a></span><span class="op">(</span>bins <span class="op">=</span> <span class="fl">10</span>, na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>

<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">training_example</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">age_control</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_histogram.html">geom_histogram</a></span><span class="op">(</span>bins <span class="op">=</span> <span class="fl">10</span>, na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></code></pre></div>
<div class="figure">
<span id="fig:trainee1"></span>
<img src="graphics/job_trainings.jpg" alt="Covariate distribution by job trainings and control." width="100%"><img src="graphics/job_controls.jpg" alt="Covariate distribution by job trainings and control." width="100%"><p class="caption">
Figure 5.1: Covariate distribution by job trainings and control.
</p>
</div>
<p>As you can see from Figure <a href="ch4.html#fig:trainee1">5.1</a>, these two populations not only have different means (Table <a href="ch4.html#tab:trainee1">5.6</a>); the entire distribution of age across the samples is different. So let’s use our matching algorithm and create the missing counterfactuals for each treatment group unit. This method, since it only imputes the missing units for each treatment unit, will yield an estimate of the <span class="math inline">\(\widehat{\delta}_{ATT}\)</span>.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:trainee2">Table 5.7: </span> Training example with exact matching (including matched sample)</caption>
<tbody>
<tr class="odd">
<td align="center"><strong>Trainees</strong></td>
<td></td>
<td></td>
<td align="center"><strong>Non-Trainees</strong></td>
<td></td>
<td></td>
<td align="center"><strong>Matched Sample</strong></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td align="center"><strong>Unit</strong></td>
<td><strong>Age</strong></td>
<td><strong>Earnings</strong></td>
<td align="center"><strong>Unit</strong></td>
<td><strong>Age</strong></td>
<td><strong>Earnings</strong></td>
<td align="center"><strong>Unit</strong></td>
<td><strong>Age</strong></td>
<td><strong>Earnings</strong></td>
</tr>
<tr class="odd">
<td align="center">1</td>
<td>18</td>
<td>9500</td>
<td align="center">1</td>
<td>20</td>
<td>8500</td>
<td align="center">14</td>
<td>18</td>
<td>8050</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td>29</td>
<td>12250</td>
<td align="center">2</td>
<td>27</td>
<td>10075</td>
<td align="center">6</td>
<td>29</td>
<td>10525</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td>24</td>
<td>11000</td>
<td align="center">3</td>
<td>21</td>
<td>8725</td>
<td align="center">9</td>
<td>24</td>
<td>9400</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td>27</td>
<td>11750</td>
<td align="center">4</td>
<td>39</td>
<td>12775</td>
<td align="center">8</td>
<td>27</td>
<td>10075</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td>33</td>
<td>13250</td>
<td align="center">5</td>
<td>38</td>
<td>12550</td>
<td align="center">11</td>
<td>33</td>
<td>11425</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td>22</td>
<td>10500</td>
<td align="center">6</td>
<td>29</td>
<td>10525</td>
<td align="center">13</td>
<td>22</td>
<td>8950</td>
</tr>
<tr class="odd">
<td align="center">7</td>
<td>19</td>
<td>9750</td>
<td align="center">7</td>
<td>39</td>
<td>12775</td>
<td align="center">17</td>
<td>19</td>
<td>8275</td>
</tr>
<tr class="even">
<td align="center">8</td>
<td>20</td>
<td>10000</td>
<td align="center">8</td>
<td>33</td>
<td>11425</td>
<td align="center">1</td>
<td>20</td>
<td>8500</td>
</tr>
<tr class="odd">
<td align="center">9</td>
<td>21</td>
<td>10250</td>
<td align="center">9</td>
<td>24</td>
<td>9400</td>
<td align="center">3</td>
<td>21</td>
<td>8725</td>
</tr>
<tr class="even">
<td align="center">10</td>
<td>30</td>
<td>12500</td>
<td align="center">10</td>
<td>30</td>
<td>10750</td>
<td align="center">10,18</td>
<td>30</td>
<td>9875</td>
</tr>
<tr class="odd">
<td align="center"></td>
<td></td>
<td></td>
<td align="center">11</td>
<td>33</td>
<td>11425</td>
<td align="center"></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td align="center"></td>
<td></td>
<td></td>
<td align="center">12</td>
<td>36</td>
<td>12100</td>
<td align="center"></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td align="center"></td>
<td></td>
<td></td>
<td align="center">13</td>
<td>22</td>
<td>8950</td>
<td align="center"></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td align="center"></td>
<td></td>
<td></td>
<td align="center">14</td>
<td>18</td>
<td>8050</td>
<td align="center"></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td align="center"></td>
<td></td>
<td></td>
<td align="center">15</td>
<td>43</td>
<td>13675</td>
<td align="center"></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td align="center"></td>
<td></td>
<td></td>
<td align="center">16</td>
<td>39</td>
<td>12775</td>
<td align="center"></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td align="center"></td>
<td></td>
<td></td>
<td align="center">17</td>
<td>19</td>
<td>8275</td>
<td align="center"></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td align="center"></td>
<td></td>
<td></td>
<td align="center">18</td>
<td>30</td>
<td>9000</td>
<td align="center"></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td align="center"></td>
<td></td>
<td></td>
<td align="center">19</td>
<td>51</td>
<td>15475</td>
<td align="center"></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td align="center"></td>
<td></td>
<td></td>
<td align="center">20</td>
<td>48</td>
<td>14800</td>
<td align="center"></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td align="center">Mean</td>
<td>24.3</td>
<td>$11,075</td>
<td align="center"></td>
<td>31.95</td>
<td>$11,101.25</td>
<td align="center"></td>
<td>24.3</td>
<td>$9,380</td>
</tr>
</tbody>
</table></div>
<p>Now let’s move to creating the matched sample. As this is exact matching, the distance traveled to the nearest neighbor will be zero integers. This won’t always be the case, but note that as the control group sample size grows, the likelihood that we find a unit with the same covariate value as one in the treatment group grows. I’ve created a data set like this. The first treatment unit has an age of 18. Searching down through the non-trainees, we find exactly one person with an age of 18, and that’s unit 14. So we move the age and earnings information to the new matched sample columns.</p>
<p>We continue doing that for all units, always moving the control group unit with the closest value on <span class="math inline">\(X\)</span> to fill in the missing counterfactual for each treatment unit. If we run into a situation where there’s more than one control group unit “close,” then we simply average over them. For instance, there are two units in the non-trainees group with an age of 30, and that’s 10 and 18. So we averaged their earnings and matched that average earnings to unit 10. This is filled out in Table <a href="ch4.html#tab:trainee2">5.7</a>.</p>
<p>Now we see that the mean age is the same for both groups. We can also check the overall age distribution (Figure <a href="ch4.html#fig:trainee2">5.2</a>). As you can see, the two groups are <em>exactly balanced</em> on age. We might say the two groups are <em>exchangeable</em>. And the difference in earnings between those in the treatment group and those in the control group is $1,695. That is, we estimate that the causal effect of the program was $1,695 in higher earnings.</p>
<div class="figure">
<span id="fig:trainee2"></span>
<img src="graphics/job_matched.jpg" alt="Covariate distribution by job trainings and matched sample." width="100%"><p class="caption">
Figure 5.2: Covariate distribution by job trainings and matched sample.
</p>
</div>
<p>Let’s summarize what we’ve learned. We’ve been using a lot of different terms, drawn from different authors and different statistical traditions, so I’d like to map them onto one another. The two groups were different in ways that were likely a direction function of potential outcomes. This means that the independence assumption was violated. Assuming that treatment assignment was conditionally random, then matching on <span class="math inline">\(X\)</span> created an exchangeable set of observations—the matched sample—and what characterized this matched sample was <em>balance</em>.</p>
</div>
<div id="approximate-matching" class="section level2" number="5.3">
<h2>
<span class="header-section-number">5.3</span> Approximate Matching<a class="anchor" aria-label="anchor" href="#approximate-matching"><i class="fas fa-link"></i></a>
</h2>
<p>The previous example of matching was relatively simple—find a unit or collection of units that have the same value of some covariate <span class="math inline">\(X\)</span> and substitute their outcomes as some unit <span class="math inline">\(j\)</span>’s counterfactuals. Once you’ve done that, average the differences for an estimate of the ATE.</p>
<p>But what if you couldn’t find another unit with that exact same value? Then you’re in the world of approximate matching.</p>
<div id="nearest-neighbor-covariate-matching" class="section level3" number="5.3.1">
<h3>
<span class="header-section-number">5.3.1</span> Nearest neighbor covariate matching<a class="anchor" aria-label="anchor" href="#nearest-neighbor-covariate-matching"><i class="fas fa-link"></i></a>
</h3>
<p>One of the instances where exact matching can break down is when the number of covariates, <span class="math inline">\(K\)</span>, grows large. And when we have to match on more than one variable but are not using the sub-classification approach, then one of the first things we confront is the concept of <em>distance</em>. What does it mean for one unit’s covariate to be “close” to someone else’s? Furthermore, what does it mean when there are multiple covariates with measurements in multiple dimensions?</p>
<p>Matching on a single covariate is straightforward because distance is measured in terms of the covariate’s own values. For instance, distance in age is simply how close in years or months or days one person is to another person. But what if we have several covariates needed for matching? Say, age and log income. A 1-point change in age is very different from a 1-point change in log income, not to mention that we are now measuring distance in two, not one, dimensions. When the number of matching covariates is more than one, we need a new definition of distance to measure closeness. We begin with the simplest measure of distance, the <em>Euclidean distance</em>:</p>
<p><span class="math display">\[\begin{align}
   ||X_i-X_j|| &amp; = \sqrt{ (X_i-X_j)'(X_i-X_j) }             
   \\
            &amp; = \sqrt{\sum_{n=1}^k (X_{ni} - X_{nj})^2 } 
\end{align}\]</span></p>
<p>The problem with this measure of distance is that the distance measure itself depends on the scale of the variables themselves. For this reason, researchers typically will use some modification of the Euclidean distance, such as the <em>normalized Euclidean distance</em>, or they’ll use a wholly different alternative distance. The normalized Euclidean distance is a commonly used distance, and what makes it different is that the distance of each variable is scaled by the variable’s variance. The distance is measured as:
<span class="math display">\[
||X_i-X_j||=\sqrt{(X_i-X_j)'\widehat{V}^{-1}(X_i-X_j)}
\]</span>
where
<span class="math display">\[
\widehat{V}^{-1} =
   \begin{pmatrix}
       \widehat{\sigma}_1^2 &amp; 0 &amp; \dots  &amp; 0                    \\
       0    &amp; \widehat{\sigma}_2^2 &amp; \dots  &amp; 0                    \\
       \vdots           &amp; \vdots            &amp; \ddots &amp; \vdots               \\
       0    &amp; 0 &amp; \dots  &amp; \widehat{\sigma}_k^2 \\
   \end{pmatrix}
\]</span>
Notice that the normalized Euclidean distance is equal to:
<span class="math display">\[
||X_i - X_j|| = \sqrt{\sum_{n=1}^k \dfrac{(X_{ni} - X_{nj})}{\widehat{\sigma}^2_n}}
\]</span>
Thus, if there are changes in the scale of <span class="math inline">\(X\)</span>, these changes also affect its variance, and so the normalized Euclidean distance does not change.</p>
<p>Finally, there is the <em>Mahalanobis</em> distance, which like the normalized Euclidean distance measure, is a scale-invariant distance metric. It is:
<span class="math display">\[
||X_i-X_j||=\sqrt{ (X_i-X_j)'\widehat{\Sigma}_X^{-1}(X_i - X_j) }
\]</span>
where <span class="math inline">\(\widehat{\Sigma}_X\)</span> is the sample variance-covariance matrix of <span class="math inline">\(X\)</span>.</p>
<p>Basically, more than one covariate creates a lot of headaches. Not only does it create the curse-of-dimensionality problem; it also makes measuring distance harder. All of this creates some challenges for finding a good match in the data. As you can see in each of these distance formulas, there are sometimes going to be matching discrepancies. Sometimes <span class="math inline">\(X_i\neq X_j\)</span>. What does this mean? It means that some unit <span class="math inline">\(i\)</span> has been matched with some unit <span class="math inline">\(j\)</span> on the basis of a similar covariate value of <span class="math inline">\(X=x\)</span>. Maybe unit <span class="math inline">\(i\)</span> has an age of 25, but unit <span class="math inline">\(j\)</span> has an age of 26. Their difference is 1. Sometimes the discrepancies are small, sometimes zero, sometimes large. But, as they move away from zero, they become more problematic for our estimation and introduce bias.</p>
<p>How severe is this bias? First, the good news. What we know is that the matching discrepancies tend to converge to zero as the sample size increases—which is one of the main reasons that approximate matching is so data greedy. It demands a large sample size for the matching discrepancies to be trivially small. But what if there are many covariates? The more covariates, the longer it takes for that convergence to zero to occur. Basically, if it’s hard to find good matches with an <span class="math inline">\(X\)</span> that has a large dimension, then you will need a lot of observations as a result. <em>The larger the dimension, the greater likelihood of matching discrepancies, and the more data you need.</em> So you can take that to the bank—most likely, your matching problem requires a large data set in order to minimize the matching discrepancies.</p>
</div>
<div id="bias-correction" class="section level3" number="5.3.2">
<h3>
<span class="header-section-number">5.3.2</span> Bias correction<a class="anchor" aria-label="anchor" href="#bias-correction"><i class="fas fa-link"></i></a>
</h3>
<p>Speaking of matching discrepancies, what sorts of options are available to us, putting aside seeking a large data set with lots of controls? Well, enter stage left, <span class="citation">Abadie and Imbens (<a href="references.html#ref-Abadie2011" role="doc-biblioref">2011</a>)</span>, who introduced bias-correction techniques with matching estimators when there are matching discrepancies in finite samples. So let’s look at that more closely, as you’ll likely need this in your work.</p>
<p>Everything we’re getting at is suggesting that matching is biased because of these poor matching discrepancies. So let’s derive this bias. First, we write out the sample ATT estimate, and then we subtract out the true ATT. So:
<span class="math display">\[
\widehat{\delta}_{ATT} = \dfrac{1}{N_T} \sum_{D_i=1} (Y_i - Y_{j(i)})
\]</span>
where each <span class="math inline">\(i\)</span> and <span class="math inline">\(j(i)\)</span> units are matched, <span class="math inline">\(X_i \approx X_{j(i)}\)</span> and <span class="math inline">\(D_{j(i)}=0\)</span>. Next we define the conditional expection outcomes</p>
<p><span class="math display">\[\begin{align}
   \mu^0(x) &amp; = E\big[Y\mid X=x, D=0\big] = E\big[Y^0\mid X=x\big] \\
   \mu^1(x) &amp; = E\big[Y\mid X=x, D=1\big] = E\big[Y^1\mid X=x\big] 
\end{align}\]</span></p>
<p>Notice, these are just the expected conditional outcome functions based on the switching equation for both control and treatment groups.</p>
<p>As always, we write out the observed value as a function of expected conditional outcomes and some stochastic element:</p>
<p><span class="math display">\[\begin{align}
   Y_i = \mu^{D_i}(X_i) + \varepsilon_i
\end{align}\]</span></p>
<p>Now rewrite the ATT estimator using the above <span class="math inline">\(\mu\)</span> terms:</p>
<p><span class="math display">\[\begin{align}
   \widehat{\delta}_{ATT}
     &amp; =\dfrac{1}{N_T} \sum_{D_i=1} \big(\mu^1(X_i) + \varepsilon_i\big) - \big(\mu^0(X_{j(i)}\big) + \varepsilon_{j(i)})                             
   \\
     &amp; =\dfrac{1}{N_T} \sum_{D_i=1} \big(\mu^1(X_i) - \mu^0(X_{j(i)})\big) + \dfrac{1}{N_T} \sum_{D_i=1} \big(\varepsilon_i - \varepsilon_{j(i)}\big) 
\end{align}\]</span></p>
<p>Notice, the first line is just the ATT with the stochastic element included from the previous line. And the second line rearranges it so that we get two terms: the estimated ATT plus the average difference in the stochastic terms for the matched sample.</p>
<p>Now we compare this estimator with the true value of ATT.
<span class="math display">\[
\widehat{\delta}_{ATT} - \delta_{ATT} = \dfrac{1}{N_T} \sum_{D_i=1} (\mu^1(X_i) - \mu^0(X_{j(i)}) - \delta_{ATT}
   + \dfrac{1}{N_T} \sum_{D_i=1}\big(\varepsilon_i - \varepsilon_{j(i)}\big)
\]</span>
which, with some simple algebraic manipulation is:</p>
<p><span class="math display">\[\begin{align}
   \widehat{\delta}_{ATT} - \delta_{ATT}
     &amp; = \dfrac{1}{N_T} \sum_{D_i=1} \left( \mu^1(X_i) - {\mu^0(X_i)} - \delta_{ATT}\right) 
   \\
     &amp; + \dfrac{1}{N_T} \sum_{D_i=1} (\varepsilon_i - \varepsilon_{j(i)})                   
   \\
     &amp; + \dfrac{1}{N_T} \sum_{D_i=1} \left( {\mu^0(X_i)} - \mu^0(X_{j(i)}) \right).         
\end{align}\]</span></p>
<p>Applying the central limit theorem and the difference, <span class="math inline">\(\sqrt{N_T}(\widehat{\delta}_{ATT} - \delta_{ATT})\)</span> converges to a normal distribution with zero mean. But:
<span class="math display">\[
E\Big[ \sqrt{N_T} (\widehat{\delta}_{ATT} - \delta_{ATT})\Big] = E\Big[ \sqrt{N_T}(\mu^0(X_i)-\mu^0(X_{j(i)}) )\mid D=1\Big].
\]</span>
Now consider the implications if the number of covariates is large. First, the difference between <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_{j(i)}\)</span> converges to zero slowly. This therefore makes the difference <span class="math inline">\(\mu^0(X_i) - \mu(X_{j(i)})\)</span> converge to zero very slowly. Third, <span class="math inline">\(E[ \sqrt{N_T} (\mu^0(X_i) - \mu^0(X_{j(i)}))\mid D=1]\)</span> may not converge to zero. And fourth, <span class="math inline">\(E[ \sqrt{N_T} (\widehat{\delta}_{ATT} - \delta_{ATT})]\)</span> may not converge to zero.</p>
<p>As you can see, the bias of the matching estimator can be severe depending on the magnitude of these matching discrepancies. However, one piece of good news is that these discrepancies are observed. We can see the degree to which each unit’s matched sample has severe mismatch on the covariates themselves. Second, we can always make the matching discrepancy small by using a large donor pool of untreated units to select our matches, because recall, the likelihood of finding a good match grows as a function of the sample size, and so if we are content to estimating the ATT, then increasing the size of the donor pool can get us out of this mess. But let’s say we can’t do that and the matching discrepancies are large. Then we can apply bias-correction methods to minimize the size of the bias. So let’s see what the bias-correction method looks like. This is based on <span class="citation">Abadie and Imbens (<a href="references.html#ref-Abadie2011" role="doc-biblioref">2011</a>)</span>.</p>
<p>Note that the total bias is made up of the bias associated with each individual unit <span class="math inline">\(i\)</span>. Thus, each treated observation contributes <span class="math inline">\(\mu^0(X_i) - \mu^0(X_{j(i)})\)</span> to the overall bias. The bias-corrected matching is the following estimator:</p>
<p><span class="math display">\[\begin{align}
   \widehat{\delta}_{ATT}^{BC} = \dfrac{1}{N_T} \sum_{D_i=1} \bigg [ (Y_i - Y_{j(i)}) - \Big(\widehat{\mu}^0(X_i) - \widehat{\mu}^0(X_{j(i)})\Big) \bigg ]
\end{align}\]</span></p>
<p>where <span class="math inline">\(\widehat{\mu}^0(X)\)</span> is an estimate of <span class="math inline">\(E[Y\mid X=x,D=0]\)</span> using, for example, OLS. Again, I find it always helpful if we take a crack at these estimators with concrete data. Table <a href="ch4.html#tab:bias1">5.8</a> contains more make-believe data for eight units, four of whom are treated and the rest of whom are functioning as controls. According to the switching equation, we only observe the actual outcomes associated with the potential outcomes under treatment or control, which means we’re missing the control values for our treatment group.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:bias1">Table 5.8: </span> Another matching example (this time to illustrate bias correction)</caption>
<thead><tr class="header">
<th align="left">Unit</th>
<th align="left"><span class="math inline">\(Y^1\)</span></th>
<th align="left"><span class="math inline">\(Y^0\)</span></th>
<th align="left"><span class="math inline">\(D\)</span></th>
<th align="left"><span class="math inline">\(X\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">5</td>
<td align="left"></td>
<td align="left">1</td>
<td align="left">11</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="left">2</td>
<td align="left"></td>
<td align="left">1</td>
<td align="left">7</td>
</tr>
<tr class="odd">
<td align="left">3</td>
<td align="left">10</td>
<td align="left"></td>
<td align="left">1</td>
<td align="left">5</td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="left">6</td>
<td align="left"></td>
<td align="left">1</td>
<td align="left">3</td>
</tr>
<tr class="odd">
<td align="left">5</td>
<td align="left"></td>
<td align="left">4</td>
<td align="left">0</td>
<td align="left">10</td>
</tr>
<tr class="even">
<td align="left">6</td>
<td align="left"></td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">8</td>
</tr>
<tr class="odd">
<td align="left">7</td>
<td align="left"></td>
<td align="left">5</td>
<td align="left">0</td>
<td align="left">4</td>
</tr>
<tr class="even">
<td align="left">8</td>
<td align="left"></td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">1</td>
</tr>
</tbody>
</table></div>
<p>Notice in this example that we cannot implement exact matching because none of the treatment group units has an exact match in the control group. It’s worth emphasizing that this is a consequence of finite samples; the likelihood of finding an exact match grows when the sample size of the control group grows faster than that of the treatment group. Instead, we use nearest-neighbor matching, which is simply going to match each treatment unit to the control group unit whose covariate value is <em>nearest</em> to that of the treatment group unit itself. But, when we do this kind of matching, we necessarily create <em>matching discrepancies</em>, which is simply another way of saying that the covariates are not perfectly matched for every unit. Nonetheless, the nearest-neighbor “algorithm” creates Table <a href="ch4.html#tab:bias2">5.9</a>.</p>
<p>Recall that
<span class="math display">\[
\widehat{\delta}_{ATT} - \dfrac{5-4}{4} + \dfrac{2-0}{4} + \dfrac{10-5}{4}+\dfrac{6-1}{4}=3.25
\]</span>
With the bias correction, we need to estimate <span class="math inline">\(\widehat{\mu}^0(X)\)</span>. We’ll use OLS. It should be clearer what <span class="math inline">\(\widehat{\mu}^0(X)\)</span> is. It is is the fitted values from a regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span>. Let’s illustrate this using the data set shown in Table <a href="ch4.html#tab:bias2">5.9</a>.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:bias2">Table 5.9: </span> Nearest neighbor matched sample</caption>
<thead><tr class="header">
<th align="left">Unit</th>
<th align="left"><span class="math inline">\(Y^1\)</span></th>
<th align="left"><span class="math inline">\(Y^0\)</span></th>
<th align="left"><span class="math inline">\(D\)</span></th>
<th align="left"><span class="math inline">\(X\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">5</td>
<td align="left"><strong>4</strong></td>
<td align="left">1</td>
<td align="left">11</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="left">2</td>
<td align="left"><strong>0</strong></td>
<td align="left">1</td>
<td align="left">7</td>
</tr>
<tr class="odd">
<td align="left">3</td>
<td align="left">10</td>
<td align="left"><strong>5</strong></td>
<td align="left">1</td>
<td align="left">5</td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="left">6</td>
<td align="left"><strong>1</strong></td>
<td align="left">1</td>
<td align="left">3</td>
</tr>
<tr class="odd">
<td align="left">5</td>
<td align="left"></td>
<td align="left">4</td>
<td align="left">0</td>
<td align="left">10</td>
</tr>
<tr class="even">
<td align="left">6</td>
<td align="left"></td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">8</td>
</tr>
<tr class="odd">
<td align="left">7</td>
<td align="left"></td>
<td align="left">5</td>
<td align="left">0</td>
<td align="left">4</td>
</tr>
<tr class="even">
<td align="left">8</td>
<td align="left"></td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">1</td>
</tr>
</tbody>
</table></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/training_bias_reduction.do"><code>training_bias_reduction.do</code></a></em></p>
<div class="sourceCode" id="cb39"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb39-1"><a href="ch4.html#cb39-1" aria-hidden="true"></a><span class="kw">use</span> https:<span class="co">//github.com/scunning1975/mixtape/raw/master/training_bias_reduction.dta, clear</span></span>
<span id="cb39-2"><a href="ch4.html#cb39-2" aria-hidden="true"></a><span class="kw">reg</span> Y X</span>
<span id="cb39-3"><a href="ch4.html#cb39-3" aria-hidden="true"></a><span class="kw">gen</span> muhat = _b[<span class="dt">_cons</span>] + _b[X]*X</span>
<span id="cb39-4"><a href="ch4.html#cb39-4" aria-hidden="true"></a><span class="ot">list</span></span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/training_bias_reduction.R"><code>training_bias_reduction.R</code></a></em></p>
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://haven.tidyverse.org">haven</a></span><span class="op">)</span>

<span class="va">read_data</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">df</span><span class="op">)</span>
<span class="op">{</span>
  <span class="va">full_path</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"https://raw.github.com/scunning1975/mixtape/master/"</span>, 
                     <span class="va">df</span>, sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span>
  <span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/read_dta.html">read_dta</a></span><span class="op">(</span><span class="va">full_path</span><span class="op">)</span>
  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">df</span><span class="op">)</span>
<span class="op">}</span>

<span class="va">training_bias_reduction</span> <span class="op">&lt;-</span> <span class="fu">read_data</span><span class="op">(</span><span class="st">"training_bias_reduction.dta"</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>
    Y1 <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">Unit</span> <span class="op">%in%</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span>,<span class="fl">3</span>,<span class="fl">4</span><span class="op">)</span> <span class="op">~</span> <span class="va">Y</span><span class="op">)</span>,
    Y0 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">4</span>,<span class="fl">0</span>,<span class="fl">5</span>,<span class="fl">1</span>,<span class="fl">4</span>,<span class="fl">0</span>,<span class="fl">5</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span>

<span class="va">train_reg</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">X</span>, <span class="va">training_bias_reduction</span><span class="op">)</span>

<span class="va">training_bias_reduction</span> <span class="op">&lt;-</span> <span class="va">training_bias_reduction</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>u_hat0 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">train_reg</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p>When we regress <span class="math inline">\(Y\)</span> onto <span class="math inline">\(X\)</span> and <span class="math inline">\(D\)</span>, we get the following estimated coefficients:</p>
<p><span class="math display">\[\begin{align}
   \widehat{\mu}^0(X) &amp; =\widehat{\beta}_0 + \widehat{\beta}_1 X 
   \\
    &amp; = 4.42 - 0.049 X                         
\end{align}\]</span></p>
<p>This gives us the outcomes, treatment status, and predicted values in Table <a href="ch4.html#tab:bias3">5.10</a>.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:bias3">Table 5.10: </span> Nearest neighbor matched sample with fitted values for bias correction</caption>
<thead><tr class="header">
<th align="left">Unit</th>
<th align="left"><span class="math inline">\(Y^1\)</span></th>
<th align="left"><span class="math inline">\(Y^0\)</span></th>
<th align="left"><span class="math inline">\(Y\)</span></th>
<th align="left"><span class="math inline">\(D\)</span></th>
<th align="center"><span class="math inline">\(X\)</span></th>
<th align="center"><span class="math inline">\(\widehat{\mu}^0(X)\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">5</td>
<td align="left">4</td>
<td align="left">5</td>
<td align="left">1</td>
<td align="center">11</td>
<td align="center">3.89</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="left">2</td>
<td align="left">0</td>
<td align="left">2</td>
<td align="left">1</td>
<td align="center">7</td>
<td align="center">4.08</td>
</tr>
<tr class="odd">
<td align="left">3</td>
<td align="left">10</td>
<td align="left">5</td>
<td align="left">10</td>
<td align="left">1</td>
<td align="center">5</td>
<td align="center">4.18</td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="left">6</td>
<td align="left">1</td>
<td align="left">6</td>
<td align="left">1</td>
<td align="center">3</td>
<td align="center">4.28</td>
</tr>
<tr class="odd">
<td align="left">5</td>
<td align="left"></td>
<td align="left"><strong>4</strong></td>
<td align="left">4</td>
<td align="left">0</td>
<td align="center"><strong>10</strong></td>
<td align="center"><strong>3.94</strong></td>
</tr>
<tr class="even">
<td align="left">6</td>
<td align="left"></td>
<td align="left"><strong>0</strong></td>
<td align="left">0</td>
<td align="left">0</td>
<td align="center"><strong>8</strong></td>
<td align="center"><strong>4.03</strong></td>
</tr>
<tr class="odd">
<td align="left">7</td>
<td align="left"></td>
<td align="left"><strong>5</strong></td>
<td align="left">5</td>
<td align="left">0</td>
<td align="center"><strong>4</strong></td>
<td align="center"><strong>4.23</strong></td>
</tr>
<tr class="even">
<td align="left">8</td>
<td align="left"></td>
<td align="left"><strong>1</strong></td>
<td align="left">1</td>
<td align="left">0</td>
<td align="center"><strong>1</strong></td>
<td align="center"><strong>4.37</strong></td>
</tr>
</tbody>
</table></div>
<p>And then this would be done for the other three simple differences, each of which is added to a bias-correction term based on the fitted values from the covariate values.</p>
<p>Now, care must be given when using the fitted values for bias correction, so let me walk you through it. You are still going to be taking the simple differences (e.g., 5 – 4 for row 1), but now you will also subtract out the fitted values associated with each observation’s unique covariate. So for instance, in row 1, the outcome 5 has a covariate of 11, which gives it a fitted value of 3.89, but the counterfactual has a value of 10, which gives it a predicted value of 3.94. So therefore we would use the following bias correction:
<span class="math display">\[
\widehat{\delta}_{ATT}^{BC} = \dfrac{ 5-4 - (3.89 - 3.94)}{4} + \dots
\]</span>
Now that we see how a specific fitted value is calculated and how it contributes to the calculation of the ATT, let’s look at the entire calculation now.</p>
<p><span class="math display">\[\begin{align}
   \widehat{\delta}_{ATT}^{BC}
     &amp; = \dfrac{ (5-4) - \Big( \widehat{\mu^0}(11) - \widehat{\mu^0}(10)\Big) }{4} + \dfrac{ (2-0) -\Big( \widehat{\mu^0}(7) - \widehat{\mu^0}(8)\Big) }{4}     
   \\
     &amp; \quad+\ \dfrac{ (10-5) - \Big( \widehat{\mu^0}(5) - \widehat{\mu^0}(4)\Big)}{4} + \dfrac{ (6-1) - \Big( \widehat{\mu^0}(3) - \widehat{\mu^0}(1)\Big)}{4} 
   \\
     &amp; = 3.28                                                                                                                                                   
\end{align}\]</span></p>
<p>which is slightly higher than the unadjusted ATE of 3.25. Note that this bias-correction adjustment becomes more significant as the matching discrepancies themselves become more common. But, if the matching discrepancies are not very common in the first place, then by definition, bias adjustment doesn’t change the estimated parameter very much.</p>
<p>Bias arises because of the effect of large matching discrepancies. To minimize these discrepancies, we need a small number of <span class="math inline">\(M\)</span> (e.g., <span class="math inline">\(M=1\)</span>). Larger values of <span class="math inline">\(M\)</span> produce large matching discrepancies. Second, we need matching with replacement. Because matching with replacement can use untreated units as a match more than once, matching with replacement produces smaller discrepancies. And finally, try to match covariates with a large effect on <span class="math inline">\(\mu^0(.)\)</span>.</p>
<p>The matching estimators have a normal distribution in large samples provided that the bias is small. For matching without replacement, the usual variance estimator is valid. That is:</p>
<p><span class="math display">\[\begin{align}
   \widehat{\sigma}^2_{ATT} = \dfrac{1}{N_T} \sum_{D_i=1} \bigg ( Y_i - \dfrac{1}{M} \sum_{m=1}^M Y_{j_m(i)} - \widehat{\delta}_{ATT} \bigg )^2
\end{align}\]</span></p>
<p>For matching with replacement:</p>
<p><span class="math display">\[\begin{align}
   \widehat{\sigma}^2_{ATT}
     &amp; = \dfrac{1}{N_T} \sum_{D_i=1} \left( Y_i - \dfrac{1}{M} \sum_{m=1}^M Y_{j_m(i)} - \widehat{\delta}_{ATT} \right)^2 
   \\
     &amp; + \dfrac{1}{N_T} \sum_{D_i=0} \left( \dfrac{K_i(K_i-1)}{M^2} \right) \widehat{\mathop{\mathrm{var}}}(\varepsilon\mid X_i,D_i=0)     
\end{align}\]</span></p>
<p>where <span class="math inline">\(K_i\)</span> is the number of times that observation <span class="math inline">\(i\)</span> is used as a match. Then <span class="math inline">\(\widehat{\mathop{\mathrm{var}}}(Y_i\mid X_i, D_i=0)\)</span> can be estimated by matching. For example, take two observations with <span class="math inline">\(D_i=D_j=0\)</span> and <span class="math inline">\(X_i \approx X_j\)</span>:
<span class="math display">\[
\widehat{\mathop{\mathrm{var}}}(Y_i\mid X_i,D_i=0) = \dfrac{(Y_i-Y_j)^2}{2}
\]</span>
is an unbiased estimator of <span class="math inline">\(\widehat{\mathop{\mathrm{var}}}(\varepsilon_i\mid X_i, D_i = 0)\)</span>. The bootstrap, though, doesn’t create valid standard errors <span class="citation">(Abadie and Imbens <a href="references.html#ref-Abadie2008" role="doc-biblioref">2008</a>)</span>.</p>
<div class="cover-box">
<div class="row">
    <div class="col-xs-8 col-md-4 cover-img">
        <a href="https://www.amazon.com/dp/0300251688"><img src="../images/cover.jpg" alt="Buy Today!"></a>
    </div>
    
    <div class="col-xs-12 col-md-8 cover-text-box">
            <h2> 
                Causal Inference: 
                <br><span style="font-style: italic; font-weight:bold; font-size: 20px;">The Mixtape.</span>
            </h2> 
            
        <div class="cover-text">
            <p>Buy the print version today:</p>
            
            <div class="chips">
                <a href="https://www.amazon.com/dp/0300251688" class="app-chip"> 
                    <i class="fab fa-amazon" aria-hidden="true"></i> Buy from Amazon 
                </a>
    
                <a href="https://yalebooks.yale.edu/book/9780300251685/causal-inference" class="app-chip"> 
                    <i class="fas fa-book" aria-hidden="true"></i> Buy from Yale Press 
                </a>
            </div>
        </div>
    </div>
</div>
</div>
</div>
<div id="propensity-score-methods" class="section level3" number="5.3.3">
<h3>
<span class="header-section-number">5.3.3</span> Propensity score methods<a class="anchor" aria-label="anchor" href="#propensity-score-methods"><i class="fas fa-link"></i></a>
</h3>
<p>There are several ways of achieving the conditioning strategy implied by the backdoor criterion, and we’ve discussed several. But one popular one was developed by Donald Rubin in the mid-1970s to early 1980s called the propensity score method <span class="citation">(Rubin <a href="references.html#ref-Rubin1977" role="doc-biblioref">1977</a>; Rosenbaum and Rubin <a href="references.html#ref-Rosenbaum1983" role="doc-biblioref">1983</a>)</span>. The propensity score is similar in many respects to both nearest-neighbor covariate matching by <span class="citation">Abadie and Imbens (<a href="references.html#ref-Abadie2006" role="doc-biblioref">2006</a>)</span> and subclassification. It’s a very popular method, particularly in the medical sciences, of addressing selection on observables, and it has gained some use among economists as well <span class="citation">(Dehejia and Wahba <a href="references.html#ref-Dehejia2002" role="doc-biblioref">2002</a>)</span>.</p>
<p>Before we dig into it, though, a couple of words to help manage expectations. Despite some early excitement caused by <span class="citation">Dehejia and Wahba (<a href="references.html#ref-Dehejia2002" role="doc-biblioref">2002</a>)</span>, subsequent enthusiasm was more tempered <span class="citation">(Smith and Todd <a href="references.html#ref-Smith2001" role="doc-biblioref">2001</a>, <a href="references.html#ref-Smith2005" role="doc-biblioref">2005</a>; King and Nielsen <a href="references.html#ref-King2019" role="doc-biblioref">2019</a>)</span>. As such, propensity score matching has not seen as wide adoption among economists as in other nonexperimental methods like regression discontinuity or difference-in-differences. The most common reason given for this is that economists are oftentimes skeptical that CIA can be achieved in any data set—almost as an article of faith. This is because for many applications, economists as a group are usually more concerned about selection on unobservables than they are selection on observables, and as such, they reach for matching methods less often. But I am agnostic as to whether CIA holds or doesn’t hold in your particular application. There’s no theoretical reason to dismiss a procedure designed to estimate causal effects on some ad hoc principle one holds because of a hunch. Only prior knowledge and deep familiarity with the institutional details of your application can tell you what the appropriate identification strategy is, and insofar as the backdoor criterion can be met, then matching methods may be perfectly appropriate. And if it cannot, then matching is inappropriate. But then, so is a naı̈ve multivariate regression in such cases.</p>
<p>We’ve mentioned that propensity score matching is an application used when a conditioning strategy can satisfy the backdoor criterion. But how exactly is it implemented? Propensity score matching takes those necessary covariates, estimates a maximum likelihood model of the conditional probability of treatment (usually a logit or probit so as to ensure that the fitted values are bounded between 0 and 1), and uses the predicted values from that estimation to collapse those covariates into a single scalar called the <em>propensity score</em>. All comparisons between the treatment and control group are then based on that value.</p>
<p>There is some subtlety to the propensity score in practice, though. Consider this scenario: two units, A and B, are assigned to treatment and control, respectively. But their propensity score is 0.6. Thus, they had the same 60% conditional probability of being assigned to treatment, but by random chance, A was assigned to treatment and B was assigned to control. The idea with propensity score methods is to compare units who, based on observables, had very similar probabilities of being placed into the treatment group even though those units differed with regard to actual treatment assignment. If conditional on <span class="math inline">\(X\)</span>, two units have the same probability of being treated, then we say they have similar <em>propensity scores</em>, and all remaining variation in treatment assignment is due to chance. And insofar as the two units A and B have the same propensity score of 0.6, but one is the treatment group and one is not, and the <em>conditional independence assumption</em> credibly holds in the data, then differences between their observed outcomes are attributable to the treatment.</p>
<p>Implicit in that example, though, we see another assumption needed for this procedure, and that’s the <em>common support</em> assumption. Common support simply requires that there be units in the treatment and control group across the estimated propensity score. We had common support for 0.6 because there was a unit in the treatment group (A) and one in the control group (B) for 0.6. In ways that are connected to this, the propensity score can be used to check for covariate balance between the treatment group and control group such that the two groups become observationally equivalent. But before walking through an example using real data, let’s review some papers that use it.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;I cannot emphasize this enough—this method, like regression more generally, only has value for your project if you can satisfy the backdoor criterion by conditioning on &lt;span class="math inline"&gt;\(X\)&lt;/span&gt;. If you cannot satisfy the backdoor criterion in your data, then the propensity score does not assist you in identifying a causal effect. At best, it helps you better understand issues related to balance on observables (but not unobservables). It is absolutely critical that your DAG be, in other words, credible, defensible, and accurate, as you depend on those theoretical relationships to design the appropriate identification strategy.&lt;/p&gt;'><sup>82</sup></a></p>
</div>
<div id="example-the-nsw-job-training-program" class="section level3" number="5.3.4">
<h3>
<span class="header-section-number">5.3.4</span> Example: The NSW job training program<a class="anchor" aria-label="anchor" href="#example-the-nsw-job-training-program"><i class="fas fa-link"></i></a>
</h3>
<p>The National Supported Work Demonstration (NSW) job-training program was operated by the Manpower Demonstration Research Corp (MRDC) in the mid-1970s. The NSW was a temporary employment program designed to help disadvantaged workers lacking basic job skills move into the labor market by giving them work experience and counseling in a sheltered environment. It was also unique in that it randomly assigned qualified applicants to training positions. The treatment group received all the benefits of the NSW program. The controls were basically left to fend for themselves. The program admitted women receiving Aid to Families with Dependent Children, recovering addicts, released offenders, and men and women of both sexes who had not completed high school.</p>
<p>Treatment group members were guaranteed a job for nine to eighteen months depending on the target group and site. They were then divided into crews of three to five participants who worked together and met frequently with an NSW counselor to discuss grievances with the program and performance. Finally, they were paid for their work. NSW offered the trainees lower wages than they would’ve received on a regular job, but allowed for earnings to increase for satisfactory performance and attendance. After participants’ terms expired, they were forced to find regular employment. The kinds of jobs varied within sites—some were gas-station attendants, some worked at a printer shop—and men and women frequently performed different kinds of work.</p>
<p>The MDRC collected earnings and demographic information from both the treatment and the control group at baseline as well as every nine months thereafter. MDRC also conducted up to four post-baseline interviews. There were different sample sizes from study to study, which can be confusing.</p>
<p>NSW was a randomized job-training program; therefore, the independence assumption was satisfied. So calculating average treatment effects was straightforward—it’s the simple difference in means estimator that we discussed in the potential outcomes chapter.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Remember, randomization means that the treatment was independent of the potential outcomes, so simple difference in means identifies the average treatment effect.&lt;/p&gt;"><sup>83</sup></a>
<span class="math display">\[
\dfrac{1}{N_T} \sum_{D_i=1} Y_i - \dfrac{1}{N_C} \sum_{D_i=0} Y_i \approx E[Y^1 - Y^0] \approx ATE
\]</span>
The good news for MDRC, and the treatment group, was that the treatment benefited the workers.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;&lt;span class="citation"&gt;Lalonde (&lt;a href="references.html#ref-Lalonde1986" role="doc-biblioref"&gt;1986&lt;/a&gt;)&lt;/span&gt; lists several studies that discuss the findings from the program.&lt;/p&gt;'><sup>84</sup></a> Treatment group participants’ real earnings post-treatment in 1978 were more than earnings of the control group by approximately $900 <span class="citation">(Lalonde <a href="references.html#ref-Lalonde1986" role="doc-biblioref">1986</a>)</span> to $1,800 <span class="citation">(Dehejia and Wahba <a href="references.html#ref-Dehejia2002" role="doc-biblioref">2002</a>)</span>, depending on the sample the researcher used.</p>
<p><span class="citation">Lalonde (<a href="references.html#ref-Lalonde1986" role="doc-biblioref">1986</a>)</span> is an interesting study both because he is evaluating the NSW program and because he is evaluating commonly used econometric methods from that time. He evaluated the econometric estimators’ performance by trading out the experimental control group data with data on the non-experimental control group drawn from the population of US citizens. He used three samples of the Current Population Survey (CPS) and three samples of the Panel Survey of Income Dynamics (PSID) for this non-experimental control group data, but I will use just one for each. Non-experimental data is, after all, the typical situation an economist finds herself in. But the difference with the NSW is that it was a randomized experiment, and therefore we know the average treatment effect. Since we know the average treatment effect, we can see how well a variety of econometric models perform. If the NSW program increased earnings by approximately <span class="math inline">\(\$900\)</span>, then we should find that if the other econometrics estimators does a good job, right?</p>
<p><span class="citation">Lalonde (<a href="references.html#ref-Lalonde1986" role="doc-biblioref">1986</a>)</span> reviewed a number of popular econometric methods used by his contemporaries with both the PSID and the CPS samples as nonexperimental comparison groups, and his results were consistently <em>horrible</em>. Not only were his estimates usually very different in magnitude, but his results were almost always the wrong sign! This paper, and its pessimistic conclusion, was influential in policy circles and led to a greater push for more experimental evaluations.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;It’s since been cited a little more than 1,700 times.&lt;/p&gt;"><sup>85</sup></a> We can see these results in the following tables from <span class="citation">Lalonde (<a href="references.html#ref-Lalonde1986" role="doc-biblioref">1986</a>)</span>. Table <a href="ch4.html#tab:lalonde1">5.11</a> shows the effect of the treatment when comparing the treatment group to the experimental control group. The baseline difference in real earnings between the two groups was negligible. The treatment group made $39 more than the control group in the pre-treatment period without controls and $21 less in the multivariate regression model, but neither is statistically significant. But the post-treatment difference in average earnings was between $798 and $886.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Lalonde reports a couple different diff-in-diff models, but for simplicity, I will only report one.&lt;/p&gt;"><sup>86</sup></a></p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:lalonde1">Table 5.11: </span> Earnings Comparisons and Estimated Training Effects for the NSW Male Participants Using Comparison Groups from the PSID and the CPS-SSA</caption>
<tbody>
<tr class="odd">
<td align="left"><strong>NSW Treatment - Control Earnings</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td align="left"><strong>Name of comparison group</strong></td>
<td><strong>Pre-Treatment Unadjusted</strong></td>
<td><strong>Pre-Treatment Adjusted</strong></td>
<td><strong>Post-Treatment Unadjusted</strong></td>
<td><strong>Post-Treatment Adjusted</strong></td>
<td><strong>Difference-in-Differences</strong></td>
</tr>
<tr class="odd">
<td align="left">Experimental controls</td>
<td>$ 39</td>
<td>$ -21</td>
<td>$ 886</td>
<td>$ 798</td>
<td>$ 856</td>
</tr>
<tr class="even">
<td align="left"></td>
<td>(383)</td>
<td>(378)</td>
<td>(476)</td>
<td>(472)</td>
<td>(558)</td>
</tr>
<tr class="odd">
<td align="left">PSID-1</td>
<td><span class="math inline">\(-\$15,997\)</span></td>
<td><span class="math inline">\(-\$ 7,624\)</span></td>
<td><span class="math inline">\(-\$ 15,578\)</span></td>
<td><span class="math inline">\(-\$ 8,067\)</span></td>
<td><span class="math inline">\(-\$749\)</span></td>
</tr>
<tr class="even">
<td align="left"></td>
<td>(795)</td>
<td>(851)</td>
<td>(913)</td>
<td>(990)</td>
<td>(692)</td>
</tr>
<tr class="odd">
<td align="left">CPS-SSA-1</td>
<td><span class="math inline">\(-\$ 10,585\)</span></td>
<td><span class="math inline">\(-\$4,654\)</span></td>
<td><span class="math inline">\(-\$8,870\)</span></td>
<td><span class="math inline">\(- \$4,416\)</span></td>
<td>$195</td>
</tr>
<tr class="even">
<td align="left"></td>
<td>(539)</td>
<td>(509)</td>
<td>(562)</td>
<td>(557)</td>
<td>(441)</td>
</tr>
</tbody>
</table></div>
<p>Table <a href="ch4.html#tab:lalonde1">5.11</a> also shows the results he got when he used the non-experimental data as the comparison group. Here I report his results when using one sample from the PSID and one from the CPS, although in his original paper he used three of each. In nearly every point estimate, the effect is negative. The one exception is the difference-in-differences model which is positive, small, and insignificant.</p>
<p>So why is there such a stark difference when we move from the NSW control group to either the PSID or CPS? The reason is because of selection bias:
<span class="math display">\[
E\big[Y^0\mid D=1\big] \neq E\big[Y^0\mid D=0\big]
\]</span>
In other words, it’s highly likely that the real earnings of NSW participants would have been much lower than the non-experimental control group’s earnings. As you recall from our decomposition of the simple difference in means estimator, the second form of bias is selection bias, and if <span class="math inline">\(E[Y^0\mid D=1] &lt; E[Y^0\mid D=0]\)</span>, this will bias the estimate of the ATE downward (e.g., estimates that show a negative effect).</p>
<p>But as I will show shortly, a violation of independence also implies that covariates will be unbalanced across the propensity score—something we call the <em>balancing property</em>. Table <a href="ch4.html#tab:covariate1">5.12</a> illustrates this showing the mean values for each covariate for the treatment and control groups, where the control is the 15,992 observations from the CPS. As you can see, the treatment group appears to be very different on average from the control group CPS sample along nearly every covariate listed. The NSW participants are more black, more Hispanic, younger, less likely to be married, more likely to have no degree and less schooling, more likely to be unemployed in 1975, and more likely to have considerably lower earnings in 1975. In short, the two groups are not <em>exchangeable</em> on observables (and likely not exchangeable on unobservables either).</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:covariate1">Table 5.12: </span> Completed matching example with single covariate</caption>
<tbody>
<tr class="odd">
<td></td>
<td align="center"><strong>All</strong></td>
<td></td>
<td align="center"><strong>CPS Controls</strong></td>
<td align="center"><strong>NSW Trainees</strong></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td align="center"></td>
<td></td>
<td align="center"><span class="math inline">\(N_c = 15,992\)</span></td>
<td align="center"><span class="math inline">\(N_t = 297\)</span></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td><strong>Covariate</strong></td>
<td align="center"><strong>Mean</strong></td>
<td><strong>SD</strong></td>
<td align="center"><strong>Mean</strong></td>
<td align="center"><strong>Mean</strong></td>
<td><strong>T-statistic</strong></td>
<td><strong>Diff.</strong></td>
</tr>
<tr class="even">
<td>Black</td>
<td align="center">0.09</td>
<td>0.28</td>
<td align="center">0.07</td>
<td align="center">0.80</td>
<td>47.04</td>
<td><span class="math inline">\(-0.73\)</span></td>
</tr>
<tr class="odd">
<td>Hispanic</td>
<td align="center">0.07</td>
<td>0.26</td>
<td align="center">0.07</td>
<td align="center">0.94</td>
<td>1.47</td>
<td><span class="math inline">\(-0.02\)</span></td>
</tr>
<tr class="even">
<td>Age</td>
<td align="center">33.07</td>
<td>11.04</td>
<td align="center">33.2</td>
<td align="center">24.63</td>
<td>13.37</td>
<td>8.6</td>
</tr>
<tr class="odd">
<td>Married</td>
<td align="center">0.70</td>
<td>0.46</td>
<td align="center">0.71</td>
<td align="center">0.17</td>
<td>20.54</td>
<td>0.54</td>
</tr>
<tr class="even">
<td>No degree</td>
<td align="center">0.30</td>
<td>0.46</td>
<td align="center">0.30</td>
<td align="center">0.73</td>
<td>16.27</td>
<td><span class="math inline">\(-0.43\)</span></td>
</tr>
<tr class="odd">
<td>Education</td>
<td align="center">12.0</td>
<td>2.86</td>
<td align="center">12.03</td>
<td align="center">10.38</td>
<td>9.85</td>
<td>1.65</td>
</tr>
<tr class="even">
<td>1975 Earnings</td>
<td align="center">13.51</td>
<td>9.31</td>
<td align="center">13.65</td>
<td align="center">3.1</td>
<td>19.63</td>
<td>10.6</td>
</tr>
<tr class="odd">
<td>1975 Unemp</td>
<td align="center">0.11</td>
<td>0.32</td>
<td align="center">0.11</td>
<td align="center">0.37</td>
<td>14.29</td>
<td><span class="math inline">\(-0.26\)</span></td>
</tr>
</tbody>
</table></div>
<p>The first paper to reevaluate <span class="citation">Lalonde (<a href="references.html#ref-Lalonde1986" role="doc-biblioref">1986</a>)</span> using propensity score methods was <span class="citation">Dehejia and Wahba (<a href="references.html#ref-Dehejia1999" role="doc-biblioref">1999</a>)</span>. Their interest was twofold. First, they wanted to examine whether propensity score matching could be an improvement in estimating treatment effects using non-experimental data. And second, they wanted to show the diagnostic value of propensity score matching. The authors used the same non-experimental control group data sets from the CPS and PSID as <span class="citation">Lalonde (<a href="references.html#ref-Lalonde1986" role="doc-biblioref">1986</a>)</span> did.</p>
<p>Let’s walk through this, and what they learned from each of these steps. First, the authors estimated the propensity score using maximum likelihood modeling. Once they had the estimated propensity score, they compared treatment units to control units within intervals of the propensity score itself. This process of checking whether there are units in both treatment and control for intervals of the propensity score is called checking for common support.</p>
<p>One easy way to check for common support is to plot the number of treatment and control group observations separately across the propensity score with a histogram. <span class="citation">Dehejia and Wahba (<a href="references.html#ref-Dehejia1999" role="doc-biblioref">1999</a>)</span> did this using both the PSID and CPS samples and found that the overlap was nearly nonexistent, but here I’ll focus on their CPS sample. The overlap was so bad that they opted to drop 12,611 observations in the control group because their propensity scores were outside the treatment group range. Also, a large number of observations have low propensity scores, evidenced by the fact that the first bin contains 2,969 comparison units. Once this “trimming” was done, the overlap improved, though still wasn’t great.</p>
<p>We learn some things from this kind of diagnostic, though. We learn, for one, that the selection bias on observables is probably extreme if for no other reason than the fact that there are so few units in both treatment and control for given values of the propensity score. When there is considerable bunching at either end of the propensity score distribution, it suggests you have units who differ remarkably on observables with respect to the treatment variable itself. Trimming around those extreme values has been a way of addressing this when employing traditional propensity score adjustment techniques.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:dw1999">Table 5.13: </span> Estimated Training Effects using Propensity Scores</caption>
<tbody>
<tr class="odd">
<td></td>
<td align="center"><strong>NSW T-C Earnings</strong></td>
<td></td>
<td align="center"><strong>Propensity Score Adjusted</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td align="center"></td>
<td></td>
<td align="center"></td>
<td><strong>Stratification</strong></td>
<td></td>
<td><strong>Matching</strong></td>
<td></td>
</tr>
<tr class="odd">
<td><strong>Comparison group</strong></td>
<td align="center"><strong>Unadj.</strong></td>
<td><strong>Adj.</strong></td>
<td align="center"><strong>Quadratic Score</strong></td>
<td><strong>Unadj.</strong></td>
<td><strong>Adj.</strong></td>
<td><strong>Unadj.</strong></td>
<td><strong>Adj.</strong></td>
</tr>
<tr class="even">
<td>Experimental controls</td>
<td align="center">1,794</td>
<td>1,672</td>
<td align="center"></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td align="center">(633)</td>
<td>(638)</td>
<td align="center"></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>PSID-1</td>
<td align="center"><span class="math inline">\(-15,205\)</span></td>
<td>731</td>
<td align="center">294</td>
<td>1,608</td>
<td>1,494</td>
<td>1,691</td>
<td>1,473</td>
</tr>
<tr class="odd">
<td></td>
<td align="center">(1154)</td>
<td>(886)</td>
<td align="center">(1389)</td>
<td>(1571)</td>
<td>(1581)</td>
<td>(2209)</td>
<td>(809)</td>
</tr>
<tr class="even">
<td>CPS-1</td>
<td align="center"><span class="math inline">\(-8498\)</span></td>
<td>972</td>
<td align="center">1,117</td>
<td>1,713</td>
<td>1,774</td>
<td>1,582</td>
<td>1,616</td>
</tr>
<tr class="odd">
<td></td>
<td align="center">(712)</td>
<td>(550)</td>
<td align="center">(747)</td>
<td>(1115)</td>
<td>(1152)</td>
<td>(1069)</td>
<td>(751)</td>
</tr>
</tbody>
</table></div>
<p>With estimated propensity score in hand, <span class="citation">Dehejia and Wahba (<a href="references.html#ref-Dehejia1999" role="doc-biblioref">1999</a>)</span> estimated the treatment effect on real earnings 1978 using the experimental treatment group compared with the non-experimental control group. The treatment effect here differs from what we found in Lalonde because <span class="citation">Dehejia and Wahba (<a href="references.html#ref-Dehejia1999" role="doc-biblioref">1999</a>)</span> used a slightly different sample. Still, using their sample, they find that the NSW program caused earnings to increase between $1,672 and $1,794 depending on whether exogenous covariates were included in a regression. Both of these estimates are highly significant.</p>
<p>The first two columns labeled “unadjusted” and “adjusted” represent OLS regressions with and without controls. Without controls, both PSID and CPS estimates are extremely negative and precise. This, again, is because the selection bias is so severe with respect to the NSW program. When controls are included, effects become positive and imprecise for the PSID sample though almost significant at 5% for CPS. But each effect size is only about half the size of the true effect.</p>
<p>Table <a href="ch4.html#tab:dw1999">5.13</a> shows the results using propensity score weighting or matching.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Let’s hold off digging into exactly how they used the propensity score to generate these estimates.&lt;/p&gt;"><sup>87</sup></a> As can be seen, the results are a considerable improvement over <span class="citation">Lalonde (<a href="references.html#ref-Lalonde1986" role="doc-biblioref">1986</a>)</span>. I won’t review every treatment effect the authors calculated, but I will note that they are all positive and similar in magnitude to what they found in columns 1 and 2 using only the experimental data.</p>
<p>Finally, the authors examined the balance between the covariates in the treatment group (NSW) and the various non-experimental (matched) samples in Table <a href="ch4.html#tab:dw-balance">5.14</a>. In the next section, I explain why we expect covariate values to balance along the propensity score for the treatment and control group after trimming the outlier propensity score units from the data. Table <a href="ch4.html#tab:dw-balance">5.14</a> shows the sample means of characteristics in the matched control sample versus the experimental NSW sample (first row). Trimming on the propensity score, in effect, helped balance the sample. Covariates are much closer in mean value to the NSW sample after trimming on the propensity score.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:dw-balance">Table 5.14: </span> Sample Means of Characteristics for Matched Control Samples</caption>
<tbody>
<tr class="odd">
<td align="left">Matched Sample</td>
<td align="center"><span class="math inline">\(N\)</span></td>
<td align="center">Age</td>
<td align="center">Education</td>
<td align="center">Black</td>
<td align="center">Hispanic</td>
<td align="center">No Degree</td>
<td align="center">Married</td>
<td align="center">RE74</td>
<td align="center">RE75</td>
</tr>
<tr class="even">
<td align="left">NSW</td>
<td align="center">185</td>
<td align="center">25.81</td>
<td align="center">10.335</td>
<td align="center">0.84</td>
<td align="center">0.06</td>
<td align="center">0.71</td>
<td align="center">0.19</td>
<td align="center">2,096</td>
<td align="center">1,532</td>
</tr>
<tr class="odd">
<td align="left">PSID</td>
<td align="center">56</td>
<td align="center">26.39</td>
<td align="center">10.62</td>
<td align="center">0.86</td>
<td align="center">0.02</td>
<td align="center">0.55</td>
<td align="center">0.15</td>
<td align="center">1,794</td>
<td align="center">1,126</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center"></td>
<td align="center">(2.56)</td>
<td align="center">(0.63)</td>
<td align="center">(0.13)</td>
<td align="center">(0.06)</td>
<td align="center">(0.13)</td>
<td align="center">(0.13)</td>
<td align="center">(0.12)</td>
<td align="center">(1,406)</td>
</tr>
<tr class="odd">
<td align="left">CPS</td>
<td align="center">119</td>
<td align="center">26.91</td>
<td align="center">10.52</td>
<td align="center">0.86</td>
<td align="center">0.04</td>
<td align="center">0.64</td>
<td align="center">0.19</td>
<td align="center">2,110</td>
<td align="center">1,396</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center"></td>
<td align="center">(1.25)</td>
<td align="center">(0.32)</td>
<td align="center">(0.06)</td>
<td align="center">(0.04)</td>
<td align="center">(0.07)</td>
<td align="center">(0.06)</td>
<td align="center">(841)</td>
<td align="center">(563)</td>
</tr>
</tbody>
</table></div>
<p class="footnote">
Standard error on the difference in means with NSW sample is given in parentheses.
</p>
<p>Propensity score is best explained using actual data. We will use data from <span class="citation">Dehejia and Wahba (<a href="references.html#ref-Dehejia2002" role="doc-biblioref">2002</a>)</span> for the following exercises. But before using the propensity score methods for estimating treatment effects, let’s calculate the average treatment effect from the actual experiment. Using the following code, we calculate that the NSW job-training program caused real earnings in 1978 to increase by $1,794.343.</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/nsw_experimental.do"><code>nsw_experimental.do</code></a></em></p>
<div class="sourceCode" id="cb41"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb41-1"><a href="ch4.html#cb41-1" aria-hidden="true"></a><span class="kw">use</span> https:<span class="co">//github.com/scunning1975/mixtape/raw/master/nsw_mixtape.dta, clear</span></span>
<span id="cb41-2"><a href="ch4.html#cb41-2" aria-hidden="true"></a>su re78 <span class="kw">if</span> treat</span>
<span id="cb41-3"><a href="ch4.html#cb41-3" aria-hidden="true"></a><span class="kw">gen</span> y1 = <span class="fu">r</span>(<span class="kw">mean</span>)</span>
<span id="cb41-4"><a href="ch4.html#cb41-4" aria-hidden="true"></a>su re78 <span class="kw">if</span> treat==0</span>
<span id="cb41-5"><a href="ch4.html#cb41-5" aria-hidden="true"></a><span class="kw">gen</span> y0 = <span class="fu">r</span>(<span class="kw">mean</span>)</span>
<span id="cb41-6"><a href="ch4.html#cb41-6" aria-hidden="true"></a><span class="kw">gen</span> ate = y1-y0</span>
<span id="cb41-7"><a href="ch4.html#cb41-7" aria-hidden="true"></a>su ate</span>
<span id="cb41-8"><a href="ch4.html#cb41-8" aria-hidden="true"></a><span class="kw">di</span> 6349.144 - 4554.801</span>
<span id="cb41-9"><a href="ch4.html#cb41-9" aria-hidden="true"></a>* ATE is 1794.34 </span>
<span id="cb41-10"><a href="ch4.html#cb41-10" aria-hidden="true"></a><span class="kw">drop</span> <span class="kw">if</span> treat==0</span>
<span id="cb41-11"><a href="ch4.html#cb41-11" aria-hidden="true"></a><span class="kw">drop</span> y1 y0 ate</span>
<span id="cb41-12"><a href="ch4.html#cb41-12" aria-hidden="true"></a><span class="kw">compress</span></span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/nsw_experimental.R"><code>nsw_experimental.R</code></a></em></p>
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://haven.tidyverse.org">haven</a></span><span class="op">)</span>

<span class="va">read_data</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">df</span><span class="op">)</span>
<span class="op">{</span>
  <span class="va">full_path</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"https://raw.github.com/scunning1975/mixtape/master/"</span>, 
                     <span class="va">df</span>, sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span>
  <span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/read_dta.html">read_dta</a></span><span class="op">(</span><span class="va">full_path</span><span class="op">)</span>
  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">df</span><span class="op">)</span>
<span class="op">}</span>

<span class="va">nsw_dw</span> <span class="op">&lt;-</span> <span class="fu">read_data</span><span class="op">(</span><span class="st">"nsw_mixtape.dta"</span><span class="op">)</span>

<span class="va">nsw_dw</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">treat</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">re78</span><span class="op">)</span>

<span class="va">mean1</span> <span class="op">&lt;-</span> <span class="va">nsw_dw</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">treat</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html">pull</a></span><span class="op">(</span><span class="va">re78</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">)</span>

<span class="va">nsw_dw</span><span class="op">$</span><span class="va">y1</span> <span class="op">&lt;-</span> <span class="va">mean1</span>

<span class="va">nsw_dw</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">treat</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">re78</span><span class="op">)</span>

<span class="va">mean0</span> <span class="op">&lt;-</span> <span class="va">nsw_dw</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">treat</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html">pull</a></span><span class="op">(</span><span class="va">re78</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">)</span>

<span class="va">nsw_dw</span><span class="op">$</span><span class="va">y0</span> <span class="op">&lt;-</span> <span class="va">mean0</span>

<span class="va">ate</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/unique.html">unique</a></span><span class="op">(</span><span class="va">nsw_dw</span><span class="op">$</span><span class="va">y1</span> <span class="op">-</span> <span class="va">nsw_dw</span><span class="op">$</span><span class="va">y0</span><span class="op">)</span>

<span class="va">nsw_dw</span> <span class="op">&lt;-</span> <span class="va">nsw_dw</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">treat</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="op">-</span><span class="va">y1</span>, <span class="op">-</span><span class="va">y0</span><span class="op">)</span></code></pre></div>
<p>Next we want to go through several examples in which we estimate the average treatment effect or some if its variants such as the average treatment effect on the treatment group or the average treatment effect on the untreated group. But here, rather than using the experimental control group from the original randomized experiment, we will use the non-experimental control group from the Current Population Survey. It is very important to stress that while the treatment group is an experimental group, the control group now consists of a random sample of Americans from that time period. Thus, the control group suffers from extreme selection bias since most Americans would not function as counterfactuals for the distressed group of workers who selected into the NSW program. In the following, we will append the CPS data to the experimental data and estimate the propensity score using logit so as to be consistent with <span class="citation">Dehejia and Wahba (<a href="references.html#ref-Dehejia2002" role="doc-biblioref">2002</a>)</span>.</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/nsw_pscore.do"><code>nsw_pscore.do</code></a></em></p>
<div class="sourceCode" id="cb43"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb43-1"><a href="ch4.html#cb43-1" aria-hidden="true"></a>* Reload experimental <span class="fu">group</span> <span class="kw">data</span></span>
<span id="cb43-2"><a href="ch4.html#cb43-2" aria-hidden="true"></a><span class="kw">use</span> https:<span class="co">//github.com/scunning1975/mixtape/raw/master/nsw_mixtape.dta, clear</span></span>
<span id="cb43-3"><a href="ch4.html#cb43-3" aria-hidden="true"></a><span class="kw">drop</span> <span class="kw">if</span> treat==0</span>
<span id="cb43-4"><a href="ch4.html#cb43-4" aria-hidden="true"></a></span>
<span id="cb43-5"><a href="ch4.html#cb43-5" aria-hidden="true"></a>* Now <span class="kw">merge</span> <span class="kw">in</span> the CPS controls from footnote 2 <span class="kw">of</span> Table 2 (Dehejia and Wahba 2002)</span>
<span id="cb43-6"><a href="ch4.html#cb43-6" aria-hidden="true"></a><span class="kw">append</span> <span class="kw">using</span> https:<span class="co">//github.com/scunning1975/mixtape/raw/master/cps_mixtape.dta</span></span>
<span id="cb43-7"><a href="ch4.html#cb43-7" aria-hidden="true"></a><span class="kw">gen</span> agesq=age*age</span>
<span id="cb43-8"><a href="ch4.html#cb43-8" aria-hidden="true"></a><span class="kw">gen</span> agecube=age*age*age</span>
<span id="cb43-9"><a href="ch4.html#cb43-9" aria-hidden="true"></a><span class="kw">gen</span> edusq=educ*edu</span>
<span id="cb43-10"><a href="ch4.html#cb43-10" aria-hidden="true"></a><span class="kw">gen</span> u74 = 0 <span class="kw">if</span> re74!=.</span>
<span id="cb43-11"><a href="ch4.html#cb43-11" aria-hidden="true"></a><span class="kw">replace</span> u74 = 1 <span class="kw">if</span> re74==0</span>
<span id="cb43-12"><a href="ch4.html#cb43-12" aria-hidden="true"></a><span class="kw">gen</span> u75 = 0 <span class="kw">if</span> re75!=.</span>
<span id="cb43-13"><a href="ch4.html#cb43-13" aria-hidden="true"></a><span class="kw">replace</span> u75 = 1 <span class="kw">if</span> re75==0</span>
<span id="cb43-14"><a href="ch4.html#cb43-14" aria-hidden="true"></a><span class="kw">gen</span> interaction1 = educ*re74</span>
<span id="cb43-15"><a href="ch4.html#cb43-15" aria-hidden="true"></a><span class="kw">gen</span> re74sq=re74^2</span>
<span id="cb43-16"><a href="ch4.html#cb43-16" aria-hidden="true"></a><span class="kw">gen</span> re75sq=re75^2</span>
<span id="cb43-17"><a href="ch4.html#cb43-17" aria-hidden="true"></a><span class="kw">gen</span> interaction2 = u74*hisp</span>
<span id="cb43-18"><a href="ch4.html#cb43-18" aria-hidden="true"></a></span>
<span id="cb43-19"><a href="ch4.html#cb43-19" aria-hidden="true"></a>* Now estimate the propensity <span class="kw">score</span></span>
<span id="cb43-20"><a href="ch4.html#cb43-20" aria-hidden="true"></a><span class="kw">logit</span> treat age agesq agecube educ edusq marr nodegree <span class="bn">black</span> hisp re74 re75 u74 u75 interaction1 </span>
<span id="cb43-21"><a href="ch4.html#cb43-21" aria-hidden="true"></a><span class="kw">predict</span> pscore</span>
<span id="cb43-22"><a href="ch4.html#cb43-22" aria-hidden="true"></a></span>
<span id="cb43-23"><a href="ch4.html#cb43-23" aria-hidden="true"></a>* Checking <span class="kw">mean</span> propensity scores <span class="kw">for</span> treatment and control groups</span>
<span id="cb43-24"><a href="ch4.html#cb43-24" aria-hidden="true"></a>su pscore <span class="kw">if</span> treat==1, <span class="kw">detail</span></span>
<span id="cb43-25"><a href="ch4.html#cb43-25" aria-hidden="true"></a>su pscore <span class="kw">if</span> treat==0, <span class="kw">detail</span></span>
<span id="cb43-26"><a href="ch4.html#cb43-26" aria-hidden="true"></a></span>
<span id="cb43-27"><a href="ch4.html#cb43-27" aria-hidden="true"></a>* Now look <span class="fu">at</span> the propensity <span class="kw">score</span> distribution <span class="kw">for</span> treatment and control groups</span>
<span id="cb43-28"><a href="ch4.html#cb43-28" aria-hidden="true"></a><span class="kw">histogram</span> pscore, <span class="kw">by</span>(treat) binrescale</span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/nsw_pscore.R"><code>nsw_pscore.R</code></a></em></p>
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://haven.tidyverse.org">haven</a></span><span class="op">)</span>

<span class="va">read_data</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">df</span><span class="op">)</span>
<span class="op">{</span>
  <span class="va">full_path</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"https://raw.github.com/scunning1975/mixtape/master/"</span>, 
                     <span class="va">df</span>, sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span>
  <span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/read_dta.html">read_dta</a></span><span class="op">(</span><span class="va">full_path</span><span class="op">)</span>
  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">df</span><span class="op">)</span>
<span class="op">}</span>

<span class="va">nsw_dw_cpscontrol</span> <span class="op">&lt;-</span> <span class="fu">read_data</span><span class="op">(</span><span class="st">"cps_mixtape.dta"</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/bind.html">bind_rows</a></span><span class="op">(</span><span class="va">nsw_dw</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>agesq <span class="op">=</span> <span class="va">age</span><span class="op">^</span><span class="fl">2</span>,
         agecube <span class="op">=</span> <span class="va">age</span><span class="op">^</span><span class="fl">3</span>,
         educsq <span class="op">=</span> <span class="va">educ</span><span class="op">*</span><span class="va">educ</span>,
         u74 <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">re74</span> <span class="op">==</span> <span class="fl">0</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span>,
         u75 <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">re75</span> <span class="op">==</span> <span class="fl">0</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span>,
         interaction1 <span class="op">=</span> <span class="va">educ</span><span class="op">*</span><span class="va">re74</span>,
         re74sq <span class="op">=</span> <span class="va">re74</span><span class="op">^</span><span class="fl">2</span>,
         re75sq <span class="op">=</span> <span class="va">re75</span><span class="op">^</span><span class="fl">2</span>,
         interaction2 <span class="op">=</span> <span class="va">u74</span><span class="op">*</span><span class="va">hisp</span><span class="op">)</span>

<span class="co"># estimating</span>
<span class="va">logit_nsw</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">treat</span> <span class="op">~</span> <span class="va">age</span> <span class="op">+</span> <span class="va">agesq</span> <span class="op">+</span> <span class="va">agecube</span> <span class="op">+</span> <span class="va">educ</span> <span class="op">+</span> <span class="va">educsq</span> <span class="op">+</span> 
                   <span class="va">marr</span> <span class="op">+</span> <span class="va">nodegree</span> <span class="op">+</span> <span class="va">black</span> <span class="op">+</span> <span class="va">hisp</span> <span class="op">+</span> <span class="va">re74</span> <span class="op">+</span> <span class="va">re75</span> <span class="op">+</span> <span class="va">u74</span> <span class="op">+</span>
                   <span class="va">u75</span> <span class="op">+</span> <span class="va">interaction1</span>, family <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/family.html">binomial</a></span><span class="op">(</span>link <span class="op">=</span> <span class="st">"logit"</span><span class="op">)</span>, 
                 data <span class="op">=</span> <span class="va">nsw_dw_cpscontrol</span><span class="op">)</span>

<span class="va">nsw_dw_cpscontrol</span> <span class="op">&lt;-</span> <span class="va">nsw_dw_cpscontrol</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>pscore <span class="op">=</span> <span class="va">logit_nsw</span><span class="op">$</span><span class="va">fitted.values</span><span class="op">)</span>

<span class="co"># mean pscore </span>
<span class="va">pscore_control</span> <span class="op">&lt;-</span> <span class="va">nsw_dw_cpscontrol</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">treat</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html">pull</a></span><span class="op">(</span><span class="va">pscore</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">)</span>

<span class="va">pscore_treated</span> <span class="op">&lt;-</span> <span class="va">nsw_dw_cpscontrol</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">treat</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html">pull</a></span><span class="op">(</span><span class="va">pscore</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">)</span>

<span class="co"># histogram</span>
<span class="va">nsw_dw_cpscontrol</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">treat</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_histogram.html">geom_histogram</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">pscore</span><span class="op">)</span><span class="op">)</span>

<span class="va">nsw_dw_cpscontrol</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">treat</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_histogram.html">geom_histogram</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">pscore</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p>The propensity score is the fitted values of the logit model. Put differently, we used the estimated coefficients from that logit regression to estimate the conditional probability of treatment, assuming that probabilities are based on the cumulative logistic distribution:
<span class="math display">\[
\Pr\big(D=1\mid X\big) = F(\beta_0 + \gamma \text{Treat} + \alpha X)
\]</span>
where <span class="math inline">\(F()=\dfrac{e}{(1+e)}\)</span> and <span class="math inline">\(X\)</span> is the exogenous covariates we are including in the model.</p>
<p>As I said earlier, the propensity score used the fitted values from the maximum likelihood regression to calculate each unit’s conditional probability of treatment <em>regardless of actual treatment status</em>. The propensity score is just the predicted conditional probability of treatment or fitted value for each unit. It is advisable to use maximum likelihood when estimating the propensity score so that the fitted values are in the range <span class="math inline">\([0,1]\)</span>. We could use a linear probability model, but linear probability models routinely create fitted values below 0 and above 1, which are not true probabilities since <span class="math inline">\(0\leq p \leq 1\)</span>.</p>
<p>The definition of the propensity score is the selection probability conditional on the confounding variables; <span class="math inline">\(p(X)=\Pr(D=1\mid X)\)</span>. Recall that we said there are two identifying assumptions for propensity score methods. The first assumption is CIA. That is, <span class="math inline">\((Y^0,Y^1) \perp \!\!\! \perp D\mid X\)</span>. It is not testable, because the assumption is based on unobservable potential outcomes. The second assumption is called the <em>common support</em> assumption. That is, <span class="math inline">\(0&lt;\Pr(D=1\mid X) &lt; 1\)</span>. This simply means that for any probability, there must be units in both the treatment group <em>and</em> the control group. The conditional independence assumption simply means that the backdoor criterion is met in the data by conditioning on a vector <span class="math inline">\(X\)</span>. Or, put another way, conditional on <span class="math inline">\(X\)</span>, the assignment of units to the treatment is <em>as good as random</em>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;CIA is expressed in different ways according to the econometric or statistical tradition. &lt;span class="citation"&gt;Rosenbaum and Rubin (&lt;a href="references.html#ref-Rosenbaum1983" role="doc-biblioref"&gt;1983&lt;/a&gt;)&lt;/span&gt; called it the ignorable treatment assignment, or unconfoundedness. &lt;span class="citation"&gt;Barnow, Cain, and Goldberger (&lt;a href="references.html#ref-Barnow1981" role="doc-biblioref"&gt;1981&lt;/a&gt;)&lt;/span&gt; and &lt;span class="citation"&gt;Dale and Krueger (&lt;a href="references.html#ref-Dale2002" role="doc-biblioref"&gt;2002&lt;/a&gt;)&lt;/span&gt; called it &lt;em&gt;selection on observables&lt;/em&gt;. In the traditional econometric pedagogy, as we discussed earlier, it’s called the zero conditional mean assumption.&lt;/p&gt;'><sup>88</sup></a></p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:pscore1">Table 5.15: </span> Distribution of propensity score for treatment group.</caption>
<tbody>
<tr class="odd">
<td></td>
<td align="center"><em>Treatment group</em></td>
<td></td>
</tr>
<tr class="even">
<td><strong>Percentiles</strong></td>
<td align="center"><strong>Values</strong></td>
<td><strong>Smallest</strong></td>
</tr>
<tr class="odd">
<td>1%</td>
<td align="center">0.0011757</td>
<td>0.0010614</td>
</tr>
<tr class="even">
<td>5%</td>
<td align="center">0.0072641</td>
<td>0.0011757</td>
</tr>
<tr class="odd">
<td>10%</td>
<td align="center">0.0260147</td>
<td>0.0018463</td>
</tr>
<tr class="even">
<td>25%</td>
<td align="center">0.1322174</td>
<td>0.0020981</td>
</tr>
<tr class="odd">
<td>50%</td>
<td align="center">0.4001992</td>
<td></td>
</tr>
<tr class="even">
<td><strong>Percentiles</strong></td>
<td align="center"><strong>Values</strong></td>
<td><strong>Largest</strong></td>
</tr>
<tr class="odd">
<td>75%</td>
<td align="center">0.6706164</td>
<td>0.935645</td>
</tr>
<tr class="even">
<td>90%</td>
<td align="center">0.8866026</td>
<td>0.93718</td>
</tr>
<tr class="odd">
<td>95%</td>
<td align="center">0.9021386</td>
<td>0.9374608</td>
</tr>
<tr class="even">
<td>99%</td>
<td align="center">0.9374608</td>
<td>0.9384554</td>
</tr>
</tbody>
</table></div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:pscore0">Table 5.16: </span> Distribution of propensity score for CPS Control group.</caption>
<tbody>
<tr class="odd">
<td></td>
<td align="center"><em>CPS Control group</em></td>
<td></td>
</tr>
<tr class="even">
<td><strong>Percentiles</strong></td>
<td align="center"><strong>Values</strong></td>
<td><strong>Largest</strong></td>
</tr>
<tr class="odd">
<td>1%</td>
<td align="center">5.90e-07</td>
<td>1.18e-09</td>
</tr>
<tr class="even">
<td>5%</td>
<td align="center">1.72e-06</td>
<td>4.07e-09</td>
</tr>
<tr class="odd">
<td>10%</td>
<td align="center">3.58e-06</td>
<td>4.24e-09</td>
</tr>
<tr class="even">
<td>25%</td>
<td align="center">0.0000193</td>
<td>1.55e-08</td>
</tr>
<tr class="odd">
<td>50%</td>
<td align="center">0.0001187</td>
<td></td>
</tr>
<tr class="even">
<td>50%</td>
<td align="center">.0003544</td>
<td></td>
</tr>
<tr class="odd">
<td><strong>Percentiles</strong></td>
<td align="center"><strong>Values</strong></td>
<td><strong>Largest</strong></td>
</tr>
<tr class="even">
<td>75%</td>
<td align="center">0.0009635</td>
<td>0.8786677</td>
</tr>
<tr class="odd">
<td>90%</td>
<td align="center">0.0066319</td>
<td>0.8893389</td>
</tr>
<tr class="even">
<td>95%</td>
<td align="center">0.0163109</td>
<td>0.9099022</td>
</tr>
<tr class="odd">
<td>99%</td>
<td align="center">0.1551548</td>
<td>0.9239787</td>
</tr>
</tbody>
</table></div>
<p>Common support is required to calculate any particular kind of defined average treatment effect, and without it, you will just get some kind of weird weighted average treatment effect for only those regions that do have common support. The reason it is “weird” is that average treatment effect doesn’t correspond to any of the interesting treatment effects the policymaker needed. Common support requires that for each value of <span class="math inline">\(X\)</span>, there is a positive probability of being both treated and untreated, or <span class="math inline">\(0&lt;\Pr(D_i=1\mid X_i) &lt; 1\)</span>. This implies that the probability of receiving treatment for every value of the vector <span class="math inline">\(X\)</span> is strictly within the unit interval. Common support ensures there is sufficient overlap in the characteristics of treated and untreated units to find adequate matches. Unlike CIA, the common support requirement is testable by simply plotting histograms or summarizing the data. Here we do that two ways: by looking at the summary statistics and by looking at a histogram. Let’s start with looking at a distribution in table form before looking at the histogram.</p>
<p>The mean value of the propensity score for the treatment group is 0.43, and the mean for the CPS control group is 0.007. The 50th percentile for the treatment group is 0.4, but the control group doesn’t reach that high a number until the 99th percentile. Let’s look at the distribution of the propensity score for the two groups using a histogram now.</p>
<div class="figure">
<span id="fig:pscorehist"></span>
<img src="graphics/propensity_score_hist.jpg" alt="Histogram of propensity score by treatment status." width="100%"><p class="caption">
Figure 5.3: Histogram of propensity score by treatment status.
</p>
</div>
<p>These two simple diagnostic tests show what is going to be a problem later when we use inverse probability weighting. The probability of treatment is spread out across the units in the treatment group, but there is a very large mass of nearly zero propensity scores in the CPS. How do we interpret this? What this means is that the characteristics of individuals in the treatment group are rare in the CPS sample. This is not surprising given the strong negative selection into treatment. These individuals are younger, less likely to be married, and more likely to be uneducated and a minority. The lesson is, if the two groups are significantly different on background characteristics, then the propensity scores will have grossly different distributions by treatment status. We will discuss this in greater detail later.</p>
<p>For now, let’s look at the treatment parameter under both assumptions.</p>
<p><span class="math display">\[\begin{align}
   E[\delta_i(X_i)] &amp; =E\big[Y_i^1 - Y_i^0\mid X_i=x\big]                     \\
    &amp; = E\big[Y_i^1\mid X_i=x\big]-E\big[Y_i^0\mid X_i=x\big] 
\end{align}\]</span></p>
<p>The conditional independence assumption allows us to make the following substitution,
<span class="math display">\[
E\big[Y^1_i\mid D_i = 1, X_i = x\big] =
   E\big[Y_i\mid D_i = 1, X_i = x\big]
\]</span>
and same for the other term. Common support means we can estimate both terms. Therefore, under both assumptions:
<span class="math display">\[
\delta = E[\delta(X_i)]
\]</span>
From these assumptions we get the <em>propensity score theorem</em>, which states that under CIA
<span class="math display">\[
(Y^1,Y^0) \perp \!\!\! \perp D\mid X
\]</span>
This then yields
<span class="math display">\[
(Y^1,Y^0) \perp \!\!\! \perp D \mid p(X)
\]</span>
where <span class="math inline">\(p(X) =\Pr(D=1\mid X)\)</span>, the propensity score. In English, this means that in order to achieve independence, assuming CIA, all we have to do is condition on the propensity score. Conditioning on the propensity score is enough to have independence between the treatment and the potential outcomes.</p>
<p>This is an extremely valuable theorem because stratifying on <span class="math inline">\(X\)</span> tends to run into the sparseness-related problems (i.e., empty cells) in finite samples for even a moderate number of covariates. But the propensity scores are just a scalar. So stratifying across a probability is going to reduce that dimensionality problem.</p>
<p>The proof of the propensity score theorem is fairly straightforward, as it’s just an application of the law of iterated expectations with nested conditioning.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;See &lt;span class="citation"&gt;Angrist and Pischke (&lt;a href="references.html#ref-Angrist2009" role="doc-biblioref"&gt;2009&lt;/a&gt;)&lt;/span&gt;, 80–81.&lt;/p&gt;'><sup>89</sup></a> If we can show that the probability an individual receives treatment conditional on potential outcomes and the propensity score is not a function of potential outcomes, then we will have proved that there is independence between the potential outcomes and the treatment conditional on <span class="math inline">\(X\)</span>. Before diving into the proof, first recognize that
<span class="math display">\[
\Pr\big(D=1\mid Y^0, Y^1, p(X)\big)=E\big[D\mid Y^0, Y^1, p(X)\big]
\]</span>
because</p>
<p><span class="math display">\[\begin{align}
   E\big[D\mid Y^0, Y^1, p(X)\big] &amp; =1 \times                                     
   \Pr\big(D=1\mid Y^0, Y^1, p(X)\big)
   \\
    &amp; +0 \times \Pr\big(D=0\mid Y^0, Y^1, p(X)\big) 
\end{align}\]</span></p>
<p>and the second term cancels out because it’s multiplied by zero. The formal proof is as follows:</p>
<p><span class="math display">\[\begin{align}
   \Pr\big(D=1\mid Y^1,Y^0, p(X)\big) &amp; = \underbrace{{E\big[D\mid Y^1, Y^0, p(X)\big]}}_{\text{See previous equation}}
   \\
        &amp; =\underbrace{{E}                                                                           
   \Big[E \big[D\mid Y^1,Y^0,p(X),X\big] {\mid Y^1,Y^0,p(X)\Big]}}_{ \text{by LIE}}
   \\
        &amp; =\underbrace{{E} \Big[E                                                                    
   \big[D\mid Y^1,Y^0,X\big] {\mid Y^1,Y^0,p(X)\Big]}}_{\text{Given $X$, we know $p(X)$}}
   \\
        &amp; =\underbrace{{E}                                                                           
   \Big[E \big[D\mid X\big] {\mid Y^1,Y^0,p(X)\Big]}}_{ \text{by conditional independence}}
   \\
        &amp; = \underbrace{{E}                                                                          
   \Big[p(X) {\mid Y^1, Y^0, p(X)\Big]}}_{\text{propensity score definition}}
   \\
        &amp; = p(X)                                                                                     
\end{align}\]</span></p>
<p>Using a similar argument, we obtain:</p>
<p><span class="math display">\[\begin{align}
   \Pr\big(D=1\mid p(X)\big) &amp; =                                                                             
   \underbrace{E\big[D\mid p(X) \big]}_{\text{Previous argument}}
   \\
            &amp; =\underbrace{E \Big[E\big[D\mid X\big]\mid p(X)\Big]}_{\text{LIE}} 
   \\
            &amp; =\underbrace{E\big[p(X)\mid p(X)\mid]}_{\text{definition}}        
   \\
            &amp; = p(X)                                                                        
\end{align}\]</span></p>
<p>and <span class="math inline">\(\Pr(D=1\mid Y^1, Y^0, p(X)) = \Pr(D=1\mid p(X))\)</span> by CIA.</p>
<p>Like the omitted variable bias formula for regression, the propensity score theorem says that you need only control for covariates that determine the likelihood a unit receives the treatment. But it also says something more than that. It technically says that the <em>only</em> covariate you need to condition on is the propensity score. All of the information from the <span class="math inline">\(X\)</span> matrix has been collapsed into a single number: the propensity score.</p>
<p>A corollary of the propensity score theorem, therefore, states that given CIA, we can estimate average treatment effects by weighting appropriately the simple difference in means.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;This all works if we match on the propensity score and then calculate differences in means. Direct propensity score matching works in the same way as the covariate matching we discussed earlier (e.g., nearest-neighbor matching), except that we match on the &lt;em&gt;score&lt;/em&gt; instead of the &lt;em&gt;covariates&lt;/em&gt; directly.&lt;/p&gt;"><sup>90</sup></a></p>
<p>Because the propensity score is a function of <span class="math inline">\(X\)</span>, we know</p>
<p><span class="math display">\[\begin{align}
   \Pr\big(D=1\mid X, p(X)\big) &amp; =\Pr\big(D=1\mid X\big) 
   \\
    &amp; = p(X)                  
\end{align}\]</span></p>
<p>Therefore, conditional on the propensity score, the probability that <span class="math inline">\(D=1\)</span> does not depend on <span class="math inline">\(X\)</span> any longer. That is, <span class="math inline">\(D\)</span> and <span class="math inline">\(X\)</span> are independent of one another conditional on the propensity score, or
<span class="math display">\[
D \perp \!\!\! \perp\mid p(X)
\]</span>
So from this we also obtain the <em>balancing property</em> of the propensity score:
<span class="math display">\[
\Pr\big(X \mid| D=1, p(X)\big)=
   \Pr\big(X\mid D=0, p(X)\big)
\]</span>
which states that conditional on the propensity score, the distribution of the covariates is the same for treatment as it is for control group units. See this in the following DAG:</p>
<div class="inline-figure"><img src="causal_inference_mixtape_files/figure-html/unnamed-chunk-64-1.png" width="50%" style="display: block; margin: auto;"></div>
<p>Notice that there exist two paths between <span class="math inline">\(X\)</span> and <span class="math inline">\(D\)</span>. There’s the direct path of <span class="math inline">\(X \rightarrow p(X) \rightarrow D\)</span>, and there’s the backdoor path <span class="math inline">\(X \rightarrow Y \leftarrow D\)</span>. The backdoor path is blocked by a collider, so there is no systematic correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(D\)</span> through it. But there is systematic correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(D\)</span> through the first directed path. But, when we condition on <span class="math inline">\(p(X)\)</span>, the propensity score, notice that <span class="math inline">\(D\)</span> and <span class="math inline">\(X\)</span> are statistically <em>independent</em>. This implies that <span class="math inline">\(D \perp \!\!\! \perp X \mid p(X)\)</span>, which implies
<span class="math display">\[
\Pr\big(X \mid D=1, \widehat{p}(X\big)=
   \Pr\big(X\mid D=0, \widehat{p}(X)\big)
\]</span>
This is something we can directly test, but note the implication: conditional on the propensity score, treatment and control should on average be the same with respect to <span class="math inline">\(X\)</span>. In other words, the propensity score theorem implies <em>balanced</em> observable covariates.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Just because something is exchangeable on observables does not make it exchangeable on unobservables. The propensity score theorem does &lt;em&gt;not&lt;/em&gt; imply balanced unobserved covariates. See &lt;span class="citation"&gt;Brooks and Ohsfeldt (&lt;a href="references.html#ref-Brooks2013" role="doc-biblioref"&gt;2013&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;'><sup>91</sup></a></p>
</div>
<div id="weighting-on-the-propensity-score" class="section level3" number="5.3.5">
<h3>
<span class="header-section-number">5.3.5</span> Weighting on the propensity score<a class="anchor" aria-label="anchor" href="#weighting-on-the-propensity-score"><i class="fas fa-link"></i></a>
</h3>
<p>There are several ways researchers can estimate average treatment effects using an estimated propensity score. <span class="citation">Busso, DiNardo, and McCrary (<a href="references.html#ref-Busso2014" role="doc-biblioref">2014</a>)</span> examined the properties of various approaches and found that inverse probability weighting was competitive in several simulations. As there are different ways in which the weights are incorporated into a weighting design, I discuss a few canonical versions of the method of inverse probability weighting and associated methods for inference. This is an expansive area in causal inference econometrics, so consider this merely an overview of and introduction to the main concepts.</p>
<p>Assuming that CIA holds in our data, then one way we can estimate treatment effects is to use a weighting procedure in which each individual’s propensity score is a weight of that individual’s outcome <span class="citation">(Imbens <a href="references.html#ref-Imbens2000" role="doc-biblioref">2000</a>)</span>. When aggregated, this has the potential to identify some average treatment effect. This estimator is based on earlier work in survey methodology first proposed by <span class="citation">Horvitz and Thompson (<a href="references.html#ref-Horvitz1952" role="doc-biblioref">1952</a>)</span>. The weight enters the expression differently depending on each unit’s treatment status and takes on two different forms depending on whether the target parameter is the ATE or the ATT (or the ATU, which is not shown here):</p>
<p><span class="math display">\[\begin{align}
   \delta_{ATE} &amp; =E[Y^1-Y^0] \nonumber                                                      
   \\
            &amp; =E \left[ Y \cdot \dfrac{D - p(X)}{p(X) \cdot (1-p(X))} \right]            
   \\
   \delta_{ATT} &amp; =E\big[Y^1-Y^0\mid D=1\big] \nonumber                                      
   \\
            &amp; =\dfrac{1}{\Pr(D=1)} \cdot E \left[ Y \cdot \dfrac{D-p(X)}{1-p(X)} \right] 
\end{align}\]</span></p>
<p>A proof for ATE is provided:</p>
<p><span class="math display">\[\begin{align}
   E \left[ Y \dfrac{D-p(X)}{p(X)(1-p(X))} \Big\vert X \right] &amp; = E \left[ \dfrac{Y}{p(X)} \Big\vert X,D=1 \right] p(X) \nonumber      
   \\
    &amp; + E\left[ \dfrac{-Y}{1-p(X)} \Big\vert X,D=0 \right](1-p(X)) \nonumber 
   \\
    &amp; = E\big[Y\mid X,D=1\big] - E\big[Y\mid X,D=0\big]                      
\end{align}\]</span></p>
<p>and the results follow from integrating over <span class="math inline">\(P(X)\)</span> and <span class="math inline">\(P(X\mid D=1)\)</span>.</p>
<p>The sample versions of both ATE and ATT are obtained by a two-step estimation procedure. In the first step, the researcher estimates the propensity score using logit or probit. In the second step, the researcher uses the estimated score to produce sample versions of one of the average treatment effect estimators shown above. Those sample versions can be written as follows:</p>
<p><span class="math display">\[\begin{align}
   \widehat{\delta}_{ATE} &amp; =\dfrac{1}{N} \sum_{i=1}^N Y_i \cdot \dfrac{D_i - \widehat{p}(X_i)}{\widehat{p}(X_i) \cdot (1-\widehat{p}(X_i))} 
   \\
   \widehat{\delta}_{ATT}           &amp; =\dfrac{1}{N_T} \sum_{i=1}^N Y_i \cdot \dfrac{D_i -\widehat{p}(X_i)}{1-\widehat{p}(X_i)}                         
\end{align}\]</span></p>
<p>We have a few options for estimating the variance of this estimator, but one is simply to use bootstrapping. First created by <span class="citation">Efron (<a href="references.html#ref-Efron1979" role="doc-biblioref">1979</a>)</span>, bootstrapping is a procedure used to estimate the variance of an estimator. In the context of inverse probability weighting, we would repeatedly draw (“with replacement”) a random sample of our original data and then use that smaller sample to calculate the sample analogs of the ATE or ATT. More specifically, using the smaller “bootstrapped” data, we would first estimate the propensity score and then use the estimated propensity score to calculate sample analogs of the ATE or ATT over and over to obtain a distribution of treatment effects corresponding to different cuts of the data itself.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Bootstrapping and randomization inference are mechanically similar. Each randomizes &lt;em&gt;something&lt;/em&gt; over and over, and under each randomization, reestimates treatment effects to obtain a distribution of treatment effects. But that is where the similarity ends. Bootstrapping is a method for computing the variance in an estimator where we take the treatment assignment as given. The uncertainty in bootstrapping stems from the sample, not the treatment assignment. And thus with each bootstrapped sample, we use fewer observations than exist in our real sample. That is not the source of uncertainty in randomization inference, though. In randomization inference, as you recall from the earlier chapter, the uncertainty in question regards the treatment assignment, not the sample. And thus in randomization inference, we randomly assign the treatment in order to reject or fail to reject Fisher’s sharp null of no individual treatment effects.&lt;/p&gt;"><sup>92</sup></a> If we do this 1,000 or 10,000 times, we get a distribution of parameter estimates from which we can calculate the standard deviation. This standard deviation becomes like a standard error and gives us a measure of the dispersion of the parameter estimate under uncertainty regarding the sample itself.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;&lt;span class="citation"&gt;Abadie and Imbens (&lt;a href="references.html#ref-Abadie2008" role="doc-biblioref"&gt;2008&lt;/a&gt;)&lt;/span&gt; show that the bootstrap fails for &lt;em&gt;matching&lt;/em&gt;, but inverse probability weighting is not matching. This may seem like a subtle point, but in my experience many people conflate propensity score based matching with other methods that use the propensity score, calling all of them “matching.” But inverse probability weighting is &lt;em&gt;not&lt;/em&gt; a matching procedure. Rather, it is a weighting procedure whose properties differ from that of using imputation and generally the bootstrap is fine.&lt;/p&gt;'><sup>93</sup></a> <span class="citation">Adudumilli (<a href="references.html#ref-Adusumilli2018" role="doc-biblioref">2018</a>)</span> and <span class="citation">Bodory et al. (<a href="references.html#ref-Bodory2020" role="doc-biblioref">2020</a>)</span> discuss the performance of various bootstrapping procedures, such as the standard bootstrap or the wild bootstrap. I encourage you to read these papers more closely when choosing which bootstrap is suitable for your question.</p>
<p>The sensitivity of inverse probability weighting to extreme values of the propensity score has led some researchers to propose an alternative that can handle extremes a bit better. <span class="citation">Hirano and Imbens (<a href="references.html#ref-Hirano2001" role="doc-biblioref">2001</a>)</span> propose an inverse probability weighting estimator of the average treatment effect that assigns weights normalized by the sum of propensity scores for treated and control groups as opposed to equal weights of <span class="math inline">\(\dfrac{1}{N}\)</span> to each observation. This procedure is sometimes associated with <span class="citation">Hájek (<a href="references.html#ref-Hajek1971" role="doc-biblioref">1971</a>)</span>. <span class="citation">Millimet and Tchernis (<a href="references.html#ref-Millimet2009" role="doc-biblioref">2009</a>)</span> refer to this estimator as the normalized estimator. Its weights sum to one within each group, which tends to make it more stable. The expression of this normalized estimator is shown here:
<span class="math display">\[
\widehat{\delta}_{ATT}=\bigg [ \sum_{i=1}^N \dfrac{Y_iD_i}{\widehat{p}} \bigg ] / \bigg [ \sum_{i=1}^N \dfrac{D_i}{\widehat{p}} \bigg ] - \bigg [ \sum_{i=1}^N \dfrac{Y_i(1-D_i)}{(1-\widehat{p})} \bigg ] / \bigg [ \sum_{i=1}^N \dfrac{(1-D_i)}{(1-\widehat{p})} \bigg ]
\]</span>
Most software packages have programs that will estimate the sample analog of these inverse probability weighted parameters that use the second method with normalized weights. For instance, Stata’s -teffects- and R’s -ipw- can both be used. These packages will also generate standard errors. But I’d like to manually calculate these point estimates so that you can see more clearly exactly how to use the propensity score to construct either non-normalized or normalized weights and then estimate ATT.</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/ipw.do"><code>ipw.do</code></a></em></p>
<div class="sourceCode" id="cb45"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb45-1"><a href="ch4.html#cb45-1" aria-hidden="true"></a>* Manual with non-normalized weights <span class="kw">using</span> <span class="ot">all</span> the <span class="kw">data</span></span>
<span id="cb45-2"><a href="ch4.html#cb45-2" aria-hidden="true"></a><span class="kw">gen</span> d1=treat/pscore</span>
<span id="cb45-3"><a href="ch4.html#cb45-3" aria-hidden="true"></a><span class="kw">gen</span> <span class="kw">d0</span>=(1-treat)/(1-pscore)</span>
<span id="cb45-4"><a href="ch4.html#cb45-4" aria-hidden="true"></a><span class="kw">egen</span> s1=<span class="kw">sum</span>(d1)</span>
<span id="cb45-5"><a href="ch4.html#cb45-5" aria-hidden="true"></a><span class="kw">egen</span> s0=<span class="kw">sum</span>(<span class="kw">d0</span>)</span>
<span id="cb45-6"><a href="ch4.html#cb45-6" aria-hidden="true"></a></span>
<span id="cb45-7"><a href="ch4.html#cb45-7" aria-hidden="true"></a><span class="kw">gen</span> y1=treat*re78/pscore</span>
<span id="cb45-8"><a href="ch4.html#cb45-8" aria-hidden="true"></a><span class="kw">gen</span> y0=(1-treat)*re78/(1-pscore)</span>
<span id="cb45-9"><a href="ch4.html#cb45-9" aria-hidden="true"></a><span class="kw">gen</span> ht=y1-y0</span>
<span id="cb45-10"><a href="ch4.html#cb45-10" aria-hidden="true"></a></span>
<span id="cb45-11"><a href="ch4.html#cb45-11" aria-hidden="true"></a>* Manual with normalized weights</span>
<span id="cb45-12"><a href="ch4.html#cb45-12" aria-hidden="true"></a><span class="kw">replace</span> y1=(treat*re78/pscore)/(s1/_N)</span>
<span id="cb45-13"><a href="ch4.html#cb45-13" aria-hidden="true"></a><span class="kw">replace</span> y0=((1-treat)*re78/(1-pscore))/(s0/_N)</span>
<span id="cb45-14"><a href="ch4.html#cb45-14" aria-hidden="true"></a><span class="kw">gen</span> <span class="fu">norm</span>=y1-y0</span>
<span id="cb45-15"><a href="ch4.html#cb45-15" aria-hidden="true"></a>su ht <span class="fu">norm</span></span>
<span id="cb45-16"><a href="ch4.html#cb45-16" aria-hidden="true"></a></span>
<span id="cb45-17"><a href="ch4.html#cb45-17" aria-hidden="true"></a>* ATT under non-normalized weights is -<span class="ot">$11</span>,876</span>
<span id="cb45-18"><a href="ch4.html#cb45-18" aria-hidden="true"></a>* ATT under normalized weights is -<span class="ot">$7</span>,238</span>
<span id="cb45-19"><a href="ch4.html#cb45-19" aria-hidden="true"></a></span>
<span id="cb45-20"><a href="ch4.html#cb45-20" aria-hidden="true"></a><span class="kw">drop</span> d1 <span class="kw">d0</span> s1 s0 y1 y0 ht <span class="fu">norm</span></span>
<span id="cb45-21"><a href="ch4.html#cb45-21" aria-hidden="true"></a></span>
<span id="cb45-22"><a href="ch4.html#cb45-22" aria-hidden="true"></a>* Trimming the propensity <span class="kw">score</span></span>
<span id="cb45-23"><a href="ch4.html#cb45-23" aria-hidden="true"></a><span class="kw">drop</span> <span class="kw">if</span> pscore &lt;= 0.1 </span>
<span id="cb45-24"><a href="ch4.html#cb45-24" aria-hidden="true"></a><span class="kw">drop</span> <span class="kw">if</span> pscore &gt;= 0.9</span>
<span id="cb45-25"><a href="ch4.html#cb45-25" aria-hidden="true"></a></span>
<span id="cb45-26"><a href="ch4.html#cb45-26" aria-hidden="true"></a>* Manual with non-normalized weights <span class="kw">using</span> trimmed <span class="kw">data</span></span>
<span id="cb45-27"><a href="ch4.html#cb45-27" aria-hidden="true"></a><span class="kw">gen</span> d1=treat/pscore</span>
<span id="cb45-28"><a href="ch4.html#cb45-28" aria-hidden="true"></a><span class="kw">gen</span> <span class="kw">d0</span>=(1-treat)/(1-pscore)</span>
<span id="cb45-29"><a href="ch4.html#cb45-29" aria-hidden="true"></a><span class="kw">egen</span> s1=<span class="kw">sum</span>(d1)</span>
<span id="cb45-30"><a href="ch4.html#cb45-30" aria-hidden="true"></a><span class="kw">egen</span> s0=<span class="kw">sum</span>(<span class="kw">d0</span>)</span>
<span id="cb45-31"><a href="ch4.html#cb45-31" aria-hidden="true"></a></span>
<span id="cb45-32"><a href="ch4.html#cb45-32" aria-hidden="true"></a><span class="kw">gen</span> y1=treat*re78/pscore</span>
<span id="cb45-33"><a href="ch4.html#cb45-33" aria-hidden="true"></a><span class="kw">gen</span> y0=(1-treat)*re78/(1-pscore)</span>
<span id="cb45-34"><a href="ch4.html#cb45-34" aria-hidden="true"></a><span class="kw">gen</span> ht=y1-y0</span>
<span id="cb45-35"><a href="ch4.html#cb45-35" aria-hidden="true"></a></span>
<span id="cb45-36"><a href="ch4.html#cb45-36" aria-hidden="true"></a>* Manual with normalized weights <span class="kw">using</span> trimmed <span class="kw">data</span></span>
<span id="cb45-37"><a href="ch4.html#cb45-37" aria-hidden="true"></a><span class="kw">replace</span> y1=(treat*re78/pscore)/(s1/_N)</span>
<span id="cb45-38"><a href="ch4.html#cb45-38" aria-hidden="true"></a><span class="kw">replace</span> y0=((1-treat)*re78/(1-pscore))/(s0/_N)</span>
<span id="cb45-39"><a href="ch4.html#cb45-39" aria-hidden="true"></a><span class="kw">gen</span> <span class="fu">norm</span>=y1-y0</span>
<span id="cb45-40"><a href="ch4.html#cb45-40" aria-hidden="true"></a>su ht <span class="fu">norm</span></span>
<span id="cb45-41"><a href="ch4.html#cb45-41" aria-hidden="true"></a></span>
<span id="cb45-42"><a href="ch4.html#cb45-42" aria-hidden="true"></a>* ATT under non-normalized weights is <span class="ot">$2</span>,006</span>
<span id="cb45-43"><a href="ch4.html#cb45-43" aria-hidden="true"></a>* ATT under normalized weights is <span class="ot">$1</span>,806</span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/ipw.R"><code>ipw.R</code></a></em></p>
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://haven.tidyverse.org">haven</a></span><span class="op">)</span>

<span class="co">#continuation</span>
<span class="va">N</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">nsw_dw_cpscontrol</span><span class="op">)</span>
<span class="co">#- Manual with non-normalized weights using all data</span>
<span class="va">nsw_dw_cpscontrol</span> <span class="op">&lt;-</span> <span class="va">nsw_dw_cpscontrol</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>d1 <span class="op">=</span> <span class="va">treat</span><span class="op">/</span><span class="va">pscore</span>,
         d0 <span class="op">=</span> <span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">treat</span><span class="op">)</span><span class="op">/</span><span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">pscore</span><span class="op">)</span><span class="op">)</span>

<span class="va">s1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">nsw_dw_cpscontrol</span><span class="op">$</span><span class="va">d1</span><span class="op">)</span>
<span class="va">s0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">nsw_dw_cpscontrol</span><span class="op">$</span><span class="va">d0</span><span class="op">)</span>


<span class="va">nsw_dw_cpscontrol</span> <span class="op">&lt;-</span> <span class="va">nsw_dw_cpscontrol</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>y1 <span class="op">=</span> <span class="va">treat</span> <span class="op">*</span> <span class="va">re78</span><span class="op">/</span><span class="va">pscore</span>,
         y0 <span class="op">=</span> <span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">treat</span><span class="op">)</span> <span class="op">*</span> <span class="va">re78</span><span class="op">/</span><span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">pscore</span><span class="op">)</span>,
         ht <span class="op">=</span> <span class="va">y1</span> <span class="op">-</span> <span class="va">y0</span><span class="op">)</span>

<span class="co">#- Manual with normalized weights</span>
<span class="va">nsw_dw_cpscontrol</span> <span class="op">&lt;-</span> <span class="va">nsw_dw_cpscontrol</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>y1 <span class="op">=</span> <span class="op">(</span><span class="va">treat</span><span class="op">*</span><span class="va">re78</span><span class="op">/</span><span class="va">pscore</span><span class="op">)</span><span class="op">/</span><span class="op">(</span><span class="va">s1</span><span class="op">/</span><span class="va">N</span><span class="op">)</span>,
         y0 <span class="op">=</span> <span class="op">(</span><span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">treat</span><span class="op">)</span><span class="op">*</span><span class="va">re78</span><span class="op">/</span><span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">pscore</span><span class="op">)</span><span class="op">)</span><span class="op">/</span><span class="op">(</span><span class="va">s0</span><span class="op">/</span><span class="va">N</span><span class="op">)</span>,
         norm <span class="op">=</span> <span class="va">y1</span> <span class="op">-</span> <span class="va">y0</span><span class="op">)</span>

<span class="va">nsw_dw_cpscontrol</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html">pull</a></span><span class="op">(</span><span class="va">ht</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">)</span>

<span class="va">nsw_dw_cpscontrol</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html">pull</a></span><span class="op">(</span><span class="va">norm</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">)</span>

<span class="co">#-- trimming propensity score</span>
<span class="va">nsw_dw_cpscontrol</span> <span class="op">&lt;-</span> <span class="va">nsw_dw_cpscontrol</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="op">-</span><span class="va">d1</span>, <span class="op">-</span><span class="va">d0</span>, <span class="op">-</span><span class="va">y1</span>, <span class="op">-</span><span class="va">y0</span>, <span class="op">-</span><span class="va">ht</span>, <span class="op">-</span><span class="va">norm</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="op">!</span><span class="op">(</span><span class="va">pscore</span> <span class="op">&gt;=</span> <span class="fl">0.9</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="op">!</span><span class="op">(</span><span class="va">pscore</span> <span class="op">&lt;=</span> <span class="fl">0.1</span><span class="op">)</span><span class="op">)</span>

<span class="va">N</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">nsw_dw_cpscontrol</span><span class="op">)</span>

<span class="co">#- Manual with non-normalized weights using trimmed data</span>
<span class="va">nsw_dw_cpscontrol</span> <span class="op">&lt;-</span> <span class="va">nsw_dw_cpscontrol</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>d1 <span class="op">=</span> <span class="va">treat</span><span class="op">/</span><span class="va">pscore</span>,
         d0 <span class="op">=</span> <span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">treat</span><span class="op">)</span><span class="op">/</span><span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">pscore</span><span class="op">)</span><span class="op">)</span>

<span class="va">s1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">nsw_dw_cpscontrol</span><span class="op">$</span><span class="va">d1</span><span class="op">)</span>
<span class="va">s0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">nsw_dw_cpscontrol</span><span class="op">$</span><span class="va">d0</span><span class="op">)</span>

<span class="va">nsw_dw_cpscontrol</span> <span class="op">&lt;-</span> <span class="va">nsw_dw_cpscontrol</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>y1 <span class="op">=</span> <span class="va">treat</span> <span class="op">*</span> <span class="va">re78</span><span class="op">/</span><span class="va">pscore</span>,
         y0 <span class="op">=</span> <span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">treat</span><span class="op">)</span> <span class="op">*</span> <span class="va">re78</span><span class="op">/</span><span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">pscore</span><span class="op">)</span>,
         ht <span class="op">=</span> <span class="va">y1</span> <span class="op">-</span> <span class="va">y0</span><span class="op">)</span>

<span class="co">#- Manual with normalized weights with trimmed data</span>
<span class="va">nsw_dw_cpscontrol</span> <span class="op">&lt;-</span> <span class="va">nsw_dw_cpscontrol</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>y1 <span class="op">=</span> <span class="op">(</span><span class="va">treat</span><span class="op">*</span><span class="va">re78</span><span class="op">/</span><span class="va">pscore</span><span class="op">)</span><span class="op">/</span><span class="op">(</span><span class="va">s1</span><span class="op">/</span><span class="va">N</span><span class="op">)</span>,
         y0 <span class="op">=</span> <span class="op">(</span><span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">treat</span><span class="op">)</span><span class="op">*</span><span class="va">re78</span><span class="op">/</span><span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">pscore</span><span class="op">)</span><span class="op">)</span><span class="op">/</span><span class="op">(</span><span class="va">s0</span><span class="op">/</span><span class="va">N</span><span class="op">)</span>,
         norm <span class="op">=</span> <span class="va">y1</span> <span class="op">-</span> <span class="va">y0</span><span class="op">)</span>

<span class="va">nsw_dw_cpscontrol</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html">pull</a></span><span class="op">(</span><span class="va">ht</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">)</span>

<span class="va">nsw_dw_cpscontrol</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html">pull</a></span><span class="op">(</span><span class="va">norm</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">)</span></code></pre></div>
<p>When we estimate the treatment effect using inverse probability weighting using the non-normalized weighting procedure described earlier, we find an estimated ATT of <span class="math inline">\(-\$11,876\)</span>. Using the normalization of the weights, we get <span class="math inline">\(-\$7,238\)</span>. Why is this so much different than what we get using the experimental data?</p>
<p>Recall what inverse probability weighting is doing. It is weighting treatment and control units according to <span class="math inline">\(\widehat{p}(X)\)</span>, which is causing units with very small values of the propensity score to blow up and become unusually influential in the calculation of ATT. Thus, we will need to trim the data. Here we will do a very small trim to eliminate the mass of values at the far-left tail. <span class="citation">Crump et al. (<a href="references.html#ref-Crump2009" role="doc-biblioref">2009</a>)</span> develop a principled method for addressing a lack of overlap. A good rule of thumb, they note, is to keep only observations on the interval [0.1,0.9], which was performed at the end of the program.</p>
<p>Now let’s repeat the analysis having trimmed the propensity score, keeping only values whose scores are between 0.1 and 0.9. Now we find $2,006 using the non-normalized weights and $1,806 using the normalized weights. This is very similar to what we know is the true causal effect using the experimental data, which was $1,794. And we can see that the normalized weights are even closer. We still need to calculate standard errors, such as based on a bootstrapping method, but I leave it to you investigate that more carefully by reading <span class="citation">Adudumilli (<a href="references.html#ref-Adusumilli2018" role="doc-biblioref">2018</a>)</span> and <span class="citation">Bodory et al. (<a href="references.html#ref-Bodory2020" role="doc-biblioref">2020</a>)</span>, who, as I mentioned, discuss the performance of various bootstrapping procedures such as the standard bootstrap and the wild bootstrap.</p>
</div>
<div id="nearest-neighbor-matching" class="section level3" number="5.3.6">
<h3>
<span class="header-section-number">5.3.6</span> Nearest-neighbor matching<a class="anchor" aria-label="anchor" href="#nearest-neighbor-matching"><i class="fas fa-link"></i></a>
</h3>
<p>An alternative, very popular approach to inverse probability weighting is matching on the propensity score. This is often done by finding a couple of units with comparable propensity scores from the control unit donor pool within some ad hoc chosen radius distance of the treated unit’s own propensity score. The researcher then averages the outcomes and then assigns that average as an imputation to the original treated unit as a proxy for the potential outcome under counterfactual control. Then effort is made to enforce common support through trimming.</p>
<p>But this method has been criticized by <span class="citation">King and Nielsen (<a href="references.html#ref-King2019" role="doc-biblioref">2019</a>)</span>. The <span class="citation">King and Nielsen (<a href="references.html#ref-King2019" role="doc-biblioref">2019</a>)</span> critique is not of the propensity score itself. For instance, the critique does not apply to stratification based on the propensity score <span class="citation">(Rosenbaum and Rubin <a href="references.html#ref-Rosenbaum1983" role="doc-biblioref">1983</a>)</span>, regression adjustment or inverse probability weighting. The problem is only focused on nearest-neighbor matching and is related to the forced balance through trimming as well as myriad other common research choices made in the course of the project that together ultimately amplify bias. <span class="citation">King and Nielsen (<a href="references.html#ref-King2019" role="doc-biblioref">2019</a>)</span> write: “The more balanced the data, or the more balance it becomes by trimming some of the observations through matching, the more likely propensity score matching will degrade inferences” (p.1).</p>
<p>Nevertheless, nearest-neighbor matching, along with inverse probability weighting, is perhaps the most common method for estimating a propensity score model. Nearest-neighbor matching using the propensity score pairs each treatment unit <span class="math inline">\(i\)</span> with one or more comparable control group units <span class="math inline">\(j\)</span>, where comparability is measured in terms of distance to the nearest propensity score. This control group unit’s outcome is then plugged into a matched sample. Once we have the matched sample, we can calculate the ATT as
<span class="math display">\[
   \widehat{ATT}=\dfrac{1}{N_T} (Y_i - Y_{i(j)})
\]</span></p>
<p>where <span class="math inline">\(Y_{i(j)}\)</span> is the matched control group unit to <span class="math inline">\(i\)</span>. We will focus on the ATT because of the problems with overlap that we discussed earlier.</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/teffects_nn.do"><code>teffects_nn.do</code></a></em></p>
<div class="sourceCode" id="cb47"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb47-1"><a href="ch4.html#cb47-1" aria-hidden="true"></a>teffects psmatch (re78) (treat age agesq agecube educ edusq marr nodegree <span class="bn">black</span> hisp re74 re75 u74 u75 interaction1, <span class="kw">logit</span>), atet <span class="kw">gen</span>(pstub_cps) nn(5)</span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/teffects_nn.R"><code>teffects_nn.R</code></a></em></p>
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">MatchIt</span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">Zelig</span><span class="op">)</span>

<span class="va">m_out</span> <span class="op">&lt;-</span> <span class="fu">matchit</span><span class="op">(</span><span class="va">treat</span> <span class="op">~</span> <span class="va">age</span> <span class="op">+</span> <span class="va">agesq</span> <span class="op">+</span> <span class="va">agecube</span> <span class="op">+</span> <span class="va">educ</span> <span class="op">+</span>
                 <span class="va">educsq</span> <span class="op">+</span> <span class="va">marr</span> <span class="op">+</span> <span class="va">nodegree</span> <span class="op">+</span>
                 <span class="va">black</span> <span class="op">+</span> <span class="va">hisp</span> <span class="op">+</span> <span class="va">re74</span> <span class="op">+</span> <span class="va">re75</span> <span class="op">+</span> <span class="va">u74</span> <span class="op">+</span> <span class="va">u75</span> <span class="op">+</span> <span class="va">interaction1</span>,
                 data <span class="op">=</span> <span class="va">nsw_dw_cpscontrol</span>, method <span class="op">=</span> <span class="st">"nearest"</span>, 
                 distance <span class="op">=</span> <span class="st">"logit"</span>, ratio <span class="op">=</span><span class="fl">5</span><span class="op">)</span>

<span class="va">m_data</span> <span class="op">&lt;-</span> <span class="fu">match.data</span><span class="op">(</span><span class="va">m_out</span><span class="op">)</span>

<span class="va">z_out</span> <span class="op">&lt;-</span> <span class="fu">zelig</span><span class="op">(</span><span class="va">re78</span> <span class="op">~</span> <span class="va">treat</span> <span class="op">+</span> <span class="va">age</span> <span class="op">+</span> <span class="va">agesq</span> <span class="op">+</span> <span class="va">agecube</span> <span class="op">+</span> <span class="va">educ</span> <span class="op">+</span>
               <span class="va">educsq</span> <span class="op">+</span> <span class="va">marr</span> <span class="op">+</span> <span class="va">nodegree</span> <span class="op">+</span>
               <span class="va">black</span> <span class="op">+</span> <span class="va">hisp</span> <span class="op">+</span> <span class="va">re74</span> <span class="op">+</span> <span class="va">re75</span> <span class="op">+</span> <span class="va">u74</span> <span class="op">+</span> <span class="va">u75</span> <span class="op">+</span> <span class="va">interaction1</span>, 
               model <span class="op">=</span> <span class="st">"ls"</span>, data <span class="op">=</span> <span class="va">m_data</span><span class="op">)</span>

<span class="va">x_out</span> <span class="op">&lt;-</span> <span class="fu">setx</span><span class="op">(</span><span class="va">z_out</span>, treat <span class="op">=</span> <span class="fl">0</span><span class="op">)</span>
<span class="va">x1_out</span> <span class="op">&lt;-</span> <span class="fu">setx</span><span class="op">(</span><span class="va">z_out</span>, treat <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>

<span class="va">s_out</span> <span class="op">&lt;-</span> <span class="fu">sim</span><span class="op">(</span><span class="va">z_out</span>, x <span class="op">=</span> <span class="va">x_out</span>, x1 <span class="op">=</span> <span class="va">x1_out</span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">s_out</span><span class="op">)</span></code></pre></div>
<p>I chose to match using five nearest neighbors. Nearest neighbors, in other words, will find the five nearest units in the control group, where “nearest” is measured as closest on the propensity score itself. Unlike covariate matching, distance here is straightforward because of the dimension reduction afforded by the propensity score. We then average actual outcome, and match that average outcome to each treatment unit. Once we have that, we subtract each unit’s matched control from its treatment value, and then divide by <span class="math inline">\(N_T\)</span>, the number of treatment units. When we do that in Stata, we get an ATT of $1,725 with <span class="math inline">\(p&lt;0.05\)</span>. Thus, it is both relatively precise and similar to what we find with the experiment itself.</p>
</div>
<div id="coarsened-exact-matching" class="section level3" number="5.3.7">
<h3>
<span class="header-section-number">5.3.7</span> Coarsened exact matching<a class="anchor" aria-label="anchor" href="#coarsened-exact-matching"><i class="fas fa-link"></i></a>
</h3>
<p>There are two kinds of matching we’ve reviewed so far. Exact matching matches a treated unit to all of the control units with the same covariate value. But sometimes this is impossible, and therefore there are matching discrepancies. For instance, say that we are matching continuous age and continuous income. The probability we find another person with the exact same value of both is very small, if not zero. This leads therefore to mismatching on the covariates, which introduces bias.</p>
<p>The second kind of matching we’ve discussed are approximate matching methods, which specify a metric to find control units that are “close” to the treated unit. This requires a distance metric, such as Euclidean, Mahalanobis, or the propensity score. All of these can be implemented in Stata or R.</p>
<p><span class="citation">Iacus, King, and Porro (<a href="references.html#ref-Iacus2011" role="doc-biblioref">2012</a>)</span> introduced a kind of exact matching called coarsened exact matching (CEM). The idea is very simple. It’s based on the notion that sometimes it’s possible to do exact matching once we coarsen the data enough. If we coarsen the data, meaning we create categorical variables (e.g., 0- to 10-year-olds, 11- to 20-year olds), then oftentimes we can find exact matches. Once we find those matches, we calculate weights on the basis of where a person fits in some strata, and those weights are used in a simple weighted regression.</p>
<p>First, we begin with covariates <span class="math inline">\(X\)</span> and make a copy called <span class="math inline">\(X*\)</span>. Next we coarsen <span class="math inline">\(X*\)</span> according to user-defined cutpoints or CEM’s automatic binning algorithm. For instance, schooling becomes less than high school, high school only, some college, college graduate, post college. Then we create one stratum per unique observation of <span class="math inline">\(X*\)</span> and place each observation in a stratum. Assign these strata to the original and uncoarsened data, <span class="math inline">\(X\)</span>, and drop any observation whose stratum doesn’t contain at least one treated and control unit. Then add weights for stratum size and analyze without matching.</p>
<p>But there are trade-offs. Larger bins mean more coarsening of the data, which results in fewer strata. Fewer strata result in more diverse observations within the same strata and thus higher covariate imbalance. CEM prunes both treatment and control group units, which changes the parameter of interest, but so long as you’re transparent about this and up front, readers may be willing to give you the benefit of the doubt.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;They also may not. The methods are easy. It’s convincing readers that’s hard.&lt;/p&gt;"><sup>94</sup></a> Just know, though, that you are not estimating the ATE or the ATT when you start trimming (just as you aren’t doing so when you trim propensity scores).</p>
<p>The key benefit of CEM is that it is part of a class of matching methods called monotonic imbalance bounding (MIB). MIB methods bound the maximum imbalance in some feature of the empirical distributions by an ex ante decision by the user. In CEM, this ex ante choice is the coarsening decision. By choosing the coarsening beforehand, users can control the amount of imbalance in the matching solution. It’s also very fast.</p>
<p>There are several ways of measuring imbalance, but here we focus on the <span class="math inline">\(L1(f,g)\)</span> measure, which is
<span class="math display">\[
L1(f,g)=\dfrac{1}{2} \sum_{l_1 \dots l_k}
   \Big|f_{l1 \dots l_k} - g_{l_1 \dots l_k}\Big|
\]</span>
where <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> record the relative frequencies for the treatment and control group units. Perfect global balance is indicated by <span class="math inline">\(L1=0\)</span>. Larger values indicate larger imbalance between the groups, with a maximum of <span class="math inline">\(L1=1\)</span>. Hence the “imbalance bounding” between 0 and 1.</p>
<p>Now let’s get to the fun part: estimation. We will use the same job-training data we’ve been working with for this estimation.</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/cem.do"><code>cem.do</code></a></em></p>
<div class="sourceCode" id="cb49"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb49-1"><a href="ch4.html#cb49-1" aria-hidden="true"></a><span class="kw">ssc</span> install cem</span>
<span id="cb49-2"><a href="ch4.html#cb49-2" aria-hidden="true"></a></span>
<span id="cb49-3"><a href="ch4.html#cb49-3" aria-hidden="true"></a>* Reload experimental <span class="fu">group</span> <span class="kw">data</span></span>
<span id="cb49-4"><a href="ch4.html#cb49-4" aria-hidden="true"></a><span class="kw">use</span> https:<span class="co">//github.com/scunning1975/mixtape/raw/master/nsw_mixtape.dta, clear</span></span>
<span id="cb49-5"><a href="ch4.html#cb49-5" aria-hidden="true"></a><span class="kw">drop</span> <span class="kw">if</span> treat==0</span>
<span id="cb49-6"><a href="ch4.html#cb49-6" aria-hidden="true"></a></span>
<span id="cb49-7"><a href="ch4.html#cb49-7" aria-hidden="true"></a>* Now <span class="kw">merge</span> <span class="kw">in</span> the CPS controls from footnote 2 <span class="kw">of</span> Table 2 (Dehejia and Wahba 2002)</span>
<span id="cb49-8"><a href="ch4.html#cb49-8" aria-hidden="true"></a><span class="kw">append</span> <span class="kw">using</span> https:<span class="co">//github.com/scunning1975/mixtape/raw/master/cps_mixtape.dta</span></span>
<span id="cb49-9"><a href="ch4.html#cb49-9" aria-hidden="true"></a><span class="kw">gen</span> agesq=age*age</span>
<span id="cb49-10"><a href="ch4.html#cb49-10" aria-hidden="true"></a><span class="kw">gen</span> agecube=age*age*age</span>
<span id="cb49-11"><a href="ch4.html#cb49-11" aria-hidden="true"></a><span class="kw">gen</span> edusq=educ*edu</span>
<span id="cb49-12"><a href="ch4.html#cb49-12" aria-hidden="true"></a><span class="kw">gen</span> u74 = 0 <span class="kw">if</span> re74!=.</span>
<span id="cb49-13"><a href="ch4.html#cb49-13" aria-hidden="true"></a><span class="kw">replace</span> u74 = 1 <span class="kw">if</span> re74==0</span>
<span id="cb49-14"><a href="ch4.html#cb49-14" aria-hidden="true"></a><span class="kw">gen</span> u75 = 0 <span class="kw">if</span> re75!=.</span>
<span id="cb49-15"><a href="ch4.html#cb49-15" aria-hidden="true"></a><span class="kw">replace</span> u75 = 1 <span class="kw">if</span> re75==0</span>
<span id="cb49-16"><a href="ch4.html#cb49-16" aria-hidden="true"></a><span class="kw">gen</span> interaction1 = educ*re74</span>
<span id="cb49-17"><a href="ch4.html#cb49-17" aria-hidden="true"></a><span class="kw">gen</span> re74sq=re74^2</span>
<span id="cb49-18"><a href="ch4.html#cb49-18" aria-hidden="true"></a><span class="kw">gen</span> re75sq=re75^2</span>
<span id="cb49-19"><a href="ch4.html#cb49-19" aria-hidden="true"></a><span class="kw">gen</span> interaction2 = u74*hisp</span>
<span id="cb49-20"><a href="ch4.html#cb49-20" aria-hidden="true"></a></span>
<span id="cb49-21"><a href="ch4.html#cb49-21" aria-hidden="true"></a>cem age (10 20 30 40 60) age agesq agecube educ edusq marr nodegree <span class="bn">black</span> hisp re74 re75 u74 u75 interaction1, treatment(treat) </span>
<span id="cb49-22"><a href="ch4.html#cb49-22" aria-hidden="true"></a><span class="kw">reg</span> re78 treat [<span class="kw">iweight</span>=cem_weights], <span class="kw">robust</span></span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/cem.R"><code>cem.R</code></a></em></p>
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">cem</span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">MatchIt</span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">Zelig</span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">estimatr</span><span class="op">)</span>


<span class="va">m_out</span> <span class="op">&lt;-</span> <span class="fu">matchit</span><span class="op">(</span><span class="va">treat</span> <span class="op">~</span> <span class="va">age</span> <span class="op">+</span> <span class="va">agesq</span> <span class="op">+</span> <span class="va">agecube</span> <span class="op">+</span> <span class="va">educ</span> <span class="op">+</span>
                   <span class="va">educsq</span> <span class="op">+</span> <span class="va">marr</span> <span class="op">+</span> <span class="va">nodegree</span> <span class="op">+</span>
                   <span class="va">black</span> <span class="op">+</span> <span class="va">hisp</span> <span class="op">+</span> <span class="va">re74</span> <span class="op">+</span> <span class="va">re75</span> <span class="op">+</span> 
                   <span class="va">u74</span> <span class="op">+</span> <span class="va">u75</span> <span class="op">+</span> <span class="va">interaction1</span>,
                 data <span class="op">=</span> <span class="va">nsw_dw_cpscontrol</span>, 
                 method <span class="op">=</span> <span class="st">"cem"</span>, 
                 distance <span class="op">=</span> <span class="st">"logit"</span><span class="op">)</span>

<span class="va">m_data</span> <span class="op">&lt;-</span> <span class="fu">match.data</span><span class="op">(</span><span class="va">m_out</span><span class="op">)</span>

<span class="va">m_ate</span> <span class="op">&lt;-</span> <span class="fu">lm_robust</span><span class="op">(</span><span class="va">re78</span> <span class="op">~</span> <span class="va">treat</span>, 
               data <span class="op">=</span> <span class="va">m_data</span>,
               weights <span class="op">=</span> <span class="va">m_data</span><span class="op">$</span><span class="va">weights</span><span class="op">)</span></code></pre></div>
<p>The estimated ATE is $2,152, which is larger than our estimated experimental effect. But this ensured a high degree of balance on the covariates, as can be seen from the output from the <code>cem</code> command itself.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:cem">Table 5.17: </span> Balance in covariates after coarsened exact matching.</caption>
<thead><tr class="header">
<th align="left">Covariate</th>
<th align="center">L1</th>
<th align="center">Mean</th>
<th align="center">Min.</th>
<th align="center">25%</th>
<th align="center">50%</th>
<th align="left">75%</th>
<th align="center">Max.</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">age</td>
<td align="center">.08918</td>
<td align="center">.55337</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">0</td>
<td align="left">1</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="left">agesq</td>
<td align="center">.1155</td>
<td align="center">21.351</td>
<td align="center">33</td>
<td align="center">35</td>
<td align="center">0</td>
<td align="left">49</td>
<td align="center">0</td>
</tr>
<tr class="odd">
<td align="left">agecube</td>
<td align="center">.05263</td>
<td align="center">626.9</td>
<td align="center">817</td>
<td align="center">919</td>
<td align="center">0</td>
<td align="left">1801</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="left">school</td>
<td align="center">6.0e-16</td>
<td align="center"><span class="math inline">\(\rm -2.3e-14\)</span></td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="left">0</td>
<td align="center">0</td>
</tr>
<tr class="odd">
<td align="left">schoolsq</td>
<td align="center">5.4e-16</td>
<td align="center"><span class="math inline">\(\rm -2.8e-13\)</span></td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="left">0</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="left">married</td>
<td align="center">1.1e-16</td>
<td align="center"><span class="math inline">\(\rm -1.1e-16\)</span></td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="left">0</td>
<td align="center">0</td>
</tr>
<tr class="odd">
<td align="left">nodegree</td>
<td align="center">4.7e-16</td>
<td align="center"><span class="math inline">\(\rm -3.3e-16\)</span></td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="left">0</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="left">black</td>
<td align="center">4.7e-16</td>
<td align="center"><span class="math inline">\(\rm -8.9e-16\)</span></td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="left">0</td>
<td align="center">0</td>
</tr>
<tr class="odd">
<td align="left">hispanic</td>
<td align="center">7.1e-17</td>
<td align="center"><span class="math inline">\(-3.1e-17\)</span></td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="left">0</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="left">re74</td>
<td align="center">.06096</td>
<td align="center">42.399</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="left">0</td>
<td align="center"><span class="math inline">\(-94.801\)</span></td>
</tr>
<tr class="odd">
<td align="left">re75</td>
<td align="center">.03756</td>
<td align="center"><span class="math inline">\(-73.999\)</span></td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="left"><span class="math inline">\(-222.85\)</span></td>
<td align="center"><span class="math inline">\(-545.65\)</span></td>
</tr>
<tr class="even">
<td align="left">u74</td>
<td align="center">1.9e-16</td>
<td align="center"><span class="math inline">\(\rm -2.2e-16\)</span></td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="left">0</td>
<td align="center">0</td>
</tr>
<tr class="odd">
<td align="left">u75</td>
<td align="center">2.5e-16</td>
<td align="center"><span class="math inline">\(\rm -1.1e-16\)</span></td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="left">0</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="left">interaction1</td>
<td align="center">.06535</td>
<td align="center">425.68</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="left">0</td>
<td align="center"><span class="math inline">\(-853.21\)</span></td>
</tr>
</tbody>
</table></div>
<p>As can be seen from Table <a href="ch4.html#tab:cem">5.17</a>, the values of <span class="math inline">\(L1\)</span> are close to zero in most cases. The largest <span class="math inline">\(L1\)</span> gets is 0.12 for age squared.</p>
</div>
<div id="conclusion-3" class="section level3" number="5.3.8">
<h3>
<span class="header-section-number">5.3.8</span> Conclusion<a class="anchor" aria-label="anchor" href="#conclusion-3"><i class="fas fa-link"></i></a>
</h3>
<p>Matching methods are an important member of the causal inference arsenal. Propensity scores are an excellent tool to check the balance and overlap of covariates. It’s an under-appreciated diagnostic, and one that you might miss if you only ran regressions. There are extensions for more than two treatments, like multinomial models, but I don’t cover those here. The propensity score can make groups comparable, but only on the variables used to estimate the propensity score in the first place. It is an area that continues to advance to include covariate balancing <span class="citation">(Imai and Ratkovic <a href="references.html#ref-Imai2013" role="doc-biblioref">2013</a>; Zubizarreta <a href="references.html#ref-Zubizarreta2015" role="doc-biblioref">2015</a>; Zhao <a href="references.html#ref-Zhao2019" role="doc-biblioref">2019</a>)</span> and doubly robust estimators <span class="citation">(Band and Robins <a href="references.html#ref-Bang2005" role="doc-biblioref">2005</a>)</span>. Consider this chapter more about the mechanics of matching when you have exact and approximate matching situations.</p>
<p>Learning about the propensity score is particularly valuable given that it appears to have a very long half-life. For instance, propensity scores make their way into other contemporary designs too, such as difference-in-differences <span class="citation">(Sant’Anna and Zhao <a href="references.html#ref-Santanna2018" role="doc-biblioref">2018</a>)</span>. So investing in a basic understanding of these ideas and methods is likely worthwhile. You never know when the right project comes along for which these methods are the perfect solution, so there’s no intelligent reason to write them off.</p>
<p>But remember, every matching solution to a causality problem requires a credible belief that the backdoor criterion can be achieved by conditioning on some matrix <span class="math inline">\(X\)</span>, or what we’ve called CIA. This explicitly requires that there are no unobservable variables opening backdoor paths as confounders, which to many researchers requires a leap of faith so great they are unwilling to make it. In some respects, CIA is somewhat advanced because it requires deep institutional knowledge to say with confidence that no such unobserved confounder exists. The method is easy compared to such domain-specific knowledge. So if you have good reason to believe that there are important, unobservable variables, you will need another tool. But if you are willing to make such an assumption, then these methods and others could be useful for you in your projects.</p>
<div class="cover-box">
<div class="row">
    <div class="col-xs-8 col-md-4 cover-img">
        <a href="https://www.amazon.com/dp/0300251688"><img src="../images/cover.jpg" alt="Buy Today!"></a>
    </div>
    
    <div class="col-xs-12 col-md-8 cover-text-box">
            <h2> 
                Causal Inference: 
                <br><span style="font-style: italic; font-weight:bold; font-size: 20px;">The Mixtape.</span>
            </h2> 
            
        <div class="cover-text">
            <p>Buy the print version today:</p>
            
            <div class="chips">
                <a href="https://www.amazon.com/dp/0300251688" class="app-chip"> 
                    <i class="fab fa-amazon" aria-hidden="true"></i> Buy from Amazon 
                </a>
    
                <a href="https://yalebooks.yale.edu/book/9780300251685/causal-inference" class="app-chip"> 
                    <i class="fas fa-book" aria-hidden="true"></i> Buy from Yale Press 
                </a>
            </div>
        </div>
    </div>
</div>
</div>

</div>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="ch3.html"><span class="header-section-number">4</span> Potential Outcomes Causal Model</a></div>
<div class="next"><a href="ch5.html"><span class="header-section-number">6</span> Regression Discontinuity</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#ch4"><span class="header-section-number">5</span> Matching and Subclassification</a></li>
<li>
<a class="nav-link" href="#subclassification"><span class="header-section-number">5.1</span> Subclassification</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#some-background"><span class="header-section-number">5.1.1</span> Some background</a></li>
<li><a class="nav-link" href="#identifying-assumptions"><span class="header-section-number">5.1.2</span> Identifying assumptions</a></li>
<li><a class="nav-link" href="#subclassification-exercise-titanic-mathrmdata-set"><span class="header-section-number">5.1.3</span> Subclassification exercise: Titanic \(\mathrm{data\ set}\)</a></li>
<li><a class="nav-link" href="#curse-of-dimensionality"><span class="header-section-number">5.1.4</span> Curse of dimensionality</a></li>
</ul>
</li>
<li><a class="nav-link" href="#exact-matching"><span class="header-section-number">5.2</span> Exact Matching</a></li>
<li>
<a class="nav-link" href="#approximate-matching"><span class="header-section-number">5.3</span> Approximate Matching</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#nearest-neighbor-covariate-matching"><span class="header-section-number">5.3.1</span> Nearest neighbor covariate matching</a></li>
<li><a class="nav-link" href="#bias-correction"><span class="header-section-number">5.3.2</span> Bias correction</a></li>
<li><a class="nav-link" href="#propensity-score-methods"><span class="header-section-number">5.3.3</span> Propensity score methods</a></li>
<li><a class="nav-link" href="#example-the-nsw-job-training-program"><span class="header-section-number">5.3.4</span> Example: The NSW job training program</a></li>
<li><a class="nav-link" href="#weighting-on-the-propensity-score"><span class="header-section-number">5.3.5</span> Weighting on the propensity score</a></li>
<li><a class="nav-link" href="#nearest-neighbor-matching"><span class="header-section-number">5.3.6</span> Nearest-neighbor matching</a></li>
<li><a class="nav-link" href="#coarsened-exact-matching"><span class="header-section-number">5.3.7</span> Coarsened exact matching</a></li>
<li><a class="nav-link" href="#conclusion-3"><span class="header-section-number">5.3.8</span> Conclusion</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/scunning1975/mixtape/blob/master/04-Matching_and_Subclassification.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/scunning1975/mixtape/edit/master/04-Matching_and_Subclassification.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong><span style="font-weight:bold">Causal Inference</span></strong>: <i>The Mixtape</i>" was written by Scott Cunningham. It was last built on 2020-12-19.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
