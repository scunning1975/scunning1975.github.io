<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>6 Regression Discontinuity | Causal Inference</title>
<meta name="author" content="Scott Cunningham">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.6/header-attrs.js"></script><script src="libs/jquery-3.5.1/jquery-3.5.1.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.5.3/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.5.3/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.3.9000/tabs.js"></script><script src="libs/bs3compat-0.2.3.9000/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><style>
    @import url('https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,400;0,700;1,400;1,700&family=Roboto:ital,wght@0,700;1,300&display=swap');
    </style>
<script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS --><link rel="stylesheet" href="css/style.css">
<link rel="stylesheet" href="css/toc.css">
<link rel="stylesheet" href="css/causal_inference_style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="&lt;i&gt;The Mixtape&lt;/i&gt;"><span style="font-weight:bold">Causal Inference</span></a>:
        <small class="text-muted"><i>The Mixtape</i></small>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="ch1.html"><span class="header-section-number">2</span> Probability and Regression Review</a></li>
<li><a class="" href="ch2.html"><span class="header-section-number">3</span> Directed Acyclic Graphs</a></li>
<li><a class="" href="ch3.html"><span class="header-section-number">4</span> Potential Outcomes Causal Model</a></li>
<li><a class="" href="ch4.html"><span class="header-section-number">5</span> Matching and Subclassification</a></li>
<li><a class="active" href="ch5.html"><span class="header-section-number">6</span> Regression Discontinuity</a></li>
<li><a class="" href="ch6.html"><span class="header-section-number">7</span> Instrumental Variables</a></li>
<li><a class="" href="ch7.html"><span class="header-section-number">8</span> Panel Data</a></li>
<li><a class="" href="ch8.html"><span class="header-section-number">9</span> Difference-in-Differences</a></li>
<li><a class="" href="ch9.html"><span class="header-section-number">10</span> Synthetic Control</a></li>
<li><a class="" href="ch10.html"><span class="header-section-number">11</span> Conclusion</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/scunning1975/mixtape">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="ch5" class="section level1" number="6">
<h1>
<span class="header-section-number">6</span> Regression Discontinuity<a class="anchor" aria-label="anchor" href="#ch5"><i class="fas fa-link"></i></a>
</h1>
<div class="cover-box">
<div class="row">
    <div class="col-xs-8 col-md-4 cover-img">
        <a href="https://www.amazon.com/dp/0300251688"><img src="images/cover.jpg" alt="Buy Today!"></a>
    </div>
    
    <div class="col-xs-12 col-md-8 cover-text-box">
            <h2> 
                Causal Inference: 
                <br><span style="font-style: italic; font-weight:bold; font-size: 20px;">The Mixtape.</span>
            </h2> 
            
        <div class="cover-text">
            <p>Buy the print version today:</p>
            
            <div class="chips">
                <a href="https://www.amazon.com/dp/0300251688" class="app-chip"> 
                    <i class="fab fa-amazon" aria-hidden="true"></i> Buy from Amazon 
                </a>
    
                <a href="https://yalebooks.yale.edu/book/9780300251685/causal-inference" class="app-chip"> 
                    <i class="fas fa-book" aria-hidden="true"></i> Buy from Yale Press 
                </a>
            </div>
        </div>
    </div>
</div>
</div>
<div id="huge-popularity-of-regression-discontinuity" class="section level2" number="6.1">
<h2>
<span class="header-section-number">6.1</span> Huge Popularity of Regression Discontinuity<a class="anchor" aria-label="anchor" href="#huge-popularity-of-regression-discontinuity"><i class="fas fa-link"></i></a>
</h2>
<div id="waiting-for-life" class="section level3" number="6.1.1">
<h3>
<span class="header-section-number">6.1.1</span> Waiting for life<a class="anchor" aria-label="anchor" href="#waiting-for-life"><i class="fas fa-link"></i></a>
</h3>
<p>Over the past twenty years, interest in the <em>regression-discontinuity design</em> (RDD) has increased (Figure <a href="ch5.html#fig:RDD-overtime">6.1</a>). It was not always so popular, though. The method dates back about sixty years to Donald Campbell, an educational psychologist, who wrote several studies using it, beginning with <span class="citation">Thistlehwaite and Campbell (<a href="references.html#ref-Thistlehwaite1960" role="doc-biblioref">1960</a>)</span>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;&lt;span class="citation"&gt;Thistlehwaite and Campbell (&lt;a href="references.html#ref-Thistlehwaite1960" role="doc-biblioref"&gt;1960&lt;/a&gt;)&lt;/span&gt; studied the effect of merit awards on future academic outcomes. Merit awards were given out to students based on a score, and anyone with a score above some cutoff received the merit award, whereas everyone below that cutoff did not. Knowing the treatment assignment allowed the authors to carefully estimate the causal effect of merit awards on future academic performance.&lt;/p&gt;'><sup>95</sup></a> In a wonderful article on the history of thought around RDD, <span class="citation">Cook (<a href="references.html#ref-Cook2008" role="doc-biblioref">2008</a>)</span> documents its social evolution. Despite Campbell’s many efforts to advocate for its usefulness and understand its properties, RDD did not catch on beyond a few doctoral students and a handful of papers here and there. Eventually, Campbell too moved on from it.</p>
<p>To see its growing popularity, let’s look at counts of papers from Google Scholar by year that mentioned the phrase “regression discontinuity design” (see Figure <a href="ch5.html#fig:RDD-overtime">6.1</a>).<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Hat tip to John Holbein for giving me these data.&lt;/p&gt;"><sup>96</sup></a> <span class="citation">Thistlehwaite and Campbell (<a href="references.html#ref-Thistlehwaite1960" role="doc-biblioref">1960</a>)</span> had no influence on the broader community of scholars using his design, confirming what <span class="citation">Cook (<a href="references.html#ref-Cook2008" role="doc-biblioref">2008</a>)</span> wrote. The first time RDD appears in the economics community is with an unpublished econometrics paper <span class="citation">(Goldberger <a href="references.html#ref-Goldberger1972" role="doc-biblioref">1972</a>)</span>. Starting in 1976, RDD finally gets annual double-digit usage for the first time, after which it begins to slowly tick upward. But for the most part, adoption was imperceptibly slow.</p>
<div class="figure">
<span id="fig:RDD-overtime"></span>
<img src="graphics/RDD_overtime.jpg" alt="Regression discontinuity over time" width="100%"><p class="caption">
Figure 6.1: Regression discontinuity over time
</p>
</div>
<p>But then things change starting in 1999. That’s the year when a couple of notable papers in the prestigious <em>Quarterly Journal of Economics</em> resurrected the method. These papers were <span class="citation">Angrist and Lavy (<a href="references.html#ref-Angrist1999b" role="doc-biblioref">1999</a>)</span> and <span class="citation">Black (<a href="references.html#ref-Black1999" role="doc-biblioref">1999</a>)</span>, followed by <span class="citation">Hahn, Todd, and Klaauw (<a href="references.html#ref-Hahn2001" role="doc-biblioref">2001</a>)</span> two years later. <span class="citation">Angrist and Lavy (<a href="references.html#ref-Angrist1999b" role="doc-biblioref">1999</a>)</span>, which we discuss in detail later, studied the effect of class size on pupil achievement using an unusual feature in Israeli public schools that created smaller classes when the number of students passed a particular threshold. <span class="citation">Black (<a href="references.html#ref-Black1999" role="doc-biblioref">1999</a>)</span> used a kind of RDD approach when she creatively exploited discontinuities at the geographical level created by school district zoning to estimate people’s willingness to pay for better schools. The year 1999 marks a watershed in the design’s widespread adoption. A 2010 <em>Journal of Economic Literature</em> article by Lee and Lemieux, which has nearly 4,000 cites shows up in a year with nearly 1,500 new papers mentioning the method. By 2019, RDD output would be over 5,600. The design is today incredibly popular and shows no sign of slowing down.</p>
<p>But 1972 to 1999 is a long time without so much as a peep for what is now considered one of the most credible research designs with observational data, so what gives? <span class="citation">Cook (<a href="references.html#ref-Cook2008" role="doc-biblioref">2008</a>)</span> says that RDD was “waiting for life” during this time. The conditions for life in empirical microeconomics were likely the growing acceptance of the potential outcomes framework among microeconomists (i.e., the so-called credibility revolution led by Josh Angrist, David Card, Alan Krueger, Steven Levitt, and many others) as well as, and perhaps even more importantly, the increased availability of large digitized the administrative data sets, many of which often captured unusual administrative rules for treatment assignments. These unusual rules, combined with the administrative data sets’ massive size, provided the much-needed necessary conditions for Campbell’s original design to bloom into thousands of flowers.</p>
</div>
<div id="graphical-representation-of-rdd" class="section level3" number="6.1.2">
<h3>
<span class="header-section-number">6.1.2</span> Graphical representation of RDD<a class="anchor" aria-label="anchor" href="#graphical-representation-of-rdd"><i class="fas fa-link"></i></a>
</h3>
<p>So what’s the big deal? Why is RDD so special? The reason RDD is so appealing to many is because of its ability to convincingly eliminate selection bias. This appeal is partly due to the fact that its underlying identifying assumptions are viewed by many as easier to accept and evaluate. Rendering selection bias impotent, the procedure is capable of recovering average treatment effects for a given subpopulation of units. The method is based on a simple, intuitive idea. Consider the following DAG developed by <span class="citation">Steiner et al. (<a href="references.html#ref-Steiner2017" role="doc-biblioref">2017</a>)</span> that illustrates this method very well.</p>
<div class="inline-figure"><img src="causal_inference_mixtape_files/figure-html/unnamed-chunk-73-1.png" width="75%" style="display: block; margin: auto;"></div>
<p>In the first graph, <span class="math inline">\(X\)</span> is a continuous variable assigning units to treatment <span class="math inline">\(D\)</span> (<span class="math inline">\(X \rightarrow D\)</span>). This assignment of units to treatment is based on a “cutoff” score <span class="math inline">\(c_0\)</span> such that any unit with a score above the cutoff gets placed into the treatment group, and units below do not. An example might be a charge of driving while intoxicated (or impaired; DWI). Individuals with a blood-alcohol content of 0.08 or higher are arrested and charged with DWI, whereas those with a blood-alcohol level below 0.08 are not <span class="citation">(Hansen <a href="references.html#ref-Hansen2015" role="doc-biblioref">2015</a>)</span>. The assignment variable may itself independently affect the outcome via the <span class="math inline">\(X \rightarrow Y\)</span> path and may even be related to a set of variables <span class="math inline">\(U\)</span> that independently determine <span class="math inline">\(Y\)</span>. Notice for the moment that a unit’s treatment status is <em>exclusively</em> determined by the assignment rule. Treatment is not determined by <span class="math inline">\(U\)</span>.</p>
<p>This DAG clearly shows that the assignment variable <span class="math inline">\(X\)</span>—or what is often called the “running variable”—is an observable confounder since it causes both <span class="math inline">\(D\)</span> and <span class="math inline">\(Y\)</span>. Furthermore, because the assignment variable assigns treatment on the basis of a cutoff, we are never able to observe units in both treatment and control for the same value of <span class="math inline">\(X\)</span>. Calling back to our matching chapter, this means a situation such as this one does not satisfy the overlap condition needed to use matching methods, and therefore the backdoor criterion cannot be met.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Think about it for a moment. The backdoor criterion calculates differences in expected outcomes between treatment and control &lt;em&gt;for a given value of &lt;span class="math inline"&gt;\(X\)&lt;/span&gt;&lt;/em&gt;. But if the assignment variable only moves units into treatment when &lt;span class="math inline"&gt;\(X\)&lt;/span&gt; passes some cutoff, then such calculations are impossible because there will not be units in treatment and control for any given value of &lt;span class="math inline"&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;'><sup>97</sup></a></p>
<p>However, we can identify causal effects using RDD, which is illustrated in the limiting graph DAG. We can identify causal effects for those subjects whose score is in a close neighborhood around some cutoff <span class="math inline">\(c_0\)</span>. Specifically, as we will show, the average causal effect for this subpopulation is identified as <span class="math inline">\(X \rightarrow c_0\)</span> in the limit. This is possible because the cutoff is the sole point where treatment and control subjects overlap in the limit.</p>
<p>There are a variety of explicit assumptions buried in this graph that must hold in order for the methods we will review later to recover any average causal effect. But the main one I discuss here is that the cutoff itself cannot be endogenous to some competing intervention occurring at precisely the same moment that the cutoff is triggering units into the <span class="math inline">\(D\)</span> treatment category. This assumption is called <em>continuity</em>, and what it formally means is that the expected potential outcomes are continuous at the cutoff. If expected potential outcomes are continuous at the cutoff, then it necessarily rules out competing interventions occurring at the same time.</p>
<p>The continuity assumption is reflected graphically by the absence of an arrow from <span class="math inline">\(X \rightarrow Y\)</span> in the second graph because the cutoff <span class="math inline">\(c_0\)</span> has cut it off. At <span class="math inline">\(c_0\)</span>, the assignment variable <span class="math inline">\(X\)</span> no longer has a direct effect on <span class="math inline">\(Y\)</span>. Understanding continuity should be one of your main goals in this chapter. It is my personal opinion that the null hypothesis should always be continuity and that any discontinuity necessarily implies some cause, because the tendency for things to change gradually is what we have come to expect in nature. Jumps are so unnatural that when we see them happen, they beg for explanation. Charles Darwin, in his <em>On the Origin of Species</em>, summarized this by saying <em>Natura non facit saltum</em>, or “nature does not make jumps.” Or to use a favorite phrase of mine from growing up in Mississippi, if you see a turtle on a fencepost, you know he didn’t get there by himself.</p>
<p>That’s the heart and soul of RDD. We use our knowledge about selection into treatment in order to estimate average treatment effects. Since we know the probability of treatment assignment changes discontinuously at <span class="math inline">\(c_0\)</span>, then our job is simply to compare people above and below <span class="math inline">\(c_0\)</span> to estimate a particular kind of average treatment effect called the <em>local average treatment effect</em>, or LATE <span class="citation">(Imbens and Angrist <a href="references.html#ref-Imbens1994" role="doc-biblioref">1994</a>)</span>. Because we do not have overlap, or “common support,” we must rely on extrapolation, which means we are comparing units with different values of the running variable. They only overlap in the limit as <span class="math inline">\(X\)</span> approaches the cutoff from either direction. All methods used for RDD are ways of handling the bias from extrapolation as cleanly as possible.</p>
</div>
<div id="a-picture-is-worth-a-thousand-words" class="section level3" number="6.1.3">
<h3>
<span class="header-section-number">6.1.3</span> A picture is worth a thousand words<a class="anchor" aria-label="anchor" href="#a-picture-is-worth-a-thousand-words"><i class="fas fa-link"></i></a>
</h3>
<p>As I’ve said before, and will say again and again—pictures of your main results, including your identification strategy, are absolutely essential to any study attempting to convince readers of a causal effect. And RDD is no different. In fact, pictures are the comparative advantage of RDD. RDD is, like many modern designs, a very visually intensive design. It and synthetic control are probably two of the most visually intensive designs you’ll ever encounter, in fact. So to help make RDD concrete, let’s first look at a couple of pictures. The following discussion derives from <span class="citation">Hoekstra (<a href="references.html#ref-Hoekstra2009" role="doc-biblioref">2009</a>)</span>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Mark Hoekstra is one of the more creative microeconomists I have met when it comes to devising compelling strategies for identifying causal effects in observational data, and this is one of my favorite papers by him.&lt;/p&gt;"><sup>98</sup></a></p>
<p>Labor economists had for decades been interested in estimating the causal effect of college on earnings. But Hoekstra wanted to crack open the black box of college’s returns a little by checking whether there were heterogeneous returns to college. He does this by estimating the causal effect of attending the state flagship university on earnings. State flagship universities are often more selective than other public universities in the same state. In Texas, the top 7% of graduating high school students can select their university in state, and the modal first choice is University of Texas at Austin. These universities are often environments of higher research, with more resources and strongly positive peer effects. So it is natural to wonder whether there are heterogeneous returns across public universities.</p>
<p>The challenge in this type of question should be easy to see. Let’s say that we were to compare individuals who attended the University of Florida to those who attended the University of South Florida. Insofar as there is positive selection into the state flagship school, we might expect individuals with higher observed and unobserved ability to sort into the state flagship school. And insofar as that ability increases one’s marginal product, then we expect those individuals to earn more in the workforce regardless of whether they had in fact attended the state flagship. Such basic forms of selection bias confound our ability to estimate the causal effect of attending the state flagship on earnings. But <span class="citation">Hoekstra (<a href="references.html#ref-Hoekstra2009" role="doc-biblioref">2009</a>)</span> had an ingenious strategy to disentangle the causal effect from the selection bias using an RDD. To illustrate, let’s look at two pictures associated with this interesting study.</p>
<p>Before talking about the picture, I want to say something about the data. Hoekstra has data on all applications to the state flagship university. To get these data, he would’ve had to build a relationship with the admissions office. This would have involved making introductions, holding meetings to explain his project, convincing administrators the project had value for them as well as him, and ultimately winning their approval to cooperatively share the data. This likely would’ve involved the school’s general counsel, careful plans to de-identify the data, agreements on data storage, and many other assurances that students’ names and identities were never released and could not be identified. There is a lot of trust and social capital that must be created to do projects like this, and this is the secret sauce in most RDDs—your acquisition of the data requires far more soft skills, such as friendship, respect, and the building of alliances, than you may be accustomed to. This isn’t as straightforward as simply downloading the CPS from IPUMS; it’s going to take genuine smiles, hustle, and luck. Given that these agencies have considerable discretion in whom they release data to, it is likely that certain groups will have more trouble than others in acquiring the data. So it is of utmost importance that you approach these individuals with humility, genuine curiosity, and most of all, scientific integrity. They ultimately are the ones who can give you the data if it is not public use, so don’t be a jerk.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;“Don’t be a jerk” applies even to situations when you aren’t seeking proprietary data.&lt;/p&gt;"><sup>99</sup></a></p>
<div class="figure">
<span id="fig:hoekstra2009a"></span>
<img src="graphics/rdd_hoekstra1.jpg" alt="Attending the state flagship university as a function of re-centered standardized test scores. Reprinted from @Hoekstra2009." width="100%"><p class="caption">
Figure 6.2: Attending the state flagship university as a function of re-centered standardized test scores. Reprinted from <span class="citation">Hoekstra (<a href="references.html#ref-Hoekstra2009" role="doc-biblioref">2009</a>)</span>.
</p>
</div>
<p>But on to the picture. Figure <a href="ch5.html#fig:hoekstra2009a">6.2</a> has a lot going on, and it’s worth carefully unpacking each element for the reader. There are four distinct elements to this picture that I want to focus on. First, notice the horizontal axis. It ranges from a negative number to a positive number with a zero around the center of the picture. The caption reads “SAT Points Above or Below the Admission Cutoff.” Hoekstra has “recentered” the university’s admissions criteria by subtracting the admission cutoff from the students’ actual score, which is something I discuss in more detail later in this chapter. The vertical line at zero marks the “cutoff,” which was this university’s minimum SAT score for admissions. It appears it was binding, but not deterministically, for there are some students who enrolled but did not have the minimum SAT requirements. These individuals likely had other qualifications that compensated for their lower SAT scores. This recentered SAT score is in today’s parlance called the “running variable.”</p>
<p>Second, notice the dots. Hoekstra used hollow dots at regular intervals along the recentered SAT variable. These dots represent conditional mean enrollments per recentered SAT score. While his administrative data set contains thousands and thousands of observations, he only shows the conditional means along evenly spaced out bins of the recentered SAT score.</p>
<p>Third are the curvy lines fitting the data. Notice that the picture has <em>two</em> such lines—there is a curvy line fitted to the left of zero, and there is a separate line fit to the right. These lines are the least squares fitted values of the running variable, where the running variable was allowed to take on higher-order terms. By including higher-order terms in the regression itself, the fitted values are allowed to more flexibly track the central tendencies of the data itself. But the thing I really want to focus your attention on is that there are two lines, not one. He fit the lines separately to the left and right of the cutoff.</p>
<p>Finally, and probably the most vivid piece of information in this picture—the gigantic jump in the dots at zero on the recentered running variable. What is going on here? Well, I think you probably know, but let me spell it out. The probability of enrolling at the flagship state university jumps discontinuously when the student just barely hits the minimum SAT score required by the school. Let’s say that the score was 1250. That means a student with 1240 had a lower chance of getting in than a student with 1250. Ten measly points and they have to go a different path.</p>
<p>Imagine two students—the first student got a 1240, and the second got a 1250. Are these two students really so different from one another? Well, sure: those two <em>individual</em> students are likely very different. But what if we had hundreds of students who made 1240 and hundreds more who made 1250. Don’t you think those two groups are probably pretty similar to one another on observable and unobservable characteristics? After all, why would there be suddenly at 1250 a major difference in the characteristics of the students in a large sample? That’s the question you should reflect on. If the university is arbitrarily picking a reasonable cutoff, are there reasons to believe they are also picking a cutoff where the natural ability of students jumps at that exact spot?</p>
<p>But I said Hoekstra is evaluating the effect of attending the state flagship university on future earnings. Here’s where the study gets even more intriguing. States collect data on workers in a variety of ways. One is through unemployment insurance tax reports. Hoekstra’s partner, the state flagship university, sent the university admissions data directly to a state office in which employers submit unemployment insurance tax reports. The university had social security numbers, so the matching of student to future worker worked quite well since a social security number uniquely identifies a worker. The social security numbers were used to match quarterly earnings records from 1998 through the second quarter of 2005 to the university records. He then estimated:
<span class="math display">\[
\ln(\text{Earnings})=\psi_{\text{Year}} + \omega_{\text{Experience}} + \theta_{\text{Cohort}} + \varepsilon
\]</span>
where <span class="math inline">\(\psi\)</span> is a vector of year dummies, <span class="math inline">\(\omega\)</span> is a dummy for years after high school that earnings were observed, and <span class="math inline">\(\theta\)</span> is a vector of dummies controlling for the cohort in which the student applied to the university (e.g., 1988). The residuals from this regression were then averaged for each applicant, with the resulting average residual earnings measure being used to implement a partialled out future earnings variable according to the Frisch-Waugh-Lovell theorem. Hoekstra then takes each students’ residuals from the natural log of earnings regression and collapses them into conditional averages for bins along the recentered running variable. Let’s look at that in Figure <a href="ch5.html#fig:hoekstra2009b">6.3</a>.</p>
<div class="figure">
<span id="fig:hoekstra2009b"></span>
<img src="graphics/rdd_hoekstra2.jpg" alt="Future earnings as a function of re-centered standardized test scores. Reprinted from @Hoekstra2009." width="100%"><p class="caption">
Figure 6.3: Future earnings as a function of re-centered standardized test scores. Reprinted from <span class="citation">Hoekstra (<a href="references.html#ref-Hoekstra2009" role="doc-biblioref">2009</a>)</span>.
</p>
</div>
<p>In this picture, we see many of the same elements we saw in Figure <a href="ch5.html#fig:hoekstra2009a">6.2</a>. For instance, we see the recentered running variable along the horizontal axis, the little hollow dots representing conditional means, the curvy lines which were fit left and right of the cutoff at zero, and a helpful vertical line at zero. But now we also have an interesting title: “Estimated Discontinuity = 0.095 (z = 3.01).” What is this exactly?</p>
<p>The visualization of a discontinuous jump at zero in earnings isn’t as compelling as the prior figure, so Hoekstra conducts hypothesis tests to determine if the mean between the groups just below and just above are the same. He finds that they are not: those just above the cutoff earn 9.5% higher wages in the long term than do those just below. In his paper, he experiments with a variety of binning of the data (what he calls the “bandwidth”), and his estimates when he does so range from 7.4% to 11.1%.</p>
<p>Now let’s think for a second about what Hoekstra is finding. Hoekstra is finding that at exactly the point where workers experienced a jump in the probability of enrolling at the state flagship university, there is, ten to fifteen years later, a separate jump in logged earnings of around 10%. Those individuals who just barely made it in to the state flagship university made around 10% more in long-term earnings than those individuals who just barely missed the cutoff.</p>
<p>This, again, is the heart and soul of the RDD. By exploiting institutional knowledge about how students were accepted (and subsequently enrolled) into the state flagship university, Hoekstra was able to craft an ingenious natural experiment. And insofar as the two groups of applicants right around the cutoff have comparable future earnings in a world where neither attended the state flagship university, then there is no selection bias confounding his comparison. And we see this result in powerful, yet simple graphs. This study was an early one to show that not only does college matter for long-term earnings, but the sort of college you attend—even among public universities—matters as well.</p>
</div>
<div id="data-requirements-for-rdd" class="section level3" number="6.1.4">
<h3>
<span class="header-section-number">6.1.4</span> Data requirements for RDD<a class="anchor" aria-label="anchor" href="#data-requirements-for-rdd"><i class="fas fa-link"></i></a>
</h3>
<p>RDD is all about finding “jumps” in the probability of treatment as we move along some running variable <span class="math inline">\(X\)</span>. So where do we find these jumps? Where do we find these <em>discontinuities</em>? The answer is that humans often embed jumps into rules. And sometimes, if we are lucky, someone gives us the data that allows us to use these rules for our study.</p>
<p>I am convinced that firms and government agencies are unknowingly sitting atop a mountain of potential RDD-based projects. Students looking for thesis and dissertation ideas might try to find them. I encourage you to find a topic you are interested in and begin building relationships with local employers and government administrators for whom that topic is a priority. Take them out for coffee, get to know them, learn about their job, and ask them how treatment assignment works. Pay close attention to precisely how individual units get assigned to the program. Is it random? Is it via a rule? Oftentimes they will describe a process whereby a running variable is used for treatment assignment, but they won’t call it that. While I can’t promise this will yield pay dirt, my hunch, based in part on experience, is that they will end up describing to you some running variable that when it exceeds a threshold, people switch into some intervention. Building alliances with local firms and agencies can pay when trying to find good research ideas.</p>
<p>The validity of an RDD doesn’t require that the assignment rule be arbitrary. It only requires that it be known, precise and free of manipulation. The most effective RDD studies involve programs where <span class="math inline">\(X\)</span> has a “hair trigger” that is not tightly related to the outcome being studied. Examples include the probability of being arrested for DWI jumping at greater than 0.08 blood-alcohol content <span class="citation">(Hansen <a href="references.html#ref-Hansen2015" role="doc-biblioref">2015</a>)</span>; the probability of receiving health-care insurance jumping at age 65, <span class="citation">(Card, Dobkin, and Maestas <a href="references.html#ref-Card2008" role="doc-biblioref">2008</a>)</span>; the probability of receiving medical attention jumping when birthweight falls below 1,500 grams <span class="citation">(Almond et al. <a href="references.html#ref-Almond2010" role="doc-biblioref">2010</a>; Barreca et al. <a href="references.html#ref-Barreca2011" role="doc-biblioref">2011</a>)</span>; the probability of attending summer school when grades fall below some minimum level <span class="citation">(Jacob and Lefgen <a href="references.html#ref-Jacob2004" role="doc-biblioref">2004</a>)</span>, and as we just saw, the probability of attending the state flagship university jumping when the applicant’s test scores exceed some minimum requirement <span class="citation">(Hoekstra <a href="references.html#ref-Hoekstra2009" role="doc-biblioref">2009</a>)</span>.</p>
<p>In all these kinds of studies, we need data. But specifically, we need a lot of data <em>around</em> the discontinuities, which itself implies that the data sets useful for RDD are likely very large. In fact, large sample sizes are characteristic features of the RDD. This is also because in the face of strong trends in the running variable, sample-size requirements get even larger. Researchers are typically using administrative data or settings such as birth records where there are many observations.</p>
</div>
</div>
<div id="estimation-using-an-rdd" class="section level2" number="6.2">
<h2>
<span class="header-section-number">6.2</span> Estimation Using an RDD<a class="anchor" aria-label="anchor" href="#estimation-using-an-rdd"><i class="fas fa-link"></i></a>
</h2>
<div id="the-sharp-rd-design" class="section level3" number="6.2.1">
<h3>
<span class="header-section-number">6.2.1</span> The Sharp RD Design<a class="anchor" aria-label="anchor" href="#the-sharp-rd-design"><i class="fas fa-link"></i></a>
</h3>
<p>There are generally accepted two kinds of RDD studies. There are designs where the probability of treatment goes from 0 to 1 at the cutoff, or what is called a “sharp” design. And there are designs where the probability of treatment discontinuously increases at the cutoff. These are often called “fuzzy” designs. In all of these, though, there is some running variable <span class="math inline">\(X\)</span> that, upon reaching a cutoff <span class="math inline">\(c_0\)</span>, the likelihood of receiving some treatment flips. Let’s look at the diagram in Figure <a href="ch5.html#fig:klaauw2002">6.4</a>, which illustrates the similarities and differences between the two designs.</p>
<div class="figure">
<span id="fig:klaauw2002"></span>
<img src="graphics/klaauw2002.png" alt="Sharp vs. Fuzzy RDD" width="100%"><p class="caption">
Figure 6.4: Sharp vs. Fuzzy RDD
</p>
</div>
<p>Sharp RDD is where treatment is a deterministic function of the running variable <span class="math inline">\(X\)</span>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;&lt;span class="citation"&gt;Klaauw (&lt;a href="references.html#ref-klaauw2002" role="doc-biblioref"&gt;2002&lt;/a&gt;)&lt;/span&gt; called the running variable the “selection variable.” This is because &lt;span class="citation"&gt;Klaauw (&lt;a href="references.html#ref-klaauw2002" role="doc-biblioref"&gt;2002&lt;/a&gt;)&lt;/span&gt; is an early paper in the new literature, and the terminology hadn’t yet been hammered out. But here they mean the same thing.&lt;/p&gt;'><sup>100</sup></a> An example might be Medicare enrollment, which happens sharply at age 65, excluding disability situations. A fuzzy RDD represents a discontinuous “jump” in the probability of treatment when <span class="math inline">\(X&gt;c_0\)</span>. In these fuzzy designs, the cutoff is used as an instrumental variable for treatment, like <span class="citation">Angrist and Lavy (<a href="references.html#ref-Angrist1999b" role="doc-biblioref">1999</a>)</span>, who instrument for class size with a class-size function they created from the rules used by Israeli schools to construct class sizes.</p>
<p>More formally, in a sharp RDD, treatment status is a deterministic and discontinuous function of a running variable <span class="math inline">\(X_i\)</span>, where
<span class="math display">\[
D_i =
   \begin{cases} 1
       \text{ if } &amp; X_i\geq{c_0} 
       \\ 0
       \text{ if } &amp; X_i &lt; c_0    
   \end{cases}
\]</span>
where <span class="math inline">\(c_0\)</span> is a known threshold or cutoff. If you know the value of <span class="math inline">\(X_i\)</span> for unit <span class="math inline">\(i\)</span>, then you know treatment assignment for unit <span class="math inline">\(i\)</span> with certainty. But, if for every value of <span class="math inline">\(X\)</span> you can perfectly predict the treatment assignment, then it necessarily means that there are no overlap along the running variable.</p>
<p>If we assume constant treatment effects, then in potential outcomes terms, we get</p>
<p><span class="math display">\[\begin{align}
   Y_i^0 &amp; = \alpha + \beta X_i 
   \\
   Y^1_i &amp; = Y_i^0 + \delta     
\end{align}\]</span></p>
<p>Using the switching equation, we get</p>
<p><span class="math display">\[\begin{align}
   Y_i &amp; =Y_i^0 + (Y_i^1 - Y_i^0) D_i                   
   \\
   Y_i &amp; =\alpha+\beta X_i + \delta D_i + \varepsilon_i 
\end{align}\]</span></p>
<p>where the treatment effect parameter, <span class="math inline">\(\delta\)</span>, is the discontinuity in the conditional expectation function:</p>
<p><span class="math display">\[\begin{align}
   \delta &amp; =\lim_{X_i\rightarrow{X_0}} 
   E\big[Y^1_i\mid X_i=X_0\big] - \lim_{X_0\leftarrow{X_i}}
   E\big[Y^0_i\mid X_i=X_0\big]
   \\
          &amp; =\lim_{X_i\rightarrow{X_0}} 
   E\big[Y_i\mid X_i=X_0\big]- \lim_{X_0\leftarrow{X_i}} E\big[Y_i\mid X_i=X_0\big]
\end{align}\]</span></p>
<p>The sharp RDD estimation is interpreted as an average causal effect of the treatment as the running variable approaches the cutoff in the limit, for it is only in the limit that we have overlap. This average causal effect is the local average treatment effect (LATE). We discuss LATE in greater detail in the instrumental variables, but I will say one thing about it here. Since identification in an RDD is a limiting case, we are technically only identifying an average causal effect for those units at the cutoff. Insofar as those units have treatment effects that differ from units along the rest of the running variable, then we have only estimated an average treatment effect that is local to the range around the cutoff. We define this local average treatment effect as follows:
<span class="math display">\[
\delta_{SRD}=E\big[Y^1_i - Y_i^0\mid X_i=c_0]
\]</span>
Notice the role that <em>extrapolation</em> plays in estimating treatment effects with sharp RDD. If unit <span class="math inline">\(i\)</span> is just below <span class="math inline">\(c_0\)</span>, then <span class="math inline">\(D_i=0\)</span>. But if unit <span class="math inline">\(i\)</span> is just above <span class="math inline">\(c_0\)</span>, then the <span class="math inline">\(D_i=1\)</span>. But for any value of <span class="math inline">\(X_i\)</span>, there are either units in the treatment group or the control group, but not both. Therefore, the RDD does not have common support, which is one of the reasons we rely on extrapolation for our estimation. See Figure <a href="ch5.html#fig:sim1">6.5</a>.</p>
<div class="figure">
<span id="fig:sim1"></span>
<img src="graphics/rdd_simul_ex.jpg" alt="Simulated data representing observed data points along a running variable below and above some binding cutoff." width="100%"><p class="caption">
Figure 6.5: Simulated data representing observed data points along a running variable below and above some binding cutoff.
</p>
</div>
<p class="footnote">
<em>Notes:</em> Dashed lines are extrapolations
</p>
</div>
<div id="continuity-assumption" class="section level3" number="6.2.2">
<h3>
<span class="header-section-number">6.2.2</span> Continuity assumption<a class="anchor" aria-label="anchor" href="#continuity-assumption"><i class="fas fa-link"></i></a>
</h3>
<p>The key identifying assumption in an RDD is called the continuity assumption. It states that <span class="math inline">\(E[Y^0_i\mid X=c_0]\)</span> and <span class="math inline">\(E[Y_i^1\mid X=c_0]\)</span> are continuous (smooth) functions of <span class="math inline">\(X\)</span> even across the <span class="math inline">\(c_0\)</span> threshold. Absent the treatment, in other words, the expected potential outcomes wouldn’t have jumped; they would’ve remained smooth functions of <span class="math inline">\(X\)</span>. But think about what that means for a moment. If the expected potential outcomes are not jumping at <span class="math inline">\(c_0\)</span>, then there necessarily are no competing interventions occurring at <span class="math inline">\(c_0\)</span>. Continuity, in other words, explicitly rules out omitted variable bias at the cutoff itself. All other unobserved determinants of <span class="math inline">\(Y\)</span> are continuously related to the running variable <span class="math inline">\(X\)</span>. Does there exist some omitted variable wherein the outcome, would jump at <span class="math inline">\(c_0\)</span> <em>even if we disregarded the treatment altogether</em>? If so, then the continuity assumption is violated and our methods do not require the LATE.</p>
<p>I apologize if I’m beating a dead horse, but continuity is a subtle assumption and merits a little more discussion. The continuity assumption means that <span class="math inline">\(E[Y^1\mid X]\)</span> wouldn’t have jumped at <span class="math inline">\(c_0\)</span>. If it had jumped, then it means something other than the treatment caused it to jump because <span class="math inline">\(Y^1\)</span> is already under treatment. So an example might be a study finding a large increase in motor vehicle accidents at age 21. I’ve reproduced a figure from and interesting study on mortality rates for different types of causes <span class="citation">(Carpenter and Dobkin <a href="references.html#ref-Carpenter2009" role="doc-biblioref">2009</a>)</span>. I have reproduced one of the key figures in Figure <a href="ch5.html#fig:mva">6.6</a>. Notice the large discontinuous jump in motor vehicle death rates at age 21. The most likely explanation is that age 21 causes people to drink more, and sometimes even while they are driving.</p>
<div class="figure">
<span id="fig:mva"></span>
<img src="graphics/carpenter1.jpg" alt="Mortality rates along age running variable [@Carpenter2009]." width="100%"><p class="caption">
Figure 6.6: Mortality rates along age running variable <span class="citation">(Carpenter and Dobkin <a href="references.html#ref-Carpenter2009" role="doc-biblioref">2009</a>)</span>.
</p>
</div>
<p>But this is only a causal effect if motor vehicle accidents don’t jump at age 21 for other reasons. Formally, this is <em>exactly</em> what is implied by continuity—the absence of simultaneous treatments at the cutoff. For instance, perhaps there is something biological that happens to 21-year-olds that causes them to suddenly become bad drivers. Or maybe 21-year-olds are all graduating from college at age 21, and during celebrations, they get into wrecks. To test this, we might replicate <span class="citation">Carpenter and Dobkin (<a href="references.html#ref-Carpenter2009" role="doc-biblioref">2009</a>)</span> using data from Uruguay, where the drinking age is 18. If we saw a jump in motor vehicle accidents at age 21 in Uruguay, then we might have reason to believe the continuity assumption does not hold in the United States. Reasonably defined placebos can help make the case that the continuity assumption holds, even if it is not a direct test per se.</p>
<p>Sometimes these abstract ideas become much easier to understand with data, so here is an example of what we mean using a simulation.</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/rdd_simulate1.do"><code>rdd_simulate1.do</code></a></em></p>
<div class="sourceCode" id="cb51"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb51-1"><a href="ch5.html#cb51-1" aria-hidden="true"></a><span class="kw">clear</span></span>
<span id="cb51-2"><a href="ch5.html#cb51-2" aria-hidden="true"></a><span class="kw">capture</span> <span class="fu">log</span> <span class="kw">close</span></span>
<span id="cb51-3"><a href="ch5.html#cb51-3" aria-hidden="true"></a><span class="kw">set</span> <span class="kw">obs</span> 1000</span>
<span id="cb51-4"><a href="ch5.html#cb51-4" aria-hidden="true"></a><span class="kw">set</span> <span class="dv">seed</span> 1234567</span>
<span id="cb51-5"><a href="ch5.html#cb51-5" aria-hidden="true"></a></span>
<span id="cb51-6"><a href="ch5.html#cb51-6" aria-hidden="true"></a>* Generate running <span class="kw">variable</span></span>
<span id="cb51-7"><a href="ch5.html#cb51-7" aria-hidden="true"></a><span class="kw">gen</span> x = rnormal(50, 25)</span>
<span id="cb51-8"><a href="ch5.html#cb51-8" aria-hidden="true"></a><span class="kw">replace</span> x=0 <span class="kw">if</span> x &lt; 0</span>
<span id="cb51-9"><a href="ch5.html#cb51-9" aria-hidden="true"></a><span class="kw">drop</span> <span class="kw">if</span> x &gt; 100</span>
<span id="cb51-10"><a href="ch5.html#cb51-10" aria-hidden="true"></a><span class="kw">sum</span> x, <span class="fu">det</span></span>
<span id="cb51-11"><a href="ch5.html#cb51-11" aria-hidden="true"></a></span>
<span id="cb51-12"><a href="ch5.html#cb51-12" aria-hidden="true"></a>* Set the cutoff <span class="fu">at</span> X=50. Treated <span class="kw">if</span> X &gt; 50</span>
<span id="cb51-13"><a href="ch5.html#cb51-13" aria-hidden="true"></a><span class="kw">gen</span> D = 0</span>
<span id="cb51-14"><a href="ch5.html#cb51-14" aria-hidden="true"></a><span class="kw">replace</span> D = 1 <span class="kw">if</span> x &gt; 50</span>
<span id="cb51-15"><a href="ch5.html#cb51-15" aria-hidden="true"></a><span class="kw">gen</span> y1 = 25 + 0*D + 1.5*x + rnormal(0, 20)</span>
<span id="cb51-16"><a href="ch5.html#cb51-16" aria-hidden="true"></a></span>
<span id="cb51-17"><a href="ch5.html#cb51-17" aria-hidden="true"></a>* Potential outcome Y1 <span class="kw">not</span> jumping <span class="fu">at</span> cutoff (continuity)</span>
<span id="cb51-18"><a href="ch5.html#cb51-18" aria-hidden="true"></a><span class="kw">twoway</span> (<span class="kw">scatter</span> y1 x <span class="kw">if</span> D==0, msize(vsmall) <span class="bn">msymbol</span>(<span class="bn">circle_hollow</span>)) (<span class="kw">scatter</span> y1 x <span class="kw">if</span> D==1, <span class="kw">sort</span> mcolor(<span class="bn">blue</span>) msize(vsmall) <span class="bn">msymbol</span>(<span class="bn">circle_hollow</span>)) (<span class="kw">lfit</span> y1 x <span class="kw">if</span> D==0, lcolor(<span class="kw">red</span>) msize(small) lwidth(medthin) lpattern(solid)) (<span class="kw">lfit</span> y1 x, lcolor(<span class="bn">dknavy</span>) msize(small) lwidth(medthin) lpattern(solid)), <span class="bn">xtitle</span>(Test <span class="kw">score</span> (X)) <span class="bn">xline</span>(50) <span class="bn">legend</span>(<span class="kw">off</span>)</span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/rdd_simulate1.R"><code>rdd_simulate1.R</code></a></em></p>
<div class="sourceCode" id="cb52"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>

<span class="co"># simulate the data</span>
<span class="va">dat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble</a></span><span class="op">(</span>
  x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">1000</span>, <span class="fl">50</span>, <span class="fl">25</span><span class="op">)</span>
<span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>
    x <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/if_else.html">if_else</a></span><span class="op">(</span><span class="va">x</span> <span class="op">&lt;</span> <span class="fl">0</span>, <span class="fl">0</span>, <span class="va">x</span><span class="op">)</span>
  <span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">x</span> <span class="op">&lt;</span> <span class="fl">100</span><span class="op">)</span>

<span class="co"># cutoff at x = 50</span>
<span class="va">dat</span> <span class="op">&lt;-</span> <span class="va">dat</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>
    D  <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/if_else.html">if_else</a></span><span class="op">(</span><span class="va">x</span> <span class="op">&gt;</span> <span class="fl">50</span>, <span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span>,
    y1 <span class="op">=</span> <span class="fl">25</span> <span class="op">+</span> <span class="fl">0</span> <span class="op">*</span> <span class="va">D</span> <span class="op">+</span> <span class="fl">1.5</span> <span class="op">*</span> <span class="va">x</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/context.html">n</a></span><span class="op">(</span><span class="op">)</span>, <span class="fl">0</span>, <span class="fl">20</span><span class="op">)</span>
  <span class="op">)</span>

<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y1</span>, colour <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">D</span><span class="op">)</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">dat</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>alpha <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_vline</a></span><span class="op">(</span>xintercept <span class="op">=</span> <span class="fl">50</span>, colour <span class="op">=</span> <span class="st">"grey"</span>, linetype <span class="op">=</span> <span class="fl">2</span><span class="op">)</span><span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">stat_smooth</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"lm"</span>, se <span class="op">=</span> <span class="cn">F</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>x <span class="op">=</span> <span class="st">"Test score (X)"</span>, y <span class="op">=</span> <span class="st">"Potential Outcome (Y1)"</span><span class="op">)</span></code></pre></div>
<p>Figure <a href="ch5.html#fig:sim2">6.7</a> shows the results from this simulation. Notice that the value of <span class="math inline">\(E[Y^1\mid X]\)</span> is changing continuously over <span class="math inline">\(X\)</span> and through <span class="math inline">\(c_0\)</span>. This is an example of the continuity assumption. It means <em>absent the treatment itself</em>, the expected potential outcomes would’ve remained a smooth function of <span class="math inline">\(X\)</span> even as passing <span class="math inline">\(c_0\)</span>. Therefore, if continuity held, then <em>only</em> the treatment, triggered at <span class="math inline">\(c_0\)</span>, could be responsible for discrete jumps in <span class="math inline">\(E[Y\mid X]\)</span>.</p>
<div class="figure">
<span id="fig:sim2"></span>
<img src="graphics/rdd_simul1.jpg" alt="Smoothness of $Y^1$ across the cutoff illustrated using simulated data." width="100%"><p class="caption">
Figure 6.7: Smoothness of <span class="math inline">\(Y^1\)</span> across the cutoff illustrated using simulated data.
</p>
</div>
<p>The nice thing about simulations is that we actually observe the potential outcomes <em>since we made them ourselves</em>. But in the real world, we don’t have data on potential outcomes. If we did, we could test the continuity assumption directly. But remember—by the switching equation, we only observe actual outcomes, never potential outcomes. Thus, since units switch from <span class="math inline">\(Y^0\)</span> to <span class="math inline">\(Y^1\)</span> at <span class="math inline">\(c_0\)</span>, we actually can’t directly evaluate the continuity assumption. This is where institutional knowledge goes a long way, because it can help build the case that nothing else is changing at the cutoff that would otherwise shift potential outcomes.</p>
<p>Let’s illustrate this using simulated data. Notice that while <span class="math inline">\(Y^1\)</span> by construction had not jumped at 50 on the <span class="math inline">\(X\)</span> running variable, <span class="math inline">\(Y\)</span> will. Let’s look at the output in Figure <a href="ch5.html#fig:sim3">6.8</a>. Notice the jump at the discontinuity in the outcome, which I’ve labeled the LATE, or local average treatment effect.</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/rdd_simulate2.do"><code>rdd_simulate2.do</code></a></em></p>
<div class="sourceCode" id="cb53"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb53-1"><a href="ch5.html#cb53-1" aria-hidden="true"></a>* Actual outcome jumping</span>
<span id="cb53-2"><a href="ch5.html#cb53-2" aria-hidden="true"></a><span class="kw">gen</span> <span class="fu">y</span> = 25 + 40*D + 1.5*x + rnormal(0, 20)</span>
<span id="cb53-3"><a href="ch5.html#cb53-3" aria-hidden="true"></a><span class="kw">scatter</span> <span class="fu">y</span> x <span class="kw">if</span> D==0, msize(vsmall) || <span class="kw">scatter</span> <span class="fu">y</span> x <span class="kw">if</span> D==1, msize(vsmall) <span class="bn">legend</span>(<span class="kw">off</span>) <span class="bn">xline</span>(50, lstyle(<span class="bn">foreground</span>)) || <span class="kw">lfit</span> <span class="fu">y</span> x <span class="kw">if</span> D ==0, <span class="kw">color</span>(<span class="kw">red</span>) || <span class="kw">lfit</span> <span class="fu">y</span> x <span class="kw">if</span> D ==1, <span class="kw">color</span>(<span class="kw">red</span>) <span class="bn">ytitle</span>(<span class="st">"Outcome (Y)"</span>)  <span class="bn">xtitle</span>(<span class="st">"Test Score (X)"</span>) </span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/rdd_simulate2.R"><code>rdd_simulate2.R</code></a></em></p>
<div class="sourceCode" id="cb54"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># simulate the discontinuity</span>
<span class="va">dat</span> <span class="op">&lt;-</span> <span class="va">dat</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>
    y2 <span class="op">=</span> <span class="fl">25</span> <span class="op">+</span> <span class="fl">40</span> <span class="op">*</span> <span class="va">D</span> <span class="op">+</span> <span class="fl">1.5</span> <span class="op">*</span> <span class="va">x</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/context.html">n</a></span><span class="op">(</span><span class="op">)</span>, <span class="fl">0</span>, <span class="fl">20</span><span class="op">)</span>
  <span class="op">)</span>

<span class="co"># figure 36</span>
<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y2</span>, colour <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">D</span><span class="op">)</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">dat</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>alpha <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_vline</a></span><span class="op">(</span>xintercept <span class="op">=</span> <span class="fl">50</span>, colour <span class="op">=</span> <span class="st">"grey"</span>, linetype <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">stat_smooth</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"lm"</span>, se <span class="op">=</span> <span class="cn">F</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>x <span class="op">=</span> <span class="st">"Test score (X)"</span>, y <span class="op">=</span> <span class="st">"Potential Outcome (Y)"</span><span class="op">)</span></code></pre></div>
<div class="figure">
<span id="fig:sim3"></span>
<img src="graphics/rdd_simul2.jpg" alt="Estimated LATE using simulated data." width="100%"><p class="caption">
Figure 6.8: Estimated LATE using simulated data.
</p>
</div>
</div>
<div id="estimation-using-local-and-global-least-squares-regressions" class="section level3" number="6.2.3">
<h3>
<span class="header-section-number">6.2.3</span> Estimation using local and global least squares regressions<a class="anchor" aria-label="anchor" href="#estimation-using-local-and-global-least-squares-regressions"><i class="fas fa-link"></i></a>
</h3>
<p>I’d like to now dig into the actual regression model you would use to estimate the LATE parameter in an RDD. We will first discuss some basic modeling choices that researchers often make—some trivial, some important. This section will focus primarily on regression-based estimation.</p>
<p>While not necessary, it is nonetheless quite common for authors to transform the running variable <span class="math inline">\(X\)</span> by recentering at <span class="math inline">\(c_0\)</span>:
<span class="math display">\[
Y_i=\alpha+\beta(X_i-c_0)+\delta D_i+\varepsilon_i
\]</span>
This doesn’t change the interpretation of the treatment effect—only the interpretation of the intercept. Let’s use <span class="citation">Card, Dobkin, and Maestas (<a href="references.html#ref-Card2008" role="doc-biblioref">2008</a>)</span> as an example. Medicare is triggered when a person turns 65. So recenter the running variable (age) by subtracting 65:</p>
<p><span class="math display">\[\begin{align}
   Y &amp; =\beta_0 + \beta_1 (Age-65) + \beta_2 Edu + \varepsilon           \\
     &amp; =\beta_0 + \beta_1 Age - \beta_1 65 + \beta_2 Edu+ \varepsilon    \\
     &amp; =(\beta_0 - \beta_1 65) + \beta_1 Age + \beta_2 Edu + \varepsilon \\
     &amp; = \alpha + \beta_1 Age + \beta_2 Edu+ \varepsilon                 
\end{align}\]</span></p>
<p>where <span class="math inline">\(\alpha=\beta_0 + \beta_1 65\)</span>. All other coefficients, notice, have the same interpretation except for the intercept.</p>
<p>Another practical question relates to nonlinear data-generating processes. A nonlinear data-generating process could easily yield false positives if we do not handle the specification carefully. Because sometimes we are fitting local linear regressions around the cutoff, we could spuriously pick up an effect simply for no other reason than that we imposed linearity on the model. But if the underlying data-generating process is nonlinear, then it may be a spurious result due to misspecification of the model. Consider an example of this nonlinearity in Figure <a href="ch5.html#fig:Stata-sim2">6.9</a>.</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/rdd_simulate3.do"><code>rdd_simulate3.do</code></a></em></p>
<div class="sourceCode" id="cb55"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb55-1"><a href="ch5.html#cb55-1" aria-hidden="true"></a>* Nonlinear <span class="kw">data</span> generating process</span>
<span id="cb55-2"><a href="ch5.html#cb55-2" aria-hidden="true"></a><span class="kw">drop</span> <span class="fu">y</span> y1 x* D</span>
<span id="cb55-3"><a href="ch5.html#cb55-3" aria-hidden="true"></a><span class="kw">set</span> <span class="kw">obs</span> 1000</span>
<span id="cb55-4"><a href="ch5.html#cb55-4" aria-hidden="true"></a><span class="kw">gen</span> x = rnormal(100, 50)</span>
<span id="cb55-5"><a href="ch5.html#cb55-5" aria-hidden="true"></a><span class="kw">replace</span> x=0 <span class="kw">if</span> x &lt; 0</span>
<span id="cb55-6"><a href="ch5.html#cb55-6" aria-hidden="true"></a><span class="kw">drop</span> <span class="kw">if</span> x &gt; 280</span>
<span id="cb55-7"><a href="ch5.html#cb55-7" aria-hidden="true"></a><span class="kw">sum</span> x, <span class="fu">det</span></span>
<span id="cb55-8"><a href="ch5.html#cb55-8" aria-hidden="true"></a></span>
<span id="cb55-9"><a href="ch5.html#cb55-9" aria-hidden="true"></a>* Set the cutoff <span class="fu">at</span> X=140. Treated <span class="kw">if</span> X &gt; 140</span>
<span id="cb55-10"><a href="ch5.html#cb55-10" aria-hidden="true"></a><span class="kw">gen</span> D = 0</span>
<span id="cb55-11"><a href="ch5.html#cb55-11" aria-hidden="true"></a><span class="kw">replace</span> D = 1 <span class="kw">if</span> x &gt; 140</span>
<span id="cb55-12"><a href="ch5.html#cb55-12" aria-hidden="true"></a><span class="kw">gen</span> <span class="kw">x2</span> = x*x</span>
<span id="cb55-13"><a href="ch5.html#cb55-13" aria-hidden="true"></a><span class="kw">gen</span> x3 = x*x*x</span>
<span id="cb55-14"><a href="ch5.html#cb55-14" aria-hidden="true"></a><span class="kw">gen</span> <span class="fu">y</span> = 10000 + 0*D - 100*x +<span class="kw">x2</span> + rnormal(0, 1000)</span>
<span id="cb55-15"><a href="ch5.html#cb55-15" aria-hidden="true"></a><span class="kw">reg</span> <span class="fu">y</span> D x</span>
<span id="cb55-16"><a href="ch5.html#cb55-16" aria-hidden="true"></a></span>
<span id="cb55-17"><a href="ch5.html#cb55-17" aria-hidden="true"></a><span class="kw">scatter</span> <span class="fu">y</span> x <span class="kw">if</span> D==0, msize(vsmall) || <span class="kw">scatter</span> <span class="fu">y</span> x <span class="co">///</span></span>
<span id="cb55-18"><a href="ch5.html#cb55-18" aria-hidden="true"></a>  <span class="kw">if</span> D==1, msize(vsmall) <span class="bn">legend</span>(<span class="kw">off</span>) <span class="bn">xline</span>(140, <span class="co">///</span></span>
<span id="cb55-19"><a href="ch5.html#cb55-19" aria-hidden="true"></a>  lstyle(<span class="bn">foreground</span>)) <span class="kw">ylabel</span>(<span class="kw">none</span>) || <span class="kw">lfit</span> <span class="fu">y</span> x <span class="co">///</span></span>
<span id="cb55-20"><a href="ch5.html#cb55-20" aria-hidden="true"></a>  <span class="kw">if</span> D ==0, <span class="kw">color</span>(<span class="kw">red</span>) || <span class="kw">lfit</span> <span class="fu">y</span> x <span class="kw">if</span> D ==1, <span class="co">///</span></span>
<span id="cb55-21"><a href="ch5.html#cb55-21" aria-hidden="true"></a>  <span class="kw">color</span>(<span class="kw">red</span>) <span class="bn">xtitle</span>(<span class="st">"Test Score (X)"</span>) <span class="co">///</span></span>
<span id="cb55-22"><a href="ch5.html#cb55-22" aria-hidden="true"></a>  <span class="bn">ytitle</span>(<span class="st">"Outcome (Y)"</span>) </span>
<span id="cb55-23"><a href="ch5.html#cb55-23" aria-hidden="true"></a></span>
<span id="cb55-24"><a href="ch5.html#cb55-24" aria-hidden="true"></a>* Polynomial estimation</span>
<span id="cb55-25"><a href="ch5.html#cb55-25" aria-hidden="true"></a><span class="kw">capture</span> <span class="kw">drop</span> <span class="fu">y</span></span>
<span id="cb55-26"><a href="ch5.html#cb55-26" aria-hidden="true"></a><span class="kw">gen</span> <span class="fu">y</span> = 10000 + 0*D - 100*x +<span class="kw">x2</span> + rnormal(0, 1000)</span>
<span id="cb55-27"><a href="ch5.html#cb55-27" aria-hidden="true"></a><span class="kw">reg</span> <span class="fu">y</span> D x <span class="kw">x2</span> x3</span>
<span id="cb55-28"><a href="ch5.html#cb55-28" aria-hidden="true"></a><span class="kw">predict</span> yhat </span>
<span id="cb55-29"><a href="ch5.html#cb55-29" aria-hidden="true"></a></span>
<span id="cb55-30"><a href="ch5.html#cb55-30" aria-hidden="true"></a><span class="kw">scatter</span> <span class="fu">y</span> x <span class="kw">if</span> D==0, msize(vsmall) || <span class="kw">scatter</span> <span class="fu">y</span> x <span class="co">///</span></span>
<span id="cb55-31"><a href="ch5.html#cb55-31" aria-hidden="true"></a>  <span class="kw">if</span> D==1, msize(vsmall) <span class="bn">legend</span>(<span class="kw">off</span>) <span class="bn">xline</span>(140, <span class="co">///</span></span>
<span id="cb55-32"><a href="ch5.html#cb55-32" aria-hidden="true"></a>  lstyle(<span class="bn">foreground</span>)) <span class="kw">ylabel</span>(<span class="kw">none</span>) || <span class="kw">line</span> yhat x <span class="co">///</span></span>
<span id="cb55-33"><a href="ch5.html#cb55-33" aria-hidden="true"></a>  <span class="kw">if</span> D ==0, <span class="kw">color</span>(<span class="kw">red</span>) <span class="kw">sort</span> || <span class="kw">line</span> yhat x <span class="kw">if</span> D==1, <span class="co">///</span></span>
<span id="cb55-34"><a href="ch5.html#cb55-34" aria-hidden="true"></a>  <span class="kw">sort</span> <span class="kw">color</span>(<span class="kw">red</span>) <span class="bn">xtitle</span>(<span class="st">"Test Score (X)"</span>) <span class="co">///</span></span>
<span id="cb55-35"><a href="ch5.html#cb55-35" aria-hidden="true"></a>  <span class="bn">ytitle</span>(<span class="st">"Outcome (Y)"</span>) </span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/rdd_simulate3.R"><code>rdd_simulate3.R</code></a></em></p>
<div class="sourceCode" id="cb56"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># simultate nonlinearity</span>
<span class="va">dat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble</a></span><span class="op">(</span>
    x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">1000</span>, <span class="fl">100</span>, <span class="fl">50</span><span class="op">)</span>
  <span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>
    x <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">x</span> <span class="op">&lt;</span> <span class="fl">0</span> <span class="op">~</span> <span class="fl">0</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="va">x</span><span class="op">)</span>,
    D <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">x</span> <span class="op">&gt;</span> <span class="fl">140</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span>,
    x2 <span class="op">=</span> <span class="va">x</span><span class="op">*</span><span class="va">x</span>,
    x3 <span class="op">=</span> <span class="va">x</span><span class="op">*</span><span class="va">x</span><span class="op">*</span><span class="va">x</span>,
    y3 <span class="op">=</span> <span class="fl">10000</span> <span class="op">+</span> <span class="fl">0</span> <span class="op">*</span> <span class="va">D</span> <span class="op">-</span> <span class="fl">100</span> <span class="op">*</span> <span class="va">x</span> <span class="op">+</span> <span class="va">x2</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">1000</span>, <span class="fl">0</span>, <span class="fl">1000</span><span class="op">)</span>
  <span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">x</span> <span class="op">&lt;</span> <span class="fl">280</span><span class="op">)</span>


<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y3</span>, colour <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">D</span><span class="op">)</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">dat</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>alpha <span class="op">=</span> <span class="fl">0.2</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_vline</a></span><span class="op">(</span>xintercept <span class="op">=</span> <span class="fl">140</span>, colour <span class="op">=</span> <span class="st">"grey"</span>, linetype <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">stat_smooth</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"lm"</span>, se <span class="op">=</span> <span class="cn">F</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>x <span class="op">=</span> <span class="st">"Test score (X)"</span>, y <span class="op">=</span> <span class="st">"Potential Outcome (Y)"</span><span class="op">)</span>

<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y3</span>, colour <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">D</span><span class="op">)</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">dat</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>alpha <span class="op">=</span> <span class="fl">0.2</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_vline</a></span><span class="op">(</span>xintercept <span class="op">=</span> <span class="fl">140</span>, colour <span class="op">=</span> <span class="st">"grey"</span>, linetype <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">stat_smooth</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"loess"</span>, se <span class="op">=</span> <span class="cn">F</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>x <span class="op">=</span> <span class="st">"Test score (X)"</span>, y <span class="op">=</span> <span class="st">"Potential Outcome (Y)"</span><span class="op">)</span></code></pre></div>
<div class="figure">
<span id="fig:Stata-sim2"></span>
<img src="graphics/rdd_simul4.jpg" alt="Simulated nonlinear data from Stata." width="100%"><p class="caption">
Figure 6.9: Simulated nonlinear data from Stata.
</p>
</div>
<p>I show this both visually and with a regression. As you can see in Figure <a href="ch5.html#fig:Stata-sim2">6.9</a>, the data-generating process was nonlinear, but when with straight lines to the left and right of the cutoff, the trends in the running variable generate a spurious discontinuity at the cutoff. This shows up in a regression as well. When we fit the model using a least squares regression controlling for the running variable, we estimate a causal effect though there isn’t one. In Table <a href="ch5.html#tab:reg-rd-simul">6.1</a>, the estimated effect of <span class="math inline">\(D\)</span> on <span class="math inline">\(Y\)</span> is large and highly significant, even though the true effect is zero. In this situation, we would need some way to model the nonlinearity below and above the cutoff to check whether, even given the nonlinearity, there had been a jump in the outcome at the discontinuity.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:reg-rd-simul">Table 6.1: </span> Estimated effect of D on Y using OLS controlling for linear running variable.</caption>
<thead><tr class="header">
<th align="left"><strong>Dependent variable</strong></th>
<th></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Treatment (D)</td>
<td>6580.16<span class="math inline">\(^{**}\)</span><span class="math inline">\(^{*}\)</span>
</td>
</tr>
<tr class="even">
<td align="left"></td>
<td>(305.88)</td>
</tr>
</tbody>
</table></div>
<p>Suppose that the nonlinear relationships is
<span class="math display">\[
E\big[Y^0_i\mid X_i\big]=f(X_i)
\]</span>
for some reasonably smooth function <span class="math inline">\(f(X_i)\)</span>. In that case, we’d fit the regression model:
<span class="math display">\[
Y_i=f(X_i) + \delta D_i + \eta_i
\]</span>
Since <span class="math inline">\(f(X_i)\)</span> is counterfactual for values of <span class="math inline">\(X_i&gt;c_0\)</span>, how will we model the nonlinearity? There are two ways of approximating <span class="math inline">\(f(X_i)\)</span>. The traditional approaches let <span class="math inline">\(f(X_Be i)\)</span> equal a <span class="math inline">\(p{th}\)</span>-order polynomial:
<span class="math display">\[
Y_i = \alpha + \beta_1 x_i + \beta_2 x_i^2 + \dots + \beta_p x_i^p + \delta D_i + \eta_i
\]</span>
Higher-order polynomials can lead to overfitting and have been found to introduce bias <span class="citation">(Gelman and Imbens <a href="references.html#ref-Gelman2019" role="doc-biblioref">2019</a>)</span>. Those authors recommend using local linear regressions with linear and quadratic forms only. Another way of approximating <span class="math inline">\(f(X_i)\)</span> is to use a nonparametric kernel, which I discuss later.</p>
<p>Though <span class="citation">Gelman and Imbens (<a href="references.html#ref-Gelman2019" role="doc-biblioref">2019</a>)</span> warn us about higher-order polynomials, I’d like to use an example with <span class="math inline">\(p\)</span>th-order polynomials, mainly because it’s not uncommon to see this done today. I’d also like you to know some of the history of this method and understand better what old papers were doing. We can generate this function, <span class="math inline">\(f(X_i)\)</span>, by allowing the <span class="math inline">\(X_i\)</span> terms to differ on both sides of the cutoff by including them both individually and interacting them with <span class="math inline">\(D_i\)</span>. In that case, we have:</p>
<p><span class="math display">\[\begin{align}
   E\big[Y^0_i\mid X_i\big] &amp; =\alpha + \beta_{01} \widetilde{X}_i + \dots + \beta_{0p}\widetilde{X}_i^p           
   \\
   E\big[Y^1_i\mid X_i\big] &amp; =\alpha + \delta + \beta_{11} \widetilde{X}_i + \dots + \beta_{1p} \widetilde{X}_i^p 
\end{align}\]</span></p>
<p>where <span class="math inline">\(\widetilde{X}_i\)</span> is the recentered running variable (i.e., <span class="math inline">\(X_i - c_0\)</span>). Centering at <span class="math inline">\(c_0\)</span> ensures that the treatment effect at <span class="math inline">\(X_i=X_0\)</span> is the coefficient on <span class="math inline">\(D_i\)</span> in a regression model with interaction terms. As <span class="citation">Lee and Lemieux (<a href="references.html#ref-Lee2010" role="doc-biblioref">2010</a>)</span> note, allowing different functions on both sides of the discontinuity should be the main results in an RDD paper.</p>
<p>To derive a regression model, first note that the observed values must be used in place of the potential outcomes:
<span class="math display">\[
E[Y\mid X]=E[Y^0\mid X]+\Big(E[Y^1\mid X] - E[Y^0 \mid X]\Big)D
\]</span>
Your regression model then is
<span class="math display">\[
Y_i= \alpha + \beta_{01}\tilde{X}_i + \dots + \beta_{0p}\tilde{X}_i^p +
   \delta D_i+ \beta_1^*D_i \tilde{X}_i + \dots + \beta_p^* D_i \tilde{X}_i^p + \varepsilon_i
\]</span>
where <span class="math inline">\(\beta_1^* = \beta_{11} - \beta_{01}\)</span>, and <span class="math inline">\(\beta_p^* = \beta_{1p} - \beta_{0p}\)</span>. The equation we looked at earlier was just a special case of the above equation with <span class="math inline">\(\beta_1^*=\beta_p^*=0\)</span>. The treatment effect at <span class="math inline">\(c_0\)</span> is <span class="math inline">\(\delta\)</span>. And the treatment effect at <span class="math inline">\(X_i-c_0&gt;0\)</span> is <span class="math inline">\(\delta + \beta_1^*c + \dots + \beta_p^* c^p\)</span>. Let’s see this in action with another simulation.</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/rdd_simulate4.do"><code>rdd_simulate4.do</code></a></em></p>
<div class="sourceCode" id="cb57"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb57-1"><a href="ch5.html#cb57-1" aria-hidden="true"></a>* Polynomial modeling</span>
<span id="cb57-2"><a href="ch5.html#cb57-2" aria-hidden="true"></a><span class="kw">capture</span> <span class="kw">drop</span> <span class="fu">y</span></span>
<span id="cb57-3"><a href="ch5.html#cb57-3" aria-hidden="true"></a><span class="kw">gen</span> <span class="fu">y</span> = 10000 + 0*D - 100*x +<span class="kw">x2</span> + rnormal(0, 1000)</span>
<span id="cb57-4"><a href="ch5.html#cb57-4" aria-hidden="true"></a><span class="kw">reg</span> <span class="fu">y</span> D##c.(x <span class="kw">x2</span> x3)</span>
<span id="cb57-5"><a href="ch5.html#cb57-5" aria-hidden="true"></a><span class="kw">predict</span> yhat </span>
<span id="cb57-6"><a href="ch5.html#cb57-6" aria-hidden="true"></a></span>
<span id="cb57-7"><a href="ch5.html#cb57-7" aria-hidden="true"></a><span class="kw">scatter</span> <span class="fu">y</span> x <span class="kw">if</span> D==0, msize(vsmall) || <span class="kw">scatter</span> <span class="fu">y</span> x <span class="co">///</span></span>
<span id="cb57-8"><a href="ch5.html#cb57-8" aria-hidden="true"></a>  <span class="kw">if</span> D==1, msize(vsmall) <span class="bn">legend</span>(<span class="kw">off</span>) <span class="bn">xline</span>(140, <span class="co">///</span></span>
<span id="cb57-9"><a href="ch5.html#cb57-9" aria-hidden="true"></a>  lstyle(<span class="bn">foreground</span>)) <span class="kw">ylabel</span>(<span class="kw">none</span>) || <span class="kw">line</span> yhat x <span class="co">///</span></span>
<span id="cb57-10"><a href="ch5.html#cb57-10" aria-hidden="true"></a>  <span class="kw">if</span> D ==0, <span class="kw">color</span>(<span class="kw">red</span>) <span class="kw">sort</span> || <span class="kw">line</span> yhat x <span class="kw">if</span> D==1, <span class="co">///</span></span>
<span id="cb57-11"><a href="ch5.html#cb57-11" aria-hidden="true"></a>  <span class="kw">sort</span> <span class="kw">color</span>(<span class="kw">red</span>) <span class="bn">xtitle</span>(<span class="st">"Test Score (X)"</span>) <span class="co">///</span></span>
<span id="cb57-12"><a href="ch5.html#cb57-12" aria-hidden="true"></a>  <span class="bn">ytitle</span>(<span class="st">"Outcome (Y)"</span>) </span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/rdd_simulate4.R"><code>rdd_simulate4.R</code></a></em></p>
<div class="sourceCode" id="cb58"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">stargazer</span><span class="op">)</span>

<span class="va">dat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble</a></span><span class="op">(</span>
  x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">1000</span>, <span class="fl">100</span>, <span class="fl">50</span><span class="op">)</span>
<span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>
    x <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">x</span> <span class="op">&lt;</span> <span class="fl">0</span> <span class="op">~</span> <span class="fl">0</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="va">x</span><span class="op">)</span>,
    D <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">x</span> <span class="op">&gt;</span> <span class="fl">140</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span>,
    x2 <span class="op">=</span> <span class="va">x</span><span class="op">*</span><span class="va">x</span>,
    x3 <span class="op">=</span> <span class="va">x</span><span class="op">*</span><span class="va">x</span><span class="op">*</span><span class="va">x</span>,
    y3 <span class="op">=</span> <span class="fl">10000</span> <span class="op">+</span> <span class="fl">0</span> <span class="op">*</span> <span class="va">D</span> <span class="op">-</span> <span class="fl">100</span> <span class="op">*</span> <span class="va">x</span> <span class="op">+</span> <span class="va">x2</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">1000</span>, <span class="fl">0</span>, <span class="fl">1000</span><span class="op">)</span>
  <span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">x</span> <span class="op">&lt;</span> <span class="fl">280</span><span class="op">)</span>

<span class="va">regression</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y3</span> <span class="op">~</span> <span class="va">D</span><span class="op">*</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">dat</span><span class="op">)</span>
  
<span class="fu"><a href="https://rdrr.io/pkg/stargazer/man/stargazer.html">stargazer</a></span><span class="op">(</span><span class="va">regression</span>, type <span class="op">=</span> <span class="st">"text"</span><span class="op">)</span> 

<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y3</span>, colour <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">D</span><span class="op">)</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">dat</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>alpha <span class="op">=</span> <span class="fl">0.2</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_vline</a></span><span class="op">(</span>xintercept <span class="op">=</span> <span class="fl">140</span>, colour <span class="op">=</span> <span class="st">"grey"</span>, linetype <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">stat_smooth</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"loess"</span>, se <span class="op">=</span> <span class="cn">F</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>x <span class="op">=</span> <span class="st">"Test score (X)"</span>, y <span class="op">=</span> <span class="st">"Potential Outcome (Y)"</span><span class="op">)</span></code></pre></div>
<p>Let’s look at the output from this exercise in Figure <a href="ch5.html#fig:Stata-sim3">6.10</a> and Table <a href="ch5.html#tab:reg-rd-simul2">6.2</a>. As you can see, once we model the data using a quadratic (the cubic ultimately was unnecessary), there is no estimated treatment effect at the cutoff. There is also no effect in our least squares regression.</p>
<div class="figure">
<span id="fig:Stata-sim3"></span>
<img src="graphics/rdd_simul5.jpg" alt="Simulated nonlinear data from Stata" width="100%"><p class="caption">
Figure 6.10: Simulated nonlinear data from Stata
</p>
</div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:reg-rd-simul2">Table 6.2: </span> Estimated effect of D on Y using OLS controlling for linear and quadratic running variable.</caption>
<thead><tr class="header">
<th align="left"><strong>Dependent variable</strong></th>
<th></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Treatment (D)</td>
<td>-43.24</td>
</tr>
<tr class="even">
<td align="left"></td>
<td>(147.29)</td>
</tr>
</tbody>
</table></div>
</div>
<div id="nonparametric-kernels" class="section level3" number="6.2.4">
<h3>
<span class="header-section-number">6.2.4</span> Nonparametric kernels<a class="anchor" aria-label="anchor" href="#nonparametric-kernels"><i class="fas fa-link"></i></a>
</h3>
<p>But, as we mentioned earlier, <span class="citation">Gelman and Imbens (<a href="references.html#ref-Gelman2019" role="doc-biblioref">2019</a>)</span> have discouraged the use of higher-order polynomials when estimating local linear regressions. An alternative is to use kernel regression. The nonparametric kernel method has problems because you are trying to estimate regressions at the cutoff point, which can result in a boundary problem (see Figure <a href="ch5.html#fig:kernel1">6.11</a>). In this picture, the bias is caused by strong trends in expected potential outcomes throughout the running variable.</p>
<div class="figure">
<span id="fig:kernel1"></span>
<img src="graphics/kernel1.png" alt="Simulated nonlinear data from Stata" width="100%"><p class="caption">
Figure 6.11: Simulated nonlinear data from Stata
</p>
</div>
<p>While the true effect in this diagram is <span class="math inline">\(AB\)</span>, with a certain bandwidth a rectangular kernel would estimate the effect as <span class="math inline">\(A'B'\)</span>, which is as you can see a biased estimator. There is systematic bias with the kernel method if the underlying nonlinear function, <span class="math inline">\(f(X)\)</span>, is upwards-or downwards-xsloping.</p>
<p>The standard solution to this problem is to run local linear nonparametric regression <span class="citation">(Hahn, Todd, and Klaauw <a href="references.html#ref-Hahn2001" role="doc-biblioref">2001</a>)</span>. In the case described above, this would substantially reduce the bias. So what is that? Think of kernel regression as a weighted regression restricted to a window (hence “local”). The kernel provides the weights to that regression.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Stata’s &lt;code&gt;poly&lt;/code&gt; command estimates kernel-weighted local polynomial regression.&lt;/p&gt;"><sup>101</sup></a> A rectangular kernel would give the same result as taking <span class="math inline">\(E[Y]\)</span> at a given bin on <span class="math inline">\(X\)</span>. The triangular kernel gives more importance to the observations closest to the center.</p>
<p>The model is some version of:
<span class="math display">\[
(\widehat{a},\widehat{b})=_{a,b}
   \sum_{i=1}^n\Big(y_i - a -b(x_i-c_0)\Big)^2K
   \left(\dfrac{x_i-c_o}{h}\right)1(x_i&gt;c_0)
\]</span>
While estimating this in a given window of width <span class="math inline">\(h\)</span> around the cutoff is straightforward, what’s not straightforward is knowing how large or small to make the bandwidth. This method is sensitive to the choice of bandwidth, but more recent work allows the researcher to estimate <em>optimal</em> bandwidths <span class="citation">(Imbens and Kalyanaraman <a href="references.html#ref-Imbens2011" role="doc-biblioref">2011</a>; Calonico, Cattaneo, and Titiunik <a href="references.html#ref-Calonico2014" role="doc-biblioref">2014</a>)</span>. These may even allow for bandwidths to vary left and right of the cutoff.</p>
</div>
<div id="medicare-and-universal-health-care" class="section level3" number="6.2.5">
<h3>
<span class="header-section-number">6.2.5</span> Medicare and universal health care<a class="anchor" aria-label="anchor" href="#medicare-and-universal-health-care"><i class="fas fa-link"></i></a>
</h3>
<p><span class="citation">Card, Dobkin, and Maestas (<a href="references.html#ref-Card2008" role="doc-biblioref">2008</a>)</span> is an example of a sharp RDD, because it focuses on the provision of universal health-care insurance for the elderly—Medicare at age 65. What makes this a policy-relevant question? Universal insurance has become highly relevant because of the debates surrounding the Affordable Care Act, as well as several Democratic senators supporting Medicare for All. But it is also important for its sheer size. In 2014, Medicare was 14% of the federal budget at $505 billion.</p>
<p>Approximately 20% of non-elderly adults in the United States lacked insurance in 2005. Most were from lower-income families, and nearly half were African American or Hispanic. Many analysts have argued that unequal insurance coverage contributes to disparities in health-care utilization and health outcomes across socioeconomic status. But, even among the policies, there is heterogeneity in the form of different copays, deductibles, and other features that affect use. Evidence that better insurance causes better health outcomes is limited because health insurance suffers from deep selection bias. Both supply and demand for insurance depend on health status, confounding observational comparisons between people with different insurance characteristics.</p>
<p>The situation for elderly looks very different, though. Less than 1% of the elderly population is uninsured. Most have fee-for-service Medicare coverage. And that transition to Medicare occurs sharply at age 65—the threshold for Medicare eligibility.</p>
<p>The authors estimate a reduced form model measuring the causal effect of health insurance status on health-care usage:
<span class="math display">\[
y_{ija} = X_{ija} \alpha + f_k(\alpha ; \beta ) + \sum_k C_{ija}^k \delta^k + u_{ija}
\]</span>
where <span class="math inline">\(i\)</span> indexes individuals, <span class="math inline">\(j\)</span> indexes a socioeconomic group, <span class="math inline">\(a\)</span> indexes age, <span class="math inline">\(u_{ija}\)</span> indexes the unobserved error, <span class="math inline">\(y_{ija}\)</span> health care usage, <span class="math inline">\(X_{ija}\)</span> a set of covariates (e.g., gender and region), <span class="math inline">\(f_j(\alpha ; \beta )\)</span> a smooth function representing the age profile of outcome <span class="math inline">\(y\)</span> for group <span class="math inline">\(j\)</span>, and <span class="math inline">\(C_{ija}^k\)</span> <span class="math inline">\((k=1,2,\dots ,K)\)</span> are characteristics of the insurance coverage held by the individual such as copayment rates. The problem with estimating this model, though, is that insurance coverage is endogenous: <span class="math inline">\(cov(u,C) \neq 0\)</span>. So the authors use as identification of the age threshold for Medicare eligibility at 65, which they argue is credibly exogenous variation in insurance status.</p>
<p>Suppose health insurance coverage can be summarized by two dummy variables: <span class="math inline">\(C_{ija}^1\)</span> (any coverage) and <span class="math inline">\(C_{ija}^2\)</span> (generous insurance). <span class="citation">Card, Dobkin, and Maestas (<a href="references.html#ref-Card2008" role="doc-biblioref">2008</a>)</span> estimate the following linear probability models:</p>
<p><span class="math display">\[\begin{align}
   C_{ija}^1 &amp; =X_{ija}\beta_j^1 + g_j^1(a) + D_a \pi_j^1 + v_{ija}^1  
   \\
   C_{ija}^2 &amp; =X_{ija} \beta_j^2 + g_j^2(a) + D_a \pi_j^2 + v_{ija}^2 
\end{align}\]</span></p>
<p>where <span class="math inline">\(\beta_j^1\)</span> and <span class="math inline">\(\beta_j^2\)</span> are group-specific coefficients, <span class="math inline">\(g_j^1(a)\)</span> and <span class="math inline">\(g_j^2(a)\)</span> are smooth age profiles for group <span class="math inline">\(j\)</span>, and <span class="math inline">\(D_a\)</span> is a dummy if the respondent is equal to or over age 65. Recall the reduced form model:</p>
<p><span class="math display">\[\begin{align}
   y_{ija} = X_{ija} \alpha + f_k(\alpha ; \beta ) + \sum_k C_{ija}^k \delta^k + u_{ija}
\end{align}\]</span></p>
<p>Combining the <span class="math inline">\(C_{ija}\)</span> equations, and rewriting the reduced form model, we get:</p>
<p><span class="math display">\[\begin{align}
   y_{ija}=X_{ija}\Big(\alpha_j+\beta_j^1\delta_j^1+ \beta_j^2 \delta_j^2\Big)h_j(a)+D_a\pi_j^y+ v_{ija}^y
\end{align}\]</span></p>
<p>where <span class="math inline">\(h(a)=f_j(a) + \delta^1 g_j^1(a) + \delta^2 g_j^2(a)\)</span> is the reduced form age profile for group <span class="math inline">\(j\)</span>, <span class="math inline">\(\pi_j^y=\pi_j^1\delta^1 + \pi_j^2 \delta^2\)</span> and <span class="math inline">\(v_{ija}^y=u_{ija} + v_{ija}^1 \delta^1 + v_{ija}^2 \delta^2\)</span> is the error term. Assuming that the profiles <span class="math inline">\(f_j(a)\)</span>, <span class="math inline">\(g_j(a)\)</span>, and <span class="math inline">\(g_j^2(a)\)</span> are continuous at age 65 (i.e., the continuity assumption necessary for identification), then any discontinuity in <span class="math inline">\(y\)</span> is due to insurance. The magnitudes will depend on the size of the insurance changes at age 65 (<span class="math inline">\(\pi_j^1\)</span> and <span class="math inline">\(\pi_j^2\)</span>) and on the associated causal effects (<span class="math inline">\(\delta^1\)</span> and <span class="math inline">\(\delta^2\)</span>).</p>
<p>For some basic health-care services, such as routine doctor visits, it may be that the only thing that matters is insurance. But, in those situations, the implied discontinuity in <span class="math inline">\(Y\)</span> at age 65 for group <span class="math inline">\(j\)</span> will be proportional to the change in insurance status experienced by that group. For more expensive or elective services, the generosity of the coverage may matter—for instance, if patients are unwilling to cover the required copay or if the managed care program won’t cover the service. This creates a potential identification problem in interpreting the discontinuity in <span class="math inline">\(y\)</span> for any one group. Since <span class="math inline">\(\pi_j^y\)</span> is a linear combination of the discontinuities in coverage and generosity, <span class="math inline">\(\delta^1\)</span> and <span class="math inline">\(\delta^2\)</span> can be estimated by a regression across groups:
<span class="math display">\[
\pi_j^y=\delta^0+\delta^1\pi_j^1+\delta_j^2\pi_j^2+e_j
\]</span>
where <span class="math inline">\(e_j\)</span> is an error term reflecting a combination of the sampling errors in <span class="math inline">\(\pi_j^y\)</span>, <span class="math inline">\(\pi_j^1\)</span> and, <span class="math inline">\(\pi_j^2\)</span>.</p>
<p><span class="citation">Card, Dobkin, and Maestas (<a href="references.html#ref-Card2008" role="doc-biblioref">2008</a>)</span> use a couple of different data sets—one a standard survey and the other administrative records from hospitals in three states. First, they use the 1992–2003 National Health Interview Survey (NHIS). The NHIS reports respondents’ birth year, birth month, and calendar quarter of the interview. Authors used this to construct an estimate of age in quarters at date of interview. A person who reaches 65 in the interview quarter is coded as age 65 and 0 quarters. Assuming a uniform distribution of interview dates, one-half of these people will be 0–6 weeks younger than 65 and one-half will be 0–6 weeks older. Analysis is limited to people between 55 and 75. The final sample has 160,821 observations.</p>
<p>The second data set is hospital discharge records for California, Florida, and New York. These records represent a complete census of discharges from all hospitals in the three states except for federally regulated institutions. The data files include information on age in months at the time of admission. Their sample selection criteria is to drop records for people admitted as transfers from other institutions and limit people between 60 and 70 years of age at admission. Sample sizes are 4,017,325 (California), 2,793,547 (Florida), and 3,121,721 (New York).</p>
<p>Some institutional details about the Medicare program may be helpful. Medicare is available to people who are at least 65 and have worked forty quarters or more in covered employment or have a spouse who did. Coverage is available to younger people with severe kidney disease and recipients of Social Security Disability Insurance. Eligible individuals can obtain Medicare hospital insurance (Part A) free of charge and medical insurance (Part B) for a modest monthly premium. Individuals receive notice of their impending eligibility for Medicare shortly before they turn 65 and are informed they have to enroll in it and choose whether to accept Part B coverage. Coverage begins on the first day of the month in which they turn 65.</p>
<p>There are five insurance-related variables: probability of Medicare coverage, any health insurance coverage, private coverage, two or more forms of coverage, and individual’s primary health insurance is managed care. Data are drawn from the 1999–2003 NHIS, and for each characteristic, authors show the incidence rate at age 63–64 and the change at age 65 based on a version of the <span class="math inline">\(C_K\)</span> equations that include a quadratic in age, fully interacted with a post-65 dummy as well as controls for gender, education, race/ethnicity, region, and sample year. Alternative specifications were also used, such as a parametric model fit to a narrower age window (age 63–67) and a local linear regression specification using a chosen bandwidth. Both show similar estimates of the change at age 65.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:card-table1">Table 6.3: </span> Insurance characteristics just before age 65 and estimated discontinuities at age 65</caption>
<thead><tr class="header">
<th></th>
<th align="left"><strong>On Medicare</strong></th>
<th align="left"><strong>Any insurance</strong></th>
<th align="left"><strong>Private coverage</strong></th>
<th align="left"><strong>2<span class="math inline">\(+\)</span> forms coverage</strong></th>
<th align="left"><strong>Managed care</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Overall sample</td>
<td align="left">59.7</td>
<td align="left">9.5</td>
<td align="left"><span class="math inline">\(-2.9\)</span></td>
<td align="left">44.1</td>
<td align="left"><span class="math inline">\(-28.4\)</span></td>
</tr>
<tr class="even">
<td></td>
<td align="left">(4.1)</td>
<td align="left">(0.6)</td>
<td align="left">(1.1)</td>
<td align="left">(2.8)</td>
<td align="left">(2.1)</td>
</tr>
<tr class="odd">
<td><em>White non-Hispanic</em></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="even">
<td>High school dropout</td>
<td align="left">58.5</td>
<td align="left">13.0</td>
<td align="left"><span class="math inline">\(-6.2\)</span></td>
<td align="left">44.5</td>
<td align="left"><span class="math inline">\(-25.0\)</span></td>
</tr>
<tr class="odd">
<td></td>
<td align="left">(4.6)</td>
<td align="left">(2.7)</td>
<td align="left">(3.3)</td>
<td align="left">(4.0)</td>
<td align="left">(4.5)</td>
</tr>
<tr class="even">
<td>High school graduate</td>
<td align="left">64.7</td>
<td align="left">7.6</td>
<td align="left"><span class="math inline">\(-1.9\)</span></td>
<td align="left">51.8</td>
<td align="left"><span class="math inline">\(-30.3\)</span></td>
</tr>
<tr class="odd">
<td></td>
<td align="left">(5.0)</td>
<td align="left">(0.7)</td>
<td align="left">(1.6)</td>
<td align="left">(3.8)</td>
<td align="left">(2.6)</td>
</tr>
<tr class="even">
<td>Some college</td>
<td align="left">68.4</td>
<td align="left">4.4</td>
<td align="left"><span class="math inline">\(-2.3\)</span></td>
<td align="left">55.1</td>
<td align="left"><span class="math inline">\(-40.1\)</span></td>
</tr>
<tr class="odd">
<td></td>
<td align="left">(4.7)</td>
<td align="left">(0.5)</td>
<td align="left">(1.8)</td>
<td align="left">(4.0)</td>
<td align="left">(2.6)</td>
</tr>
<tr class="even">
<td><em>Minority</em></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td>High school dropout</td>
<td align="left">44.5</td>
<td align="left">21.5</td>
<td align="left"><span class="math inline">\(-1.2\)</span></td>
<td align="left">19.4</td>
<td align="left"><span class="math inline">\(-8.3\)</span></td>
</tr>
<tr class="even">
<td></td>
<td align="left">(3.1)</td>
<td align="left">(2.1)</td>
<td align="left">(2.5)</td>
<td align="left">(1.9)</td>
<td align="left">(3.1)</td>
</tr>
<tr class="odd">
<td>High school graduate</td>
<td align="left">44.6</td>
<td align="left">8.9</td>
<td align="left"><span class="math inline">\(-5.8\)</span></td>
<td align="left">23.4</td>
<td align="left"><span class="math inline">\(-15.4\)</span></td>
</tr>
<tr class="even">
<td></td>
<td align="left">(4.7)</td>
<td align="left">(2.8)</td>
<td align="left">(5.1)</td>
<td align="left">(4.8)</td>
<td align="left">(3.5)</td>
</tr>
<tr class="odd">
<td>Some college</td>
<td align="left">52.1</td>
<td align="left">5.8</td>
<td align="left"><span class="math inline">\(-5.4\)</span></td>
<td align="left">38.4</td>
<td align="left"><span class="math inline">\(-22.3\)</span></td>
</tr>
<tr class="even">
<td></td>
<td align="left">(4.9)</td>
<td align="left">(2.0)</td>
<td align="left">(4.3)</td>
<td align="left">(3.8)</td>
<td align="left">(7.2)</td>
</tr>
<tr class="odd">
<td><em>Classified by ethnicity only</em></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="even">
<td>White non-Hispanic</td>
<td align="left">65.2</td>
<td align="left">7.3</td>
<td align="left"><span class="math inline">\(-2.8\)</span></td>
<td align="left">51.9</td>
<td align="left"><span class="math inline">\(-33.6\)</span></td>
</tr>
<tr class="odd">
<td></td>
<td align="left">(4.6)</td>
<td align="left">(0.5)</td>
<td align="left">(1.4)</td>
<td align="left">(3.5)</td>
<td align="left">(2.3)</td>
</tr>
<tr class="even">
<td>Black non-Hispanic</td>
<td align="left">48.5</td>
<td align="left">11.9</td>
<td align="left"><span class="math inline">\(-4.2\)</span></td>
<td align="left">27.8</td>
<td align="left"><span class="math inline">\(-13.5\)</span></td>
</tr>
<tr class="odd">
<td></td>
<td align="left">(3.6)</td>
<td align="left">(2.0)</td>
<td align="left">(2.8)</td>
<td align="left">(3.7)</td>
<td align="left">(3.7)</td>
</tr>
<tr class="even">
<td>Hispanic</td>
<td align="left">44.4</td>
<td align="left">17.3</td>
<td align="left"><span class="math inline">\(-2.0\)</span></td>
<td align="left">21.7</td>
<td align="left"><span class="math inline">\(-12.1\)</span></td>
</tr>
<tr class="odd">
<td></td>
<td align="left">(3.7)</td>
<td align="left">(3.0)</td>
<td align="left">(1.7)</td>
<td align="left">(2.1)</td>
<td align="left">(3.7)</td>
</tr>
</tbody>
</table></div>
<p>The authors present their findings in Table <a href="ch5.html#tab:card-table1">6.3</a>. The way that you read this table is each cell shows the <em>average treatment effect</em> for the 65-year-old population that complies with the treatment. We can see, not surprisingly, that the effect of receiving Medicare is to cause a very large increase of being on Medicare, as well as reducing coverage on private and managed care.</p>
<p>Formal identification in an RDD relating to some outcome (insurance coverage) to a treatment (Medicare age-eligibility) that itself depends on some running variable, age, relies on the continuity assumptions that we discussed earlier. That is, we must assume that the conditional expectation functions for both potential outcomes is continuous at age<span class="math inline">\(=65\)</span>. This means that both <span class="math inline">\(E[Y^0\mid a]\)</span> and <span class="math inline">\(E[Y^1\mid a]\)</span> are continuous through age of 65. If that assumption is plausible, then the average treatment effect at age 65 is identified as:
<span class="math display">\[
\lim_{65 \leftarrow a}E\big[y^1\mid a\big] -
   \lim_{a \rightarrow 65}E\big[y^0\mid a\big]
\]</span>
The continuity assumption requires that all other factors, observed and unobserved, that affect insurance coverage are trending smoothly at the cutoff, in other words. But what else changes at age 65 other than Medicare eligibility? Employment changes. Typically, 65 is the traditional age when people retire from the labor force. Any abrupt change in employment could lead to differences in health-care utilization if nonworkers have more time to visit doctors.</p>
<p>The authors need to, therefore, investigate this possible confounder. They do this by testing for any potential discontinuities at age 65 for confounding variables using a third data set—the March CPS 1996–2004. And they ultimately find no evidence for discontinuities in employment at age 65 (Figure <a href="ch5.html#fig:medicare3">6.12</a>).</p>
<div class="figure">
<span id="fig:medicare3"></span>
<img src="graphics/EmploymentRatesMenbyAge.jpg" alt="Investigating the CPS for discontinuities at age 65 [@Card2008]." width="100%"><p class="caption">
Figure 6.12: Investigating the CPS for discontinuities at age 65 <span class="citation">(Card, Dobkin, and Maestas <a href="references.html#ref-Card2008" role="doc-biblioref">2008</a>)</span>.
</p>
</div>
<p>Next the authors investigate the impact that Medicare had on access to care and utilization using the NHIS data. Since 1997, NHIS has asked four questions. They are:</p>
<blockquote>
<p>“During the past 12 months has medical care been delayed for this person because of worry about the cost?”</p>
<p>“During the past 12 months was there any time when this person needed medical care but did not get it because <em>this person</em> could not afford it?”</p>
<p>“Did the individual have at least one doctor visit in the past year?”</p>
<p>“Did the individual have one or more overnight hospital stays in the past year?”</p>
</blockquote>
<p>Estimates from this analysis are presented in Table <a href="ch5.html#tab:card-table3">6.4</a>. Each cell measures the average treatment effect for the complier population</p>
<p>at the discontinuity. Standard errors are in parentheses. There are a few encouraging findings from this table. First, the share of the relevant population who delayed care the previous year fell 1.8 points, and similar for the share who did not get care at all in the previous year. The share who saw a doctor went up slightly, as did the share who stayed at a hospital. These are not very large effects in magnitude, it is important to note, but they are relatively precisely estimated. Note that these effects differed considerably by race and ethnicity as well as education.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:card-table3">Table 6.4: </span> Measures of access to care just before 65 and estimated discontinuities at age 65</caption>
<thead><tr class="header">
<th></th>
<th align="left"><strong>Delayed last year</strong></th>
<th align="left"><strong>Did not get care last year</strong></th>
<th align="left"><strong>Saw doctor last year</strong></th>
<th align="left"><strong>Hospital stay last year</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Overall sample</td>
<td align="left"><span class="math inline">\(-1.8\)</span></td>
<td align="left"><span class="math inline">\(-1.3\)</span></td>
<td align="left">1.3</td>
<td align="left">1.2</td>
</tr>
<tr class="even">
<td></td>
<td align="left">(0.4)</td>
<td align="left">(0.3)</td>
<td align="left">(0.7)</td>
<td align="left">(0.4)</td>
</tr>
<tr class="odd">
<td><em>White non-Hispanic</em></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="even">
<td>High school dropout</td>
<td align="left"><span class="math inline">\(-1.5\)</span></td>
<td align="left"><span class="math inline">\(-0.2\)</span></td>
<td align="left">3.1</td>
<td align="left">1.6</td>
</tr>
<tr class="odd">
<td></td>
<td align="left">(1.1)</td>
<td align="left">(1.0)</td>
<td align="left">(1.3)</td>
<td align="left">(1.3)</td>
</tr>
<tr class="even">
<td>High school graduate</td>
<td align="left">0.3</td>
<td align="left"><span class="math inline">\(-1.3\)</span></td>
<td align="left"><span class="math inline">\(-0.4\)</span></td>
<td align="left">0.3</td>
</tr>
<tr class="odd">
<td></td>
<td align="left">(2.8)</td>
<td align="left">(2.8)</td>
<td align="left">(1.5)</td>
<td align="left">(0.7)</td>
</tr>
<tr class="even">
<td>Some college</td>
<td align="left"><span class="math inline">\(-1.5\)</span></td>
<td align="left"><span class="math inline">\(-1.4\)</span></td>
<td align="left">0.0</td>
<td align="left">2.1</td>
</tr>
<tr class="odd">
<td></td>
<td align="left">(0.4)</td>
<td align="left">(0.3)</td>
<td align="left">(1.3)</td>
<td align="left">(0.7)</td>
</tr>
<tr class="even">
<td><em>Minority</em></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td>High school dropout</td>
<td align="left"><span class="math inline">\(-5.3\)</span></td>
<td align="left"><span class="math inline">\(-4.2\)</span></td>
<td align="left">5.0</td>
<td align="left">0.0</td>
</tr>
<tr class="even">
<td></td>
<td align="left">(1.0)</td>
<td align="left">(0.9)</td>
<td align="left">(2.2)</td>
<td align="left">(1.4)</td>
</tr>
<tr class="odd">
<td>High school graduate</td>
<td align="left"><span class="math inline">\(-3.8\)</span></td>
<td align="left">1.5</td>
<td align="left">1.9</td>
<td align="left">1.8</td>
</tr>
<tr class="even">
<td></td>
<td align="left">(3.2)</td>
<td align="left">(3.7)</td>
<td align="left">(2.7)</td>
<td align="left">(1.4)</td>
</tr>
<tr class="odd">
<td>Some college</td>
<td align="left"><span class="math inline">\(-0.6\)</span></td>
<td align="left"><span class="math inline">\(-0.2\)</span></td>
<td align="left">3.7</td>
<td align="left">0.7</td>
</tr>
<tr class="even">
<td></td>
<td align="left">(1.1)</td>
<td align="left">(0.8)</td>
<td align="left">(3.9)</td>
<td align="left">(2.0)</td>
</tr>
<tr class="odd">
<td><em>Classified by ethnicity only</em></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="even">
<td>White non-Hispanic</td>
<td align="left"><span class="math inline">\(-1.6\)</span></td>
<td align="left"><span class="math inline">\(-1.2\)</span></td>
<td align="left">0.6</td>
<td align="left">1.3</td>
</tr>
<tr class="odd">
<td></td>
<td align="left">(0.4)</td>
<td align="left">(0.3)</td>
<td align="left">(0.8)</td>
<td align="left">(0.5)</td>
</tr>
<tr class="even">
<td>Black non-Hispanic</td>
<td align="left"><span class="math inline">\(-1.9\)</span></td>
<td align="left"><span class="math inline">\(-0.3\)</span></td>
<td align="left">3.6</td>
<td align="left">0.5</td>
</tr>
<tr class="odd">
<td></td>
<td align="left">(1.1)</td>
<td align="left">(1.1)</td>
<td align="left">(1.9)</td>
<td align="left">(1.1)</td>
</tr>
<tr class="even">
<td>Hispanic</td>
<td align="left"><span class="math inline">\(-4.9\)</span></td>
<td align="left"><span class="math inline">\(-3.8\)</span></td>
<td align="left">8.2</td>
<td align="left">11.8</td>
</tr>
<tr class="odd">
<td></td>
<td align="left">(0.8)</td>
<td align="left">(0.7)</td>
<td align="left">(0.8)</td>
<td align="left">(1.6)</td>
</tr>
</tbody>
</table></div>
<p>Having shown modest effects on care and utilization, the authors turn to examining the kinds of care they received by examining specific changes in hospitalizations. Figure <a href="ch5.html#fig:medicare5">6.13</a> shows the effect of Medicare on hip and knee replacements by race. The effects are largest for whites.</p>
<div class="figure">
<span id="fig:medicare5"></span>
<img src="graphics/cardetal2008_fig3.jpg" alt="Changes in hospitalizations [@Card2008]" width="100%"><p class="caption">
Figure 6.13: Changes in hospitalizations <span class="citation">(Card, Dobkin, and Maestas <a href="references.html#ref-Card2008" role="doc-biblioref">2008</a>)</span>
</p>
</div>
<p>In conclusion, the authors find that universal health-care coverage for the elderly increases care and utilization as well as coverage. In a subsequent study <span class="citation">(Card, Dobkin, and Maestas <a href="references.html#ref-Card2009" role="doc-biblioref">2009</a>)</span>, the authors examined the impact of Medicare on mortality and found slight decreases in mortality rates (see Table <a href="ch5.html#tab:card-table5">6.5</a>).</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:card-table5">Table 6.5: </span> Regression discontinuity estimates of changes in mortality rates</caption>
<tbody>
<tr class="odd">
<td align="left">Quadratic no controls</td>
<td align="center"><span class="math inline">\(-1.1\)</span></td>
<td align="center"><span class="math inline">\(-1.0\)</span></td>
<td align="center"><span class="math inline">\(-1.1\)</span></td>
<td align="center"><span class="math inline">\(-1.2\)</span></td>
<td align="center"><span class="math inline">\(-1.0\)</span></td>
<td></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center">(0.2)</td>
<td align="center">(0.2)</td>
<td align="center">(0.3)</td>
<td align="center">(0.3)</td>
<td align="center">(0.4)</td>
<td>(0.4)</td>
</tr>
<tr class="odd">
<td align="left">Quadratic plus controls</td>
<td align="center"><span class="math inline">\(-1.0\)</span></td>
<td align="center"><span class="math inline">\(-0.8\)</span></td>
<td align="center"><span class="math inline">\(-0.9\)</span></td>
<td align="center"><span class="math inline">\(-0.9\)</span></td>
<td align="center"><span class="math inline">\(-0.8\)</span></td>
<td><span class="math inline">\(-0.7\)</span></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center">0.2)</td>
<td align="center">(0.2)</td>
<td align="center">(0.3)</td>
<td align="center">(0.3)</td>
<td align="center">(0.3)</td>
<td>(0.4)</td>
</tr>
<tr class="odd">
<td align="left">Cubic plus controls</td>
<td align="center"><span class="math inline">\(-0.7\)</span></td>
<td align="center"><span class="math inline">\(-0.7\)</span></td>
<td align="center"><span class="math inline">\(-0.6\)</span></td>
<td align="center"><span class="math inline">\(-0.9\)</span></td>
<td align="center"><span class="math inline">\(-0.9\)</span></td>
<td><span class="math inline">\(-0.4\)</span></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center">(0.3)</td>
<td align="center">(0.2)</td>
<td align="center">(0.4)</td>
<td align="center">(0.4)</td>
<td align="center">(0.5)</td>
<td>(0.5)</td>
</tr>
<tr class="odd">
<td align="left">Local OLS with ad hoc bandwidths</td>
<td align="center"><span class="math inline">\(-0.8\)</span></td>
<td align="center"><span class="math inline">\(-0.8\)</span></td>
<td align="center"><span class="math inline">\(-0.8\)</span></td>
<td align="center"><span class="math inline">\(-0.9\)</span></td>
<td align="center"><span class="math inline">\(-1.1\)</span></td>
<td><span class="math inline">\(-0.8\)</span></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center">(0.2)</td>
<td align="center">(0.2)</td>
<td align="center">(0.2)</td>
<td align="center">(0.2)</td>
<td align="center">(0.3)</td>
<td>(0.3)</td>
</tr>
</tbody>
</table></div>
<p class="footnote">
Dependent variable for death within interval shown in the column heading. Regression estimates at the discontinuity of age 65 for flexible regression models. Standard errors in parenthesis.
</p>
</div>
<div id="inference" class="section level3" number="6.2.6">
<h3>
<span class="header-section-number">6.2.6</span> Inference<a class="anchor" aria-label="anchor" href="#inference"><i class="fas fa-link"></i></a>
</h3>
<p>As we’ve mentioned, it’s standard practice in the RDD to estimate causal effects using local polynomial regressions. In its simplest form, this amounts to nothing more complicated than fitting a linear specification separately on each side of the cutoff using a least squares regression. But when this is done, you are using only the observations within some pre-specified window (hence “local”). As the true conditional expectation function is probably not linear at this window, the resulting estimator likely suffers from specification bias. But if you can get the window narrow enough, then the bias of the estimator is probably small relative to its standard deviation.</p>
<p>But what if the window cannot be narrowed enough? This can happen if the running variable only takes on a few values, or if the gap between values closest to the cutoff are large. The result could be you simply do not have enough observations close to the cutoff for the local polynomial regression. This also can lead to the heteroskedasticity-robust confidence intervals to undercover the average causal effect because it is not centered. And here’s the really bad news—this probably is happening <em>a lot</em> in practice.</p>
<p>In a widely cited and very influential study, <span class="citation">Lee and Card (<a href="references.html#ref-Lee2008b" role="doc-biblioref">2008</a>)</span> suggested that researchers should cluster their standard errors by the running variable. This advice has since become common practice in the empirical literature. <span class="citation">Lee and Lemieux (<a href="references.html#ref-Lee2010" role="doc-biblioref">2010</a>)</span>, in a survey article on proper RDD methodology, recommend this practice, just to name one example. But in a recent study, <span class="citation">Kolesár and Rothe (<a href="references.html#ref-Kolesar2018" role="doc-biblioref">2018</a>)</span> provide extensive theoretical and simulation-based evidence that clustering on the running variable is perhaps one of the <em>worst</em> approaches you could take. In fact, clustering on the running variable can actually be substantially worse than heteroskedastic-robust standard errors.</p>
<p>As an alternative to clustering and robust standard errors, the authors propose two alternative confidence intervals that have guaranteed coverage properties under various restrictions on the conditional expectation function. Both confidence intervals are “honest,” which means they achieve correct coverage uniformly over all conditional expectation functions in large samples. These confidence intervals are currently unavailable in Stata as of the time of this writing, but they can be implemented in R with the RDHonest package.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;RDHonest is available at &lt;a href="https://github.com/kolesarm/RDHonest" class="uri"&gt;https://github.com/kolesarm/RDHonest&lt;/a&gt;.&lt;/p&gt;'><sup>102</sup></a> R users are encouraged to use these confidence intervals. Stata users are encouraged to switch (grudgingly) to R so as to use these confidence intervals. Barring that, Stata users should use the heteroskedastic robust standard errors. But whatever you do, don’t cluster on the running variable, as that is nearly an unambiguously bad idea.</p>
<p>A separate approach may be to use randomization inference. As we noted, <span class="citation">Hahn, Todd, and Klaauw (<a href="references.html#ref-Hahn2001" role="doc-biblioref">2001</a>)</span> emphasized that the conditional expected potential outcomes must be continuous across the cutoff for a regression discontinuity design to identify the local average treatment effect. But <span class="citation">Cattaneo, Frandsen, and Titiunik (<a href="references.html#ref-Cattaneo2015" role="doc-biblioref">2015</a>)</span> suggest an alternative assumption which has implications for inference. They ask us to consider that perhaps around the cutoff, in a short enough window, the treatment was assigned to units randomly. It was effectively a coin flip which side of the cutoff someone would be for a small enough window around the cutoff. Assuming there exists a neighborhood around the cutoff where this randomization-type condition holds, then this assumption may be viewed as an approximation of a randomized experiment around the cutoff. Assuming this is plausible, we can proceed as if only those observations closest to the discontinuity were randomly assigned, which leads naturally to randomization inference as a methodology for conducting exact or approximate <em>p</em>-values.</p>
</div>
<div id="the-fuzzy-rd-design" class="section level3" number="6.2.7">
<h3>
<span class="header-section-number">6.2.7</span> The Fuzzy RD Design<a class="anchor" aria-label="anchor" href="#the-fuzzy-rd-design"><i class="fas fa-link"></i></a>
</h3>
<p>In the sharp RDD, treatment was <em>determined</em> when <span class="math inline">\(X_i \geq c_0\)</span>. But that kind of deterministic assignment does not always happen. Sometimes there is a discontinuity, but it’s not entirely deterministic, though it nonetheless is associated with a discontinuity in treatment assignment. When there is an increase in the <em>probability</em> of treatment assignment, we have a <em>fuzzy</em> RDD. The earlier paper by <span class="citation">Hoekstra (<a href="references.html#ref-Hoekstra2009" role="doc-biblioref">2009</a>)</span> had this feature, as did <span class="citation">Angrist and Lavy (<a href="references.html#ref-Angrist1999b" role="doc-biblioref">1999</a>)</span>. The formal definition of a probabilistic treatment assignment is
<span class="math display">\[
\lim_{X_i\rightarrow{c_0}}
   \Pr\big(D_i=1\mid X_i=c_0\big) \ne
   \lim_{c_0 \leftarrow X_i}
   \Pr\big(D_i=1\mid X_i=c_0\big)
\]</span></p>
<p>In other words, the conditional probability is discontinuous as <span class="math inline">\(X\)</span> approaches <span class="math inline">\(c_0\)</span> in the limit. A visualization of this is presented from <span class="citation">Imbens and Lemieux (<a href="references.html#ref-Imbens2008" role="doc-biblioref">2008</a>)</span> in Figure <a href="ch5.html#fig:fuzzy1">6.14</a>.</p>
<div class="figure">
<span id="fig:fuzzy1"></span>
<img src="graphics/fuzzy1.png" alt="Vertical axis is the probability of treatment for each value of the running variable." width="100%"><p class="caption">
Figure 6.14: Vertical axis is the probability of treatment for each value of the running variable.
</p>
</div>
<p>The identifying assumptions are the same under fuzzy designs as they are under sharp designs: they are the continuity assumptions. For identification, we must assume that the conditional expectation of the potential outcomes (e.g., <span class="math inline">\(E[Y^0|X&lt;c_0]\)</span>) is changing smoothly through <span class="math inline">\(c_0\)</span>. What changes at <span class="math inline">\(c_0\)</span> is the treatment assignment probability. An illustration of this identifying assumption is in Figure <a href="ch5.html#fig:fuzzy2">6.15</a>.</p>
<div class="figure">
<span id="fig:fuzzy2"></span>
<img src="graphics/fuzzy2.png" alt="Vertical axis is the probability of treatment for each value of the running variable." width="100%"><p class="caption">
Figure 6.15: Vertical axis is the probability of treatment for each value of the running variable.
</p>
</div>
<p>Estimating some average treatment effect under a fuzzy RDD is very similar to how we estimate a local average treatment effect with instrumental variables. I will cover instrumental variables in more detail later in the book, but for now let me tell you about estimation under fuzzy designs using IV. One can estimate several ways. One simple way is a type of Wald estimator, where you estimate some causal effect as the ratio of a reduced form difference in mean outcomes around the cutoff and a reduced form difference in mean treatment assignment around the cutoff.
<span class="math display">\[
\delta_{\text{Fuzzy RDD}} = \dfrac{\lim_{X \rightarrow c_0}
       E\big[Y\mid X = c_0\big]-\lim_{X_0 \leftarrow X}
       E\big[Y\mid X=c_0\big]}{\lim_{X \rightarrow c_0}
       E\big[D\mid X=c_0\big]-\lim_{X_0 \leftarrow X}
       E\big[D\mid X=c_0\big]}
\]</span>
The assumptions for identification here are the same as with any instrumental variables design: all the caveats about exclusion restrictions, monotonicity, SUTVA, and the strength of the first stage.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;I discuss these assumptions and diagnostics in greater detail later in the chapter on instrument variables.&lt;/p&gt;"><sup>103</sup></a></p>
<p>But one can also estimate the effect using a two-stage least squares model or similar appropriate model such as limited-information maximum likelihood. Recall that there are now two events: the first event is when the running variable exceeds the cutoff, and the second event is when a unit is placed in the treatment. Let <span class="math inline">\(Z_i\)</span> be an indicator for when <span class="math inline">\(X\)</span> exceeds <span class="math inline">\(c_0\)</span>. One can use both <span class="math inline">\(Z_i\)</span> and the interaction terms as instruments for the treatment <span class="math inline">\(D_i\)</span>. If one uses only <span class="math inline">\(Z_i\)</span> as an instrumental variable, then it is a “just identified” model, which usually has good finite sample properties.</p>
<p>Let’s look at a few of the regressions that are involved in this instrumental variables approach. There are three possible regressions: the first stage, the reduced form, and the second stage. Let’s look at them in order. In the case just identified (meaning only one instrument for one endogenous variable), the first stage would be:
<span class="math display">\[
D_i = \gamma_0 + \gamma_1X_i+\gamma_2X_i^2 + \dots + \gamma_pX_i^p + \pi{Z}_i + \zeta_{1i}
\]</span>
where <span class="math inline">\(\pi\)</span> is the causal effect of <span class="math inline">\(Z_i\)</span> on the conditional probability of treatment. The fitted values from this regression would then be used in a second stage. We can also use both <span class="math inline">\(Z_i\)</span> and the interaction terms as instruments for <span class="math inline">\(D_i\)</span>. If we used <span class="math inline">\(Z_i\)</span> and all its interactions, the estimated first stage would be:
<span class="math display">\[
D_i= \gamma_{00} + \gamma_{01}\tilde{X}_i + \gamma_{02}\tilde{X}_i^2 + \dots + \gamma_{0p}\tilde{X}_i^p
   + \pi Z_i + \gamma_1^*\tilde{X}_iZ_i + \gamma_2^* \tilde{X}_i Z_i + \dots + \gamma_p^*Z_i + \zeta_{1i}
\]</span>
We would also construct analogous first stages for <span class="math inline">\(\tilde{X}_iD_i, \dots, \tilde{X}_i^pD_i\)</span>.</p>
<p>If we wanted to forgo estimating the full IV model, we might estimate the reduced form only. You’d be surprised how many applied people prefer to simply report the reduced form and not the fully specified instrumental variables model. If you read <span class="citation">Hoekstra (<a href="references.html#ref-Hoekstra2009" role="doc-biblioref">2009</a>)</span>, for instance, he favored presenting the reduced form—that second figure, in fact, was a picture of the reduced form. The reduced form would regress the outcome <span class="math inline">\(Y\)</span> onto the instrument and the running variable. The form of this fuzzy RDD reduced form is:
<span class="math display">\[
Y_i = \mu + \kappa_1X_i + \kappa_2X_i^2 + \dots + \kappa_pX_i^p + \delta \pi Z_i + \zeta_{2i}
\]</span>
As in the sharp RDD case, one can allow the smooth function to be different on both sides of the discontinuity by interacting <span class="math inline">\(Z_i\)</span> with the running variable. The reduced form for this regression is:</p>
<p><span class="math display">\[\begin{align}
   Y_i &amp; =\mu + \kappa_{01}X_i\tilde{X}_i + \kappa_{02}X_i{}\tilde{X}_i^2 + \dots + \kappa_{0p}X_i{}\tilde{X}_i^p                        
   \\
       &amp; + \delta \pi Z_i + \kappa_{01}X_i^*\tilde{X}_iZ_i + \kappa_{02}X_i^* \tilde{X}_i Z_i + \dots + \kappa_{0p}X_i^*Z_i + \zeta_{1i} 
\end{align}\]</span></p>
<p>But let’s say you wanted to present the estimated effect of the treatment on some outcome. That requires estimating a first stage, using fitted values from that regression, and then estimating a second stage on those fitted values. This, and only this, will identify the causal effect of the treatment on the outcome of interest. The reduced form only estimates the causal effect of the instrument on the outcome. The second-stage model with interaction terms would be the same as before:</p>
<p><span class="math display">\[\begin{align}
   Y_i &amp; =\alpha + \beta_{01}\tilde{x}_i + \beta_{02}\tilde{x}_i^2 + \dots + \beta_{0p}\tilde{x}_i^p                                                             
   \\
       &amp; + \delta \widehat{D_i} + \beta_1^*\widehat{D_i}\tilde{x}_i + \beta_2^*\widehat{D_i}\tilde{x}_i^2 + \dots + \beta_p^*\widehat{D_i}\tilde{x}_i^p + \eta_i 
\end{align}\]</span></p>
<p>Where <span class="math inline">\(\tilde{x}\)</span> are now not only normalized with respect to <span class="math inline">\(c_0\)</span> but are also fitted values obtained from the first-stage regressions.</p>
<p>As <span class="citation">Hahn, Todd, and Klaauw (<a href="references.html#ref-Hahn2001" role="doc-biblioref">2001</a>)</span> point out, one needs the same assumptions for identification as one needs with IV. As with other binary instrumental variables, the fuzzy RDD is estimating the local average treatment effect (LATE) <span class="citation">(Imbens and Angrist <a href="references.html#ref-Imbens1994" role="doc-biblioref">1994</a>)</span>, which is the average treatment effect for the compliers. In RDD, the compliers are those whose treatment status changed as we moved the value of <span class="math inline">\(x_i\)</span> from just to the left of <span class="math inline">\(c_0\)</span> to just to the right of <span class="math inline">\(c_0\)</span>.</p>
</div>
</div>
<div id="challenges-to-identification" class="section level2" number="6.3">
<h2>
<span class="header-section-number">6.3</span> Challenges to Identification<a class="anchor" aria-label="anchor" href="#challenges-to-identification"><i class="fas fa-link"></i></a>
</h2>
<p>The requirement for RDD to estimate a causal effect are the continuity assumptions. That is, the expected potential outcomes change smoothly as a function of the running variable through the cutoff. In words, this means that the only thing that causes the outcome to change abruptly at <span class="math inline">\(c_0\)</span> is the treatment. But, this can be violated in practice if any of the following is true:</p>
<ol style="list-style-type: decimal">
<li><p>The assignment rule is known in advance.</p></li>
<li><p>Agents are interested in adjusting.</p></li>
<li><p>Agents have time to adjust.</p></li>
<li><p>The cutoff is endogenous to factors that independently cause potential outcomes to shift.</p></li>
<li><p>There is nonrandom heaping along the running variable.</p></li>
</ol>
<p>Examples include retaking an exam, self-reporting income, and so on. But some other unobservable characteristic change could happen at the threshold, and this has a direct effect on the outcome. In other words, the cutoff is endogenous. An example would be age thresholds used for policy, such as when a person turns 18 years old and faces more severe penalties for crime. This age threshold triggers the treatment (i.e., higher penalties for crime), but is also correlated with variables that affect the outcomes, such as graduating from high school and voting rights. Let’s tackle these problems separately.</p>
<div id="mccrarys-density-test" class="section level3" number="6.3.1">
<h3>
<span class="header-section-number">6.3.1</span> McCrary’s density test<a class="anchor" aria-label="anchor" href="#mccrarys-density-test"><i class="fas fa-link"></i></a>
</h3>
<p>Because of these challenges to identification, a lot of work by econometricians and applied microeconomists has gone toward trying to figure out solutions to these problems. The most influential is a density test by Justin McCrary, now called the McCrary density test <span class="citation">(McCrary <a href="references.html#ref-Mccrary2008" role="doc-biblioref">2008</a>)</span>. The McCrary density test is used to check whether units are sorting on the running variable. Imagine that there are two rooms with patients in line for some life-saving treatment. Patients in room A will receive the life-saving treatment, and patients in room B will <em>knowingly</em> receive nothing. What would you do if you were in room B? Like me, you’d probably stand up, open the door, and walk across the hall to room A. There are natural incentives for the people in room B to get into room A, and the only thing that would keep people in room B from sorting into room A is if doing so were impossible.</p>
<p>But, let’s imagine that the people in room B had successfully sorted themselves into room A. What would that look like to an outsider? If they were successful, then room A would have more patients than room B. In fact, in the extreme, room A is crowded and room B is empty. This is the heart of the McCrary density test, and when we see such things at the cutoff, we have some suggestive evidence that people are sorting on the running variable. This is sometimes called manipulation.</p>
<p>Remember earlier when I said we should think of continuity as the null because nature doesn’t make jumps? If you see a turtle on a fencepost, it probably didn’t get there itself. Well, the same goes for the density. If the null is a continuous density through the cutoff, then bunching in the density at the cutoff is a sign that someone is moving over to the cutoff—probably to take advantage of the rewards that await there. Sorting on the sorting variable is a testable prediction under the null of a continuous density. Assuming a continuous distribution of units, sorting on the running variable means that units are moving just on the other side of the cutoff. Formally, if we assume a desirable treatment <span class="math inline">\(D\)</span> and an assignment rule <span class="math inline">\(X\geq c_0\)</span>, then we expect individuals will sort into <span class="math inline">\(D\)</span> by choosing <span class="math inline">\(X\)</span> such that <span class="math inline">\(X\geq c_0\)</span>—so long as they’re able. If they do, then it could imply selection bias insofar as their sorting is a function of potential outcomes.</p>
<p>The kind of test needed to investigate whether manipulation is occurring is a test that checks whether there is bunching of units at the cutoff. In other words, we need a <em>density test</em>. <span class="citation">McCrary (<a href="references.html#ref-Mccrary2008" role="doc-biblioref">2008</a>)</span> suggests a formal test where under the null, the density should be continuous at the cutoff point. Under the alternative hypothesis, the density should increase at the kink.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;In those situations, anyway, where the treatment is desirable to the units.&lt;/p&gt;"><sup>104</sup></a> I’ve always liked this test because it’s a really simple statistical test based on a theory that human beings are optimizing under constraints. And if they are optimizing, that makes for testable predictions—like a discontinuous jump in the density at the cutoff. Statistics built on behavioral theory can take us further.</p>
<p>To implement the McCrary density test, partition the assignment variable into bins and calculate frequencies (i.e., the number of observations) in each bin. Treat the frequency counts as the dependent variable in a local linear regression. If you can estimate the conditional expectations, then you have the data on the running variable, so in principle you can always do a density test. I recommend the package <code>rddensity</code>,<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;&lt;a href="https://sites.google.com/site/rdpackages/rddensity" class="uri"&gt;https://sites.google.com/site/rdpackages/rddensity&lt;/a&gt;.&lt;/p&gt;'><sup>105</sup></a> which you can install for R as well.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;&lt;a href="http://cran.r-project.org/web/packages/rdd/rdd.eps" class="uri"&gt;http://cran.r-project.org/web/packages/rdd/rdd.eps&lt;/a&gt;.&lt;/p&gt;'><sup>106</sup></a> These packages are based on <span class="citation">Cattaneo, Jansson, and Ma (<a href="references.html#ref-Cattaneo2019" role="doc-biblioref">2019</a>)</span>, which is based on local polynomial regressions that have less bias in the border regions.</p>
<p>This is a high-powered test. You need a lot of observations at <span class="math inline">\(c_0\)</span> to distinguish a discontinuity in the density from noise. Let me illustrate in Figure <a href="ch5.html#fig:mccrary1">6.16</a> with a picture from <span class="citation">McCrary (<a href="references.html#ref-Mccrary2008" role="doc-biblioref">2008</a>)</span> that shows a situation with and without manipulation.</p>
<div class="figure">
<span id="fig:mccrary1"></span>
<img src="graphics/mccrary_density_test.jpg" alt="A picture with and without a discontinuity in the density from @Mccrary2008." width="100%"><p class="caption">
Figure 6.16: A picture with and without a discontinuity in the density from <span class="citation">McCrary (<a href="references.html#ref-Mccrary2008" role="doc-biblioref">2008</a>)</span>.
</p>
</div>
</div>
<div id="covariate-balance-and-other-placebos" class="section level3" number="6.3.2">
<h3>
<span class="header-section-number">6.3.2</span> Covariate balance and other placebos<a class="anchor" aria-label="anchor" href="#covariate-balance-and-other-placebos"><i class="fas fa-link"></i></a>
</h3>
<p>It has become common in this literature to provide evidence for the credibility of the underlying identifying assumptions, at least to some degree. While the assumptions cannot be directly tested, indirect evidence may be persuasive. I’ve already mentioned one such test—the McCrary density test. A second test is a covariate balance test. For RDD to be valid in your study, there must not be an observable discontinuous change in the average values of reasonably chosen covariates around the cutoff. As these are pretreatment characteristics, they should be invariant to change in treatment assignment. An example of this is from <span class="citation">Lee, Moretti, and Butler (<a href="references.html#ref-lmb2004" role="doc-biblioref">2004</a>)</span>, who evaluated the impact of Democratic vote share just at 50%, on various demographic factors (Figure <a href="ch5.html#fig:lmb1">6.17</a>).</p>
<div class="figure">
<span id="fig:lmb1"></span>
<img src="graphics/lee_moretti_butler_fig3.jpg" alt="Panels refer to (top left to bottom right) district characteristics: real income, percent high school degree, percent black, and percent eligible to vote. Circles represent the average characteristic within intervals of 0.01 in Democratic vote share. The continuous line represents the predicted values from a fourth-order polynomial in vote share fitted separately for points above and below the 50 percent threshold. The dotted line represents the 95 percent confidence interval. Reprinted from @lmb2004." width="100%"><p class="caption">
Figure 6.17: Panels refer to (top left to bottom right) district characteristics: real income, percent high school degree, percent black, and percent eligible to vote. Circles represent the average characteristic within intervals of 0.01 in Democratic vote share. The continuous line represents the predicted values from a fourth-order polynomial in vote share fitted separately for points above and below the 50 percent threshold. The dotted line represents the 95 percent confidence interval. Reprinted from <span class="citation">Lee, Moretti, and Butler (<a href="references.html#ref-lmb2004" role="doc-biblioref">2004</a>)</span>.
</p>
</div>
<p>This test is basically what is sometimes called a <em>placebo</em> test. That is, you are looking for there to be no effects where there shouldn’t be any. So a third kind of test is an extension of that—just as there shouldn’t be effects at the cutoff on pretreatment values, there shouldn’t be effects on the outcome of interest at arbitrarily chosen cutoffs. <span class="citation">Imbens and Lemieux (<a href="references.html#ref-Imbens2008" role="doc-biblioref">2008</a>)</span> suggest looking at one side of the discontinuity, taking the median value of the running variable in that section, and pretending it was a discontinuity, <span class="math inline">\(c_0'\)</span>. Then test whether there is a discontinuity in the outcome at <span class="math inline">\(c_0'\)</span>. You do <em>not</em> want to find anything.</p>
</div>
<div id="nonrandom-heaping-on-the-running-variable" class="section level3" number="6.3.3">
<h3>
<span class="header-section-number">6.3.3</span> Nonrandom heaping on the running variable<a class="anchor" aria-label="anchor" href="#nonrandom-heaping-on-the-running-variable"><i class="fas fa-link"></i></a>
</h3>
<p><span class="citation">Almond et al. (<a href="references.html#ref-Almond2010" role="doc-biblioref">2010</a>)</span> is a fascinating study. The authors are interested in estimating the causal effect of medical expenditures on health outcomes, in part because many medical technologies, while effective, may not justify the costs associated with their use. Determining their effectiveness is challenging given that medical resources are, we hope, optimally assigned to patients based on patient potential outcomes. To put it a different way, if the physician perceives that an intervention will have the best outcome, then that is likely a treatment that will be assigned to the patient. This violates independence, and more than likely, if the endogeneity of the treatment is deep enough, controlling for selection directly will be tough, if not impossible. As we saw with our earlier example of the perfect doctor, such nonrandom assignment of interventions can lead to confusing correlations. Counterintuitive correlations may be nothing more than selection bias.</p>
<p>But <span class="citation">Almond et al. (<a href="references.html#ref-Almond2010" role="doc-biblioref">2010</a>)</span> had an ingenious insight—in the United States, it is typically the case that babies with a very low birth weight receive heightened medical attention. This categorization is called the “very low birth weight” range, and such low birth weight is quite dangerous for the child. Using administrative hospital records linked to mortality data, the authors find that the 1-year infant mortality decreases by around 1 percentage point when the child’s birth weight is just below the 1,500-gram threshold compared to those born just above. Given the mean 1-year mortality of 5.5%, this estimate is sizable, suggesting that the medical interventions triggered by the very-low-birth-weight classification have benefits that far exceed their costs.</p>
<p><span class="citation">Barreca et al. (<a href="references.html#ref-Barreca2011" role="doc-biblioref">2011</a>)</span> and <span class="citation">Barreca, Lindo, and Waddell (<a href="references.html#ref-Barreca2016" role="doc-biblioref">2016</a>)</span> highlight some of econometric issues related to what they call “heaping” on the running variable. Heaping is when there is an excess number of units at certain points along the running variable. In this case, it appeared to be at regular 100-gram intervals and was likely caused by a tendency for hospitals to round to the nearest integer. A visualization of this problem can be seen in the original <span class="citation">Almond et al. (<a href="references.html#ref-Almond2010" role="doc-biblioref">2010</a>)</span>, which I reproduce here in Figure <a href="ch5.html#fig:heaping">6.18</a>. The long black lines appearing regularly across the birth-weight distribution are excess mass of children born at those numbers. This sort of event is unlikely to occur naturally in nature, and it is almost certainly caused by either sorting or rounding. It could be due to less sophisticated scales or, more troubling, to staff rounding a child’s birth weight to 1,500 grams in order to make the child eligible for increased medical attention.</p>
<div class="figure">
<span id="fig:heaping"></span>
<img src="graphics/heaping.jpg" alt="Distribution of births by gram. Reprinted from @Almond2010." width="100%"><p class="caption">
Figure 6.18: Distribution of births by gram. Reprinted from <span class="citation">Almond et al. (<a href="references.html#ref-Almond2010" role="doc-biblioref">2010</a>)</span>.
</p>
</div>
<p><span class="citation">Almond et al. (<a href="references.html#ref-Almond2010" role="doc-biblioref">2010</a>)</span> attempt to study this more carefully using the conventional McCrary density test and find no clear, statistically significant evidence for sorting on the running variable at the 1,500-gram cutoff. Satisfied, they conduct their main analysis, in which they find a causal effect of around a 1-percentage-point reduction in 1-year mortality.</p>
<p>The focus of <span class="citation">Barreca et al. (<a href="references.html#ref-Barreca2011" role="doc-biblioref">2011</a>)</span> and <span class="citation">Barreca, Lindo, and Waddell (<a href="references.html#ref-Barreca2016" role="doc-biblioref">2016</a>)</span> is very much on the heaping phenomenon shown in Figure <a href="ch5.html#fig:heaping">6.18</a>. Part of the strength of their work, though, is their illustration of some of the shortcomings of a conventional McCrary density test. In this case, the data heap at 1,500 grams appears to be babies whose mortality rates are unusually high. These children are outliers compared to units to <em>both</em> the immediate left and the immediate right. It is important to note that such events would not occur naturally; there is no reason to believe that nature would produce heaps of children born with outlier health defects every 100 grams. The authors comment on what might be going on:</p>
<blockquote>
<p>This heaping at 1,500 grams may be a signal that poor-quality hospitals have relatively high propensities to round birth weights but is also consistent with manipulation of recorded birth weights by doctors, nurses, or parents to obtain favorable treatment for their children. <span class="citation">Barreca et al. (<a href="references.html#ref-Barreca2011" role="doc-biblioref">2011</a>)</span> show that this nonrandom heaping leads one to conclude that it is “good” to be strictly less than any 100-g cutoff between 1,000 and 3,000 grams.</p>
</blockquote>
<p>Since estimation in an RDD compares means as we approach the threshold from either side, the estimates should not be sensitive to the observations at the thresholds itself. Their solution is a so-called “donut hole” RDD, wherein they remove units in the vicinity of 1,500 grams and reestimate the model. Insofar as units are dropped, the parameter we are estimating at the cutoff has become an even more unusual type of local average treatment effect that may be even less informative about the average treatment effects that policy makers are desperate to know. But the strength of this rule is that it allows for the possibility that units at the heap differ markedly due to selection bias from those in the surrounding area. Dropping these units reduces the sample size by around 2% but has very large effects on 1-year mortality, which is approximately 50% lower than what was found by <span class="citation">Almond et al. (<a href="references.html#ref-Almond2010" role="doc-biblioref">2010</a>)</span>.</p>
<p>These companion papers help us better understand some of the ways in which selection bias can creep into the RDD. Heaping is not the end of the world, which is good news for researchers facing such a problem. The donut hole RDD can be used to circumvent some of the problems. But ultimately this solution involves dropping observations, and insofar as your sample size is small relative to the number of heaping units, the donut hole approach could be infeasible. It also changes the parameter of interest to be estimated in ways that may be difficult to understand or explain. Caution with nonrandom heaping along the running variable is probably a good thing.</p>
</div>
</div>
<div id="replicating-a-popular-design-the-close-election" class="section level2" number="6.4">
<h2>
<span class="header-section-number">6.4</span> Replicating a Popular Design: The Close Election<a class="anchor" aria-label="anchor" href="#replicating-a-popular-design-the-close-election"><i class="fas fa-link"></i></a>
</h2>
<p>Within RDD, there is a particular kind of design that has become quite popular, the close-election design. Essentially, this design exploits a feature of American democracies wherein winners in political races are declared when a candidate gets the minimum needed share of votes. Insofar as very close races represent exogenous assignments of a party’s victory, which I’ll discuss below, then we can use these close elections to identify the causal effect of the winner on a variety of outcomes. We may also be able to test political economy theories that are otherwise nearly impossible to evaluate.</p>
<p>The following section has two goals. First, to discuss in detail the close election design using the classic <span class="citation">Lee, Moretti, and Butler (<a href="references.html#ref-lmb2004" role="doc-biblioref">2004</a>)</span>. Second, to show how to implement the close-election design by replicating several parts of <span class="citation">Lee, Moretti, and Butler (<a href="references.html#ref-lmb2004" role="doc-biblioref">2004</a>)</span>.</p>
<p><em>Do Politicians or Voters Pick Policies?</em> The big question motivating Lee et al. (2004) has to do with whether and in which way voters affect policy. There are two fundamentally different views of the role of elections in a representative democracy: convergence theory and divergence theory.</p>
<p>The convergence theory states that heterogeneous voter ideology forces each candidate to moderate his or her position (e.g., similar to the median voter theorem):</p>
<blockquote>
<p>Competition for votes can force even the most partisan Republicans and Democrats to moderate their policy choices. In the extreme case, competition may be so strong that it leads to “full policy convergence”: opposing parties are forced to adopt identical policies. <span class="citation">Lee, Moretti, and Butler (<a href="references.html#ref-lmb2004" role="doc-biblioref">2004</a>)</span> (p.87)</p>
</blockquote>
<p>Divergence theory is a slightly more commonsense view of political actors. When partisan politicians cannot credibly commit to certain policies, then convergence is undermined and the result can be full policy “divergence.” Divergence is when the winning candidate, after taking office, simply pursues her most-preferred policy. In this extreme case, voters are unable to compel candidates to reach any kind of policy compromise, and this is expressed as two opposing candidates choosing very different policies under different counterfactual victory scenarios.</p>
<p><span class="citation">Lee, Moretti, and Butler (<a href="references.html#ref-lmb2004" role="doc-biblioref">2004</a>)</span> present a model, which I’ve simplified. Let <span class="math inline">\(R\)</span> and <span class="math inline">\(D\)</span> be candidates in a congressional race. The policy space is a single dimension where <span class="math inline">\(D\)</span>’s and <span class="math inline">\(R\)</span>’s policy preferences in a period are quadratic loss functions, <span class="math inline">\(u(l)\)</span> and <span class="math inline">\(v(l)\)</span>, and <span class="math inline">\(l\)</span> is the policy variable. Each player has some bliss point, which is his or her most preferred location along the unidimensional policy range. For Democrats, it’s <span class="math inline">\(l^*=c(&gt;0)\)</span>, and for Republicans it’s <span class="math inline">\(l*=0\)</span>. Here’s what this means.</p>
<p>Ex ante, voters expect the candidate to choose some policy and they expect the candidate to win with probability <span class="math inline">\(P(x^e,y^e)\)</span>, where <span class="math inline">\(x^e\)</span> and <span class="math inline">\(y^e\)</span> are the policies chosen by Democrats and Republicans, respectively. When <span class="math inline">\(x^&gt;y^e\)</span>, then <span class="math inline">\(\dfrac{\partial P}{\partial x^e}&gt;0, \dfrac{\partial P}{\partial y^e}&lt;0\)</span>.</p>
<p><span class="math inline">\(P^*\)</span> represents the underlying popularity of the Democratic Party, or put differently, the probability that <span class="math inline">\(D\)</span> would win if the policy chosen <span class="math inline">\(x\)</span> equaled the Democrat’s bliss point <span class="math inline">\(c\)</span>.</p>
<p>The solution to this game has multiple Nash equilibria, which I discuss now.</p>
<ol style="list-style-type: decimal">
<li><p>Partial/complete convergence: Voters affect policies.</p></li>
<li><p>The key result under this equilibrium is <span class="math inline">\(\dfrac{\partial x^*}{\partial P^*}&gt;0\)</span>.</p></li>
<li><p>Interpretation: If we dropped more Democrats into the district from a helicopter, it would exogenously increase <span class="math inline">\(P^*\)</span> and this would result in candidates changing their policy positions, i.e., <span class="math inline">\(\dfrac{\partial x^*}{\partial P^*}&gt;0\)</span>.</p></li>
<li><p>Complete divergence: Voters elect politicians with fixed policies who do whatever they want to do.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;The honey badger doesn’t care. It takes what it wants. See &lt;a href="https://www.youtube.com/watch?v=4r7wHMg5Yjg" class="uri"&gt;https://www.youtube.com/watch?v=4r7wHMg5Yjg&lt;/a&gt;.&lt;/p&gt;'><sup>107</sup></a></p></li>
<li><p>Key result is that more popularity has no effect on policies. That is, <span class="math inline">\(\dfrac{\partial x^*}{\partial P^*}=0\)</span>.</p></li>
<li><p>An exogenous shock to <span class="math inline">\(P^*\)</span> (i.e., dropping Democrats into the district) does <em>nothing</em> to equilibrium policies. Voters elect politicians who then do whatever they want because of their fixed policy preferences.</p></li>
</ol>
<p>The potential roll-call voting record outcomes of the candidate following some election is
<span class="math display">\[
RC_t = D_tx_t + (1-D_t)y_t
\]</span>
where <span class="math inline">\(D_t\)</span> indicates whether a Democrat won the election. That is, only the winning candidate’s policy is observed. This expression can be transformed into regression equations:</p>
<p><span class="math display">\[\begin{align}
   RC_t     &amp; = \alpha_0+\pi_0 P_t^*+\pi_1D_t+\varepsilon_t                   
   \\
   RC_{t+1} &amp; = \beta_0 + \pi_0 P^*_{t+1} + \pi_1 D_{t+1} + \varepsilon_{t+1} 
\end{align}\]</span></p>
<p>where <span class="math inline">\(\alpha_0\)</span> and <span class="math inline">\(\beta_0\)</span> are constants.</p>
<p>This equation can’t be directly estimated because we never observe <span class="math inline">\(P^*\)</span>. But suppose we could randomize <span class="math inline">\(D_t\)</span>. Then <span class="math inline">\(D_t\)</span> would be independent of <span class="math inline">\(P^*_t\)</span> and <span class="math inline">\(\varepsilon_t\)</span>. Then taking conditional expectations with respect to <span class="math inline">\(D_t\)</span>, we get:</p>
<p><span class="math display">\[\begin{align}
\small
\underbrace{E\big[RC_{t+1}\mid D_t=1\big] - E\big[RC_{t+1}\mid D_t=0\big]}_{ \text{Observable}} &amp;= \pi_0\big[P^{*D}_{t+1} - P^{*R}_{t+1}\big] \\
&amp;+ \underbrace{\pi_1\big[P^D_{t+1}-P^R_{t+1}\big]}_{\text{Observable}} \\
&amp;=\underbrace{\gamma}_{ \text{Total effect of initial win on future roll call votes}} \\
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\underbrace{E\big[RC_{t}\mid D_t=1\big] - E\big[RC_{t}\mid D_t=0\big]}_{ \text{Observable}} = \pi_1 \\
\underbrace{E\big[D_{t+1}\mid D_t=1\big] - E\big[D_{t+1}\mid D_t=0\big]}_{\text{Observable}} = P_{t+1}^D - P_{t+1}^R
\end{align}\]</span></p>
<p>The “elect” component is <span class="math inline">\(\pi_1[P_{t+1}^D - P_{t+1}^R]\)</span> and is estimated as the difference in mean voting records between the parties at time <span class="math inline">\(t\)</span>. The fraction of districts won by Democrats in <span class="math inline">\(t+1\)</span> is an estimate of <span class="math inline">\([P_{t+1}^D - P_{t+1}^R]\)</span>. Because we can estimate the total effect, <span class="math inline">\(\gamma\)</span>, of a Democrat victory in <span class="math inline">\(t\)</span> on <span class="math inline">\(RC_{t+1}\)</span>, we can net out the elect component to implicitly get the “effect” component.</p>
<p>But random assignment of <span class="math inline">\(D_t\)</span> is crucial. For without it, this equation would reflect <span class="math inline">\(\pi_1\)</span> <em>and</em> selection (i.e., Democratic districts have more liberal bliss points). So the authors aim to randomize <span class="math inline">\(D_t\)</span> using a RDD, which I’ll now discuss in detail.</p>
<div class="cover-box">
<div class="row">
    <div class="col-xs-8 col-md-4 cover-img">
        <a href="https://www.amazon.com/dp/0300251688"><img src="images/cover.jpg" alt="Buy Today!"></a>
    </div>
    
    <div class="col-xs-12 col-md-8 cover-text-box">
            <h2> 
                Causal Inference: 
                <br><span style="font-style: italic; font-weight:bold; font-size: 20px;">The Mixtape.</span>
            </h2> 
            
        <div class="cover-text">
            <p>Buy the print version today:</p>
            
            <div class="chips">
                <a href="https://www.amazon.com/dp/0300251688" class="app-chip"> 
                    <i class="fab fa-amazon" aria-hidden="true"></i> Buy from Amazon 
                </a>
    
                <a href="https://yalebooks.yale.edu/book/9780300251685/causal-inference" class="app-chip"> 
                    <i class="fas fa-book" aria-hidden="true"></i> Buy from Yale Press 
                </a>
            </div>
        </div>
    </div>
</div>
</div>
<div id="replication-exercise" class="section level3" number="6.4.1">
<h3>
<span class="header-section-number">6.4.1</span> Replication exercise<a class="anchor" aria-label="anchor" href="#replication-exercise"><i class="fas fa-link"></i></a>
</h3>
<p>There are two main data sets in this project. The first is a measure of how liberal an official voted. This is collected from the Americans for Democratic Action (ADA) linked with House of Representatives election results for 1946–1995. Authors use the ADA score for all US House representatives from 1946 to 1995 as their voting record index. For each Congress, the ADA chose about twenty-five high-profile roll-call votes and created an index varying from 0 to 100 for each representative. Higher scores correspond to a more “liberal” voting record. The running variable in this study is the vote share. That is the share of all votes that went to a Democrat. ADA scores are then linked to election returns data during that period.</p>
<p>Recall that we need randomization of <span class="math inline">\(D_t\)</span>. The authors have a clever solution. They will use arguably exogenous variation in Democratic wins to check whether convergence or divergence is correct. If convergence is true, then Republicans and Democrats who just barely won should vote almost identically, whereas if divergence is true, they should vote differently at the margins of a close race. This “at the margins of a close race” is crucial because the idea is that it is at the margins of a close race that the distribution of voter preferences is the same. And if voter preferences are the same, but policies diverge at the cutoff, then it suggests politicians and not voters are driving policy making.</p>
<p>The exogenous shock comes from the discontinuity in the running variable. At a vote share of just above 0.5, the Democratic candidate wins. They argue that just around that cutoff, random chance determined the Democratic win—hence the random assignment of <span class="math inline">\(D_t\)</span> <span class="citation">(Cattaneo, Frandsen, and Titiunik <a href="references.html#ref-Cattaneo2015" role="doc-biblioref">2015</a>)</span>. Table <a href="ch5.html#tab:lmboriginal1">6.6</a> is a reproduction of Cattaneo et al.’s main results. The effect of a Democratic victory increases liberal voting by 21 points in the next period, 48 points in the current period, and the probability of reelection by 48%. The authors find evidence for both divergence and incumbency advantage using this design. Let’s dig into the data ourselves now and see if we can find where the authors are getting these results. We will examine the results around Table <a href="ch5.html#tab:lmboriginal1">6.6</a> by playing around with the data and different specifications.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:lmboriginal1">Table 6.6: </span> Original results based on ADA Scores – Close Elections Sample</caption>
<thead><tr class="header">
<th align="left"><strong>Dependent variable</strong></th>
<th align="center"><span class="math inline">\(ADA_{t+1}\)</span></th>
<th align="center"><span class="math inline">\(ADA_{t}\)</span></th>
<th align="center"><span class="math inline">\(DEM_{t+1}\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Estimated gap</td>
<td align="center">21.2</td>
<td align="center">47.6</td>
<td align="center">0.48</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center">(1.9)</td>
<td align="center">(1.3)</td>
<td align="center">(0.02)</td>
</tr>
</tbody>
</table></div>
<p class="footnote">
Standard errors in parentheses. The unit of observation is a district-congressional session. The sample includes only observations where the Democrat vote share at time <span class="math inline">\(t\)</span> is strictly between 48 percent and 52 percent. The estimated gap is the difference in the average of the relevant variable for observations for which the Democrat vote share at time <span class="math inline">\(t\)</span> is strictly between 50 percent and 52 percent and observations for which the Democrat vote share at time <span class="math inline">\(t\)</span> is strictly between 48 percent and 50 percent. Time <span class="math inline">\(t\)</span> and <span class="math inline">\(t+1\)</span> refer to congressional sessions. <span class="math inline">\(ADA_t\)</span> is the adjusted ADA voting score. Higher ADA scores correspond to more liberal roll-call voting records. Sample size is 915
</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/lmb_1.do"><code>lmb_1.do</code></a></em></p>
<div class="sourceCode" id="cb59"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb59-1"><a href="ch5.html#cb59-1" aria-hidden="true"></a><span class="kw">use</span> https:<span class="co">//github.com/scunning1975/mixtape/raw/master/lmb-data.dta, clear</span></span>
<span id="cb59-2"><a href="ch5.html#cb59-2" aria-hidden="true"></a></span>
<span id="cb59-3"><a href="ch5.html#cb59-3" aria-hidden="true"></a>* Replicating Table 1 <span class="kw">of</span> Lee, Moretti and Butler (2004)</span>
<span id="cb59-4"><a href="ch5.html#cb59-4" aria-hidden="true"></a><span class="kw">reg</span> <span class="kw">score</span> lagdemocrat    <span class="kw">if</span> lagdemvoteshare&gt;.48 &amp; lagdemvoteshare&lt;.52, <span class="kw">cluster</span>(id)</span>
<span id="cb59-5"><a href="ch5.html#cb59-5" aria-hidden="true"></a><span class="kw">reg</span> <span class="kw">score</span> democrat       <span class="kw">if</span> lagdemvoteshare&gt;.48 &amp; lagdemvoteshare&lt;.52, <span class="kw">cluster</span>(id)</span>
<span id="cb59-6"><a href="ch5.html#cb59-6" aria-hidden="true"></a><span class="kw">reg</span> democrat lagdemocrat <span class="kw">if</span> lagdemvoteshare&gt;.48 &amp; lagdemvoteshare&lt;.52, <span class="kw">cluster</span>(id)</span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/lmb_1.R"><code>lmb_1.R</code></a></em></p>
<div class="sourceCode" id="cb60"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://haven.tidyverse.org">haven</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">estimatr</span><span class="op">)</span>

<span class="va">read_data</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">df</span><span class="op">)</span>
<span class="op">{</span>
  <span class="va">full_path</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"https://raw.github.com/scunning1975/mixtape/master/"</span>, 
                     <span class="va">df</span>, sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span>
  <span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/read_dta.html">read_dta</a></span><span class="op">(</span><span class="va">full_path</span><span class="op">)</span>
  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">df</span><span class="op">)</span>
<span class="op">}</span>

<span class="va">lmb_data</span> <span class="op">&lt;-</span> <span class="fu">read_data</span><span class="op">(</span><span class="st">"lmb-data.dta"</span><span class="op">)</span>

<span class="va">lmb_subset</span> <span class="op">&lt;-</span> <span class="va">lmb_data</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">lagdemvoteshare</span><span class="op">&gt;</span><span class="fl">.48</span> <span class="op">&amp;</span> <span class="va">lagdemvoteshare</span><span class="op">&lt;</span><span class="fl">.52</span><span class="op">)</span>

<span class="va">lm_1</span> <span class="op">&lt;-</span> <span class="fu">lm_robust</span><span class="op">(</span><span class="va">score</span> <span class="op">~</span> <span class="va">lagdemocrat</span>, data <span class="op">=</span> <span class="va">lmb_subset</span>, clusters <span class="op">=</span> <span class="va">id</span><span class="op">)</span>
<span class="va">lm_2</span> <span class="op">&lt;-</span> <span class="fu">lm_robust</span><span class="op">(</span><span class="va">score</span> <span class="op">~</span> <span class="va">democrat</span>, data <span class="op">=</span> <span class="va">lmb_subset</span>, clusters <span class="op">=</span> <span class="va">id</span><span class="op">)</span>
<span class="va">lm_3</span> <span class="op">&lt;-</span> <span class="fu">lm_robust</span><span class="op">(</span><span class="va">democrat</span> <span class="op">~</span> <span class="va">lagdemocrat</span>, data <span class="op">=</span> <span class="va">lmb_subset</span>, clusters <span class="op">=</span> <span class="va">id</span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm_1</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm_2</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm_3</span><span class="op">)</span></code></pre></div>
<p>We reproduce regression results from Lee, Moretti, and Butler in Table 46. While the results are close to Lee, Moretti, and Butler’s original table, they are slightly different. But ignore that for now. The main thing to see is that we used regressions limited to the window right around the cutoff to estimate the effect. These are local regressions in the sense that they use data close to the cutoff. Notice the window we chose—we are only using observations between 0.48 and 0.52 vote share. So this regression is estimating the coefficient on <span class="math inline">\(D_t\)</span> right around the cutoff. What happens if we use all the data?</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/lmb_2.do"><code>lmb_2.do</code></a></em></p>
<div class="sourceCode" id="cb61"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb61-1"><a href="ch5.html#cb61-1" aria-hidden="true"></a>* Use <span class="ot">all</span> the <span class="kw">data</span></span>
<span id="cb61-2"><a href="ch5.html#cb61-2" aria-hidden="true"></a><span class="kw">reg</span> <span class="kw">score</span> lagdemocrat, <span class="kw">cluster</span>(id)</span>
<span id="cb61-3"><a href="ch5.html#cb61-3" aria-hidden="true"></a><span class="kw">reg</span> <span class="kw">score</span> democrat, <span class="kw">cluster</span>(id)</span>
<span id="cb61-4"><a href="ch5.html#cb61-4" aria-hidden="true"></a><span class="kw">reg</span> democrat lagdemocrat, <span class="kw">cluster</span>(id)</span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/lmb_2.R"><code>lmb_2.R</code></a></em></p>
<div class="sourceCode" id="cb62"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">#using all data (note data used is lmb_data, not lmb_subset)</span>

<span class="va">lm_1</span> <span class="op">&lt;-</span> <span class="fu">lm_robust</span><span class="op">(</span><span class="va">score</span> <span class="op">~</span> <span class="va">lagdemocrat</span>, data <span class="op">=</span> <span class="va">lmb_data</span>, clusters <span class="op">=</span> <span class="va">id</span><span class="op">)</span>
<span class="va">lm_2</span> <span class="op">&lt;-</span> <span class="fu">lm_robust</span><span class="op">(</span><span class="va">score</span> <span class="op">~</span> <span class="va">democrat</span>, data <span class="op">=</span> <span class="va">lmb_data</span>, clusters <span class="op">=</span> <span class="va">id</span><span class="op">)</span>
<span class="va">lm_3</span> <span class="op">&lt;-</span> <span class="fu">lm_robust</span><span class="op">(</span><span class="va">democrat</span> <span class="op">~</span> <span class="va">lagdemocrat</span>, data <span class="op">=</span> <span class="va">lmb_data</span>, clusters <span class="op">=</span> <span class="va">id</span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm_1</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm_2</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm_3</span><span class="op">)</span></code></pre></div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:lmbtable2">Table 6.7: </span> Results based on ADA Scores – Full Sample</caption>
<thead><tr class="header">
<th align="left"><strong>Dependent variable</strong></th>
<th align="center"><span class="math inline">\(ADA_{t+1}\)</span></th>
<th align="center"><span class="math inline">\(ADA_{t}\)</span></th>
<th align="center"><span class="math inline">\(DEM_{t+1}\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Estimated gap</td>
<td align="center">31.50<span class="math inline">\(^{***}\)</span>
</td>
<td align="center">40.76<span class="math inline">\(^{***}\)</span>
</td>
<td align="center">0.82<span class="math inline">\(^{***}\)</span>
</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center">(0.48)</td>
<td align="center">(0.42)</td>
<td align="center">(0.01)</td>
</tr>
<tr class="odd">
<td align="left">N</td>
<td align="center">13,588</td>
<td align="center">13,588</td>
<td align="center">13,588</td>
</tr>
</tbody>
</table></div>
<p class="footnote">
Cluster robust standard errors in parenthesis. <span class="math inline">\(^{*}\)</span> p&lt;0.10, <span class="math inline">\(^{**}\)</span> p&lt;0.05, <span class="math inline">\(^{***}\)</span> p&lt;0.01
</p>
<p>Notice that when we use all of the data, we get somewhat different effects (Table<a href="ch5.html#tab:lmbtable2">6.7</a>). The effect on future ADA scores gets larger by 10 points, but the contemporaneous effect gets smaller. The effect on incumbency, though, increases considerably. So here we see that simply running the regression yields different estimates when we include data far from the cutoff itself.</p>
<p>Neither of these regressions included controls for the running variable though. It also doesn’t use the recentering of the running variable. So let’s do both. We will simply subtract 0.5 from the running variable so that values of 0 are where the vote share equals 0.5, negative values are Democratic vote shares less than 0.5, and positive values are Democratic vote shares above 0.5. To do this, type in the following lines:</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/lmb_3.do"><code>lmb_3.do</code></a></em></p>
<div class="sourceCode" id="cb63"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb63-1"><a href="ch5.html#cb63-1" aria-hidden="true"></a>* Re-<span class="bn">center</span> the running <span class="kw">variable</span> (voteshare)</span>
<span id="cb63-2"><a href="ch5.html#cb63-2" aria-hidden="true"></a><span class="kw">gen</span> demvoteshare_c = demvoteshare - 0.5</span>
<span id="cb63-3"><a href="ch5.html#cb63-3" aria-hidden="true"></a><span class="kw">reg</span> <span class="kw">score</span> lagdemocrat demvoteshare_c, <span class="kw">cluster</span>(id)</span>
<span id="cb63-4"><a href="ch5.html#cb63-4" aria-hidden="true"></a><span class="kw">reg</span> <span class="kw">score</span> democrat demvoteshare_c, <span class="kw">cluster</span>(id)</span>
<span id="cb63-5"><a href="ch5.html#cb63-5" aria-hidden="true"></a><span class="kw">reg</span> democrat lagdemocrat demvoteshare_c, <span class="kw">cluster</span>(id)</span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/lmb_3.R"><code>lmb_3.R</code></a></em></p>
<div class="sourceCode" id="cb64"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">lmb_data</span> <span class="op">&lt;-</span> <span class="va">lmb_data</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>demvoteshare_c <span class="op">=</span> <span class="va">demvoteshare</span> <span class="op">-</span> <span class="fl">0.5</span><span class="op">)</span>

<span class="va">lm_1</span> <span class="op">&lt;-</span> <span class="fu">lm_robust</span><span class="op">(</span><span class="va">score</span> <span class="op">~</span> <span class="va">lagdemocrat</span> <span class="op">+</span> <span class="va">demvoteshare_c</span>, data <span class="op">=</span> <span class="va">lmb_data</span>, clusters <span class="op">=</span> <span class="va">id</span><span class="op">)</span>
<span class="va">lm_2</span> <span class="op">&lt;-</span> <span class="fu">lm_robust</span><span class="op">(</span><span class="va">score</span> <span class="op">~</span> <span class="va">democrat</span> <span class="op">+</span> <span class="va">demvoteshare_c</span>, data <span class="op">=</span> <span class="va">lmb_data</span>, clusters <span class="op">=</span> <span class="va">id</span><span class="op">)</span>
<span class="va">lm_3</span> <span class="op">&lt;-</span> <span class="fu">lm_robust</span><span class="op">(</span><span class="va">democrat</span> <span class="op">~</span> <span class="va">lagdemocrat</span> <span class="op">+</span> <span class="va">demvoteshare_c</span>, data <span class="op">=</span> <span class="va">lmb_data</span>, clusters <span class="op">=</span> <span class="va">id</span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm_1</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm_2</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm_3</span><span class="op">)</span></code></pre></div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:lmbtable3">Table 6.8: </span> Results based on ADA Scores – Full Sample</caption>
<thead><tr class="header">
<th align="left"><strong>Dependent variable</strong></th>
<th align="center"><span class="math inline">\(ADA_{t+1}\)</span></th>
<th align="center"><span class="math inline">\(ADA_{t}\)</span></th>
<th align="center"><span class="math inline">\(DEM_{t+1}\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Estimated gap</td>
<td align="center">33.45<span class="math inline">\(^{***}\)</span>
</td>
<td align="center">58.50<span class="math inline">\(^{***}\)</span>
</td>
<td align="center">0.55<span class="math inline">\(^{**}\)</span>
</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center">(0.85)</td>
<td align="center">(0.66)</td>
<td align="center">(0.01)</td>
</tr>
<tr class="odd">
<td align="left">N</td>
<td align="center">13,577</td>
<td align="center">13,577</td>
<td align="center">13,577</td>
</tr>
</tbody>
</table></div>
<p class="footnote">
Cluster robust standard errors in parenthesis. <span class="math inline">\(^{*}\)</span> <span class="math inline">\(p&lt;0.10\)</span>, <span class="math inline">\(^{**}\)</span> <span class="math inline">\(p&lt;0.05\)</span>, <span class="math inline">\(^{***}\)</span> <span class="math inline">\(p&lt;0.01\)</span>
</p>
<p>We report our analysis from the programming in Table <a href="ch5.html#tab:lmbtable3">6.8</a>. While the incumbency effect falls closer to what <span class="citation">Lee, Moretti, and Butler (<a href="references.html#ref-lmb2004" role="doc-biblioref">2004</a>)</span> find, the effects are still quite different.</p>
<p>It is common, though, to allow the running variable to vary on either side of the discontinuity, but how exactly do we implement that? Think of it—we need for a regression line to be on either side, which means necessarily that we have <em>two</em> lines left and right of the discontinuity. To do this, we need an interaction—specifically an interaction of the running variable with the treatment variable. To implement this in Stata, we can use the code shown in lmb_4.do.</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/lmb_4.do"><code>lmb_4.do</code></a></em></p>
<div class="sourceCode" id="cb65"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb65-1"><a href="ch5.html#cb65-1" aria-hidden="true"></a>* Use <span class="ot">all</span> the <span class="kw">data</span> but interact the treatment <span class="kw">variable</span> with the running <span class="kw">variable</span></span>
<span id="cb65-2"><a href="ch5.html#cb65-2" aria-hidden="true"></a><span class="kw">xi</span>: <span class="kw">reg</span> <span class="kw">score</span> i.lagdemocrat*demvoteshare_c, <span class="kw">cluster</span>(id)</span>
<span id="cb65-3"><a href="ch5.html#cb65-3" aria-hidden="true"></a><span class="kw">xi</span>: <span class="kw">reg</span> <span class="kw">score</span> i.democrat*demvoteshare_c, <span class="kw">cluster</span>(id)</span>
<span id="cb65-4"><a href="ch5.html#cb65-4" aria-hidden="true"></a><span class="kw">xi</span>: <span class="kw">reg</span> democrat i.lagdemocrat*demvoteshare_c, <span class="kw">cluster</span>(id)</span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/lmb_4.R"><code>lmb_4.R</code></a></em></p>
<div class="sourceCode" id="cb66"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">lm_1</span> <span class="op">&lt;-</span> <span class="fu">lm_robust</span><span class="op">(</span><span class="va">score</span> <span class="op">~</span> <span class="va">lagdemocrat</span><span class="op">*</span><span class="va">demvoteshare_c</span>, 
                  data <span class="op">=</span> <span class="va">lmb_data</span>, clusters <span class="op">=</span> <span class="va">id</span><span class="op">)</span>
<span class="va">lm_2</span> <span class="op">&lt;-</span> <span class="fu">lm_robust</span><span class="op">(</span><span class="va">score</span> <span class="op">~</span> <span class="va">democrat</span><span class="op">*</span><span class="va">demvoteshare_c</span>, 
                  data <span class="op">=</span> <span class="va">lmb_data</span>, clusters <span class="op">=</span> <span class="va">id</span><span class="op">)</span>
<span class="va">lm_3</span> <span class="op">&lt;-</span> <span class="fu">lm_robust</span><span class="op">(</span><span class="va">democrat</span> <span class="op">~</span> <span class="va">lagdemocrat</span><span class="op">*</span><span class="va">demvoteshare_c</span>, 
                  data <span class="op">=</span> <span class="va">lmb_data</span>, clusters <span class="op">=</span> <span class="va">id</span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm_1</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm_2</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm_3</span><span class="op">)</span></code></pre></div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:lmbtable4">Table 6.9: </span> Results based on ADA Scores – Full Sample with linear interactions</caption>
<thead><tr class="header">
<th align="left"><strong>Dependent variable</strong></th>
<th align="center"><span class="math inline">\(ADA_{t+1}\)</span></th>
<th align="center"><span class="math inline">\(ADA_{t}\)</span></th>
<th align="center"><span class="math inline">\(DEM_{t+1}\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Estimated gap</td>
<td align="center">30.51<span class="math inline">\(^{***}\)</span>
</td>
<td align="center">55.43<span class="math inline">\(^{***}\)</span>
</td>
<td align="center">0.53<span class="math inline">\(^{***}\)</span>
</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center">(0.82)</td>
<td align="center">(0.64)</td>
<td align="center">(0.01)</td>
</tr>
<tr class="odd">
<td align="left">N</td>
<td align="center">13,577</td>
<td align="center">13,577</td>
<td align="center">13,577</td>
</tr>
</tbody>
</table></div>
<p class="footnote">
Cluster robust standard errors in parenthesis. <span class="math inline">\(^{*}\)</span> <span class="math inline">\(p&lt;0.10\)</span>, <span class="math inline">\(^{**}\)</span> <span class="math inline">\(p&lt;0.05\)</span>, <span class="math inline">\(^{***}\)</span> <span class="math inline">\(p&lt;0.01\)</span>
</p>
<p>In Table <a href="ch5.html#tab:lmbtable4">6.9</a>, we report the global regression analysis with the running variable interacted with the treatment variable. This pulled down the coefficients somewhat, but they remain larger than what was found when we used only those observations within 0.02 points of the 0.5. Finally, let’s estimate the model with a quadratic.</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/lmb_5.do"><code>lmb_5.do</code></a></em></p>
<div class="sourceCode" id="cb67"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb67-1"><a href="ch5.html#cb67-1" aria-hidden="true"></a>* Use <span class="ot">all</span> the <span class="kw">data</span> but interact the treatment <span class="kw">variable</span> with the running <span class="kw">variable</span> and a quadratic</span>
<span id="cb67-2"><a href="ch5.html#cb67-2" aria-hidden="true"></a><span class="kw">gen</span> demvoteshare_sq = demvoteshare_c^2</span>
<span id="cb67-3"><a href="ch5.html#cb67-3" aria-hidden="true"></a><span class="kw">xi</span>: <span class="kw">reg</span> <span class="kw">score</span> lagdemocrat##c.(demvoteshare_c demvoteshare_sq), <span class="kw">cluster</span>(id)</span>
<span id="cb67-4"><a href="ch5.html#cb67-4" aria-hidden="true"></a><span class="kw">xi</span>: <span class="kw">reg</span> <span class="kw">score</span> democrat##c.(demvoteshare_c demvoteshare_sq), <span class="kw">cluster</span>(id)</span>
<span id="cb67-5"><a href="ch5.html#cb67-5" aria-hidden="true"></a><span class="kw">xi</span>: <span class="kw">reg</span> democrat lagdemocrat##c.(demvoteshare_c demvoteshare_sq), <span class="kw">cluster</span>(id)</span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/lmb_5.R"><code>lmb_5.R</code></a></em></p>
<div class="sourceCode" id="cb68"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">lmb_data</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>demvoteshare_sq <span class="op">=</span> <span class="va">demvoteshare_c</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span>

<span class="va">lm_1</span> <span class="op">&lt;-</span> <span class="fu">lm_robust</span><span class="op">(</span><span class="va">score</span> <span class="op">~</span> <span class="va">lagdemocrat</span><span class="op">*</span><span class="va">demvoteshare_c</span> <span class="op">+</span> <span class="va">lagdemocrat</span><span class="op">*</span><span class="va">demvoteshare_sq</span>, 
                  data <span class="op">=</span> <span class="va">lmb_data</span>, clusters <span class="op">=</span> <span class="va">id</span><span class="op">)</span>
<span class="va">lm_2</span> <span class="op">&lt;-</span> <span class="fu">lm_robust</span><span class="op">(</span><span class="va">score</span> <span class="op">~</span> <span class="va">democrat</span><span class="op">*</span><span class="va">demvoteshare_c</span> <span class="op">+</span> <span class="va">democrat</span><span class="op">*</span><span class="va">demvoteshare_sq</span>, 
                  data <span class="op">=</span> <span class="va">lmb_data</span>, clusters <span class="op">=</span> <span class="va">id</span><span class="op">)</span>
<span class="va">lm_3</span> <span class="op">&lt;-</span> <span class="fu">lm_robust</span><span class="op">(</span><span class="va">democrat</span> <span class="op">~</span> <span class="va">lagdemocrat</span><span class="op">*</span><span class="va">demvoteshare_c</span> <span class="op">+</span> <span class="va">lagdemocrat</span><span class="op">*</span><span class="va">demvoteshare_sq</span>, 
                  data <span class="op">=</span> <span class="va">lmb_data</span>, clusters <span class="op">=</span> <span class="va">id</span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm_1</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm_2</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm_3</span><span class="op">)</span></code></pre></div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:lmbtable5">Table 6.10: </span> Results based on ADA Scores – Full Sample with linear and quadratic interactions</caption>
<thead><tr class="header">
<th align="left"><strong>Dependent variable</strong></th>
<th align="center"><span class="math inline">\(ADA_{t+1}\)</span></th>
<th align="center"><span class="math inline">\(ADA_{t}\)</span></th>
<th align="center"><span class="math inline">\(DEM_{t+1}\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Estimated gap</td>
<td align="center">13.03<span class="math inline">\(^{***}\)</span>
</td>
<td align="center">44.40 <span class="math inline">\(^{***}\)</span>
</td>
<td align="center">0.32<span class="math inline">\(^{**}\)</span>
</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center">(1.27)</td>
<td align="center">(0.91)</td>
<td align="center">(1.74)</td>
</tr>
<tr class="odd">
<td align="left">N</td>
<td align="center">13,577</td>
<td align="center">13,577</td>
<td align="center">13,577</td>
</tr>
</tbody>
</table></div>
<p class="footnote">
Cluster robust standard errors in parenthesis. <span class="math inline">\(^{*}\)</span> <span class="math inline">\(p&lt;0.10\)</span>, <span class="math inline">\(^{**}\)</span> <span class="math inline">\(p&lt;0.05\)</span>, <span class="math inline">\(^{***}\)</span> <span class="math inline">\(p&lt;0.01\)</span>
</p>
<p>Including the quadratic causes the estimated effect of a democratic victory on future voting to fall considerably (see Table <a href="ch5.html#tab:lmbtable5">6.10</a>). The effect on contemporaneous voting is smaller than what <span class="citation">Lee, Moretti, and Butler (<a href="references.html#ref-lmb2004" role="doc-biblioref">2004</a>)</span> find, as is the incumbency effect. But the purpose here is simply to illustrate the standard steps using global regressions.</p>
<p>But notice, we are still estimating <em>global</em> regressions. And it is for that reason that the coefficient is larger. This suggests that there exist strong outliers in the data that are causing the distance at <span class="math inline">\(c_0\)</span> to spread more widely. So a natural solution is to again limit our analysis to a smaller window. What this does is drop the observations far away from <span class="math inline">\(c_0\)</span> and omit the influence of outliers from our estimation at the cutoff. Since we used <span class="math inline">\(+/-\)</span> <span class="math inline">\(-0.02\)</span> last time, we’ll use <span class="math inline">\(+/-\)</span> <span class="math inline">\(-0.05\)</span> this time just to mix things up.</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/lmb_6.do"><code>lmb_6.do</code></a></em></p>
<div class="sourceCode" id="cb69"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb69-1"><a href="ch5.html#cb69-1" aria-hidden="true"></a>* Use 5 points from the cutoff</span>
<span id="cb69-2"><a href="ch5.html#cb69-2" aria-hidden="true"></a><span class="kw">xi</span>: <span class="kw">reg</span> <span class="kw">score</span> lagdemocrat##c.(demvoteshare_c demvoteshare_sq) <span class="kw">if</span> lagdemvoteshare&gt;.45 &amp; lagdemvoteshare&lt;.55, <span class="kw">cluster</span>(id)</span>
<span id="cb69-3"><a href="ch5.html#cb69-3" aria-hidden="true"></a><span class="kw">xi</span>: <span class="kw">reg</span> <span class="kw">score</span> democrat##c.(demvoteshare_c demvoteshare_sq) <span class="kw">if</span> lagdemvoteshare&gt;.45 &amp; lagdemvoteshare&lt;.55, <span class="kw">cluster</span>(id)</span>
<span id="cb69-4"><a href="ch5.html#cb69-4" aria-hidden="true"></a><span class="kw">xi</span>: <span class="kw">reg</span> democrat lagdemocrat##c.(demvoteshare_c demvoteshare_sq) <span class="kw">if</span> lagdemvoteshare&gt;.45 &amp; lagdemvoteshare&lt;.55, <span class="kw">cluster</span>(id)</span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/lmb_6.R"><code>lmb_6.R</code></a></em></p>
<div class="sourceCode" id="cb70"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">lmb_data</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">demvoteshare</span> <span class="op">&gt;</span> <span class="fl">.45</span> <span class="op">&amp;</span> <span class="va">demvoteshare</span> <span class="op">&lt;</span> <span class="fl">.55</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>demvoteshare_sq <span class="op">=</span> <span class="va">demvoteshare_c</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span>

<span class="va">lm_1</span> <span class="op">&lt;-</span> <span class="fu">lm_robust</span><span class="op">(</span><span class="va">score</span> <span class="op">~</span> <span class="va">lagdemocrat</span><span class="op">*</span><span class="va">demvoteshare_c</span> <span class="op">+</span> <span class="va">lagdemocrat</span><span class="op">*</span><span class="va">demvoteshare_sq</span>, 
                  data <span class="op">=</span> <span class="va">lmb_data</span>, clusters <span class="op">=</span> <span class="va">id</span><span class="op">)</span>
<span class="va">lm_2</span> <span class="op">&lt;-</span> <span class="fu">lm_robust</span><span class="op">(</span><span class="va">score</span> <span class="op">~</span> <span class="va">democrat</span><span class="op">*</span><span class="va">demvoteshare_c</span> <span class="op">+</span> <span class="va">democrat</span><span class="op">*</span><span class="va">demvoteshare_sq</span>, 
                  data <span class="op">=</span> <span class="va">lmb_data</span>, clusters <span class="op">=</span> <span class="va">id</span><span class="op">)</span>
<span class="va">lm_3</span> <span class="op">&lt;-</span> <span class="fu">lm_robust</span><span class="op">(</span><span class="va">democrat</span> <span class="op">~</span> <span class="va">lagdemocrat</span><span class="op">*</span><span class="va">demvoteshare_c</span> <span class="op">+</span> <span class="va">lagdemocrat</span><span class="op">*</span><span class="va">demvoteshare_sq</span>, 
                  data <span class="op">=</span> <span class="va">lmb_data</span>, clusters <span class="op">=</span> <span class="va">id</span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm_1</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm_2</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">lm_3</span><span class="op">)</span></code></pre></div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:lmbtable6">Table 6.11: </span> Results based on ADA Scores – Close election sample with linear and quadratic interactions</caption>
<thead><tr class="header">
<th align="left"><strong>Dependent variable</strong></th>
<th align="center"><span class="math inline">\(ADA_{t+1}\)</span></th>
<th align="center"><span class="math inline">\(ADA_{t}\)</span></th>
<th align="center"><span class="math inline">\(DEM_{t+1}\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Estimated gap</td>
<td align="center">3.97<span class="math inline">\(^{***}\)</span>
</td>
<td align="center">46.88<span class="math inline">\(^{***}\)</span>
</td>
<td align="center">0.12<span class="math inline">\(^{**}\)</span>
</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center">(1.49)</td>
<td align="center">(1.54)</td>
<td align="center">(0.02)</td>
</tr>
<tr class="odd">
<td align="left">N</td>
<td align="center">2,441</td>
<td align="center">2,441</td>
<td align="center">2,441</td>
</tr>
</tbody>
</table></div>
<p class="footnote">
Cluster robust standard errors in parenthesis. <span class="math inline">\(^{*}\)</span><span class="math inline">\(p&lt;0.10\)</span>, <span class="math inline">\(^{**}\)</span><span class="math inline">\(p&lt;0.05\)</span>, <span class="math inline">\(^{**}\)</span><span class="math inline">\(^{*}\)</span><span class="math inline">\(p&lt;0.01\)</span>
</p>
<p>As can be seen in Table <a href="ch5.html#tab:lmbtable6">6.11</a>, when we limit our analysis to <span class="math inline">\(+/-\)</span> 0.05 around the cutoff, we are using more observations away from the cutoff than we used in our initial analysis. That’s why we only have 2,441 observations for analysis as opposed to the 915 we had in our original analysis. But we also see that including the quadratic interaction pulled the estimated size on future voting down considerably, even when using the smaller sample.</p>
<p>But putting that aside, let’s talk about all that we just did. First we fit a model without controlling for the running variable. But then we included the running variable, introduced in a variety of ways. For instance, we interacted the variable of Democratic vote share with the democratic dummy, as well as including a quadratic. In all this analysis, we extrapolated trends lines from the running variable beyond the support of the data to estimate local average treatment effects right at the cutoff.</p>
<p>But we also saw that the inclusion of the running variable in any form tended to reduce the effect of a victory for Democrats on future Democratic voting patterns, which was interesting. <span class="citation">Lee, Moretti, and Butler (<a href="references.html#ref-lmb2004" role="doc-biblioref">2004</a>)</span> original estimate of around 21 is attenuated considerably when we include controls for the running variable, even when we go back to estimating very local flexible regressions. While the effect remains significant, it is considerably smaller, whereas the immediate effect remains quite large.</p>
<p>But there are still other ways to explore the impact of the treatment at the cutoff. For instance, while <span class="citation">Hahn, Todd, and Klaauw (<a href="references.html#ref-Hahn2001" role="doc-biblioref">2001</a>)</span> clarified assumptions about RDD—specifically, continuity of the conditional expected potential outcomes—they also framed estimation as a nonparametric problem and emphasized using local polynomial regressions. What exactly does this mean though in practice?</p>
<div class="figure">
<span id="fig:lmbfig1"></span>
<img src="graphics/lee_fig1.jpg" alt="@lmb2004, Figure I" width="100%"><p class="caption">
Figure 6.19: <span class="citation">Lee, Moretti, and Butler (<a href="references.html#ref-lmb2004" role="doc-biblioref">2004</a>)</span>, Figure I
</p>
</div>
<p>Nonparametric methods mean a lot of different things to different people in statistics, but in RDD contexts, the idea is to estimate a model that doesn’t assume a functional form for the relationship between the outcome variable <span class="math inline">\((Y)\)</span> and the running variable <span class="math inline">\((X)\)</span>. The model would be something like this:
<span class="math display">\[
Y=f(X) + \varepsilon
\]</span>
A very basic method would be to calculate <span class="math inline">\(E[Y]\)</span> for each bin on <span class="math inline">\(X\)</span>, like a histogram. And Stata has an option to do this called <code>cmogram</code>, created by Christopher Robert. The program has a lot of useful options, and we can re-create important figures from <span class="citation">Lee, Moretti, and Butler (<a href="references.html#ref-lmb2004" role="doc-biblioref">2004</a>)</span>. Figure <a href="ch5.html#fig:lmbfig1">6.19</a> shows the relationship between the Democratic win (as a function of the running variable, Democratic vote share) and the candidates, second-period ADA score.</p>
<p>To reproduce this, there are a few options. You could manually create this figure yourself using either the “twoway” command in Stata or “ggplot” in R. But I’m going to show you using the canned cmogram routine that was created, as it’s a quick-and-dirty way to get some information about the data.</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/lmb_7.do"><code>lmb_7.do</code></a></em></p>
<div class="sourceCode" id="cb71"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb71-1"><a href="ch5.html#cb71-1" aria-hidden="true"></a>* Nonparametric estimation graphic</span>
<span id="cb71-2"><a href="ch5.html#cb71-2" aria-hidden="true"></a><span class="kw">ssc</span> install cmogram</span>
<span id="cb71-3"><a href="ch5.html#cb71-3" aria-hidden="true"></a>cmogram <span class="kw">score</span> lagdemvoteshare, <span class="fu">cut</span>(0.5) <span class="kw">scatter</span> <span class="kw">line</span>(0.5) qfitci</span>
<span id="cb71-4"><a href="ch5.html#cb71-4" aria-hidden="true"></a>cmogram <span class="kw">score</span> lagdemvoteshare, <span class="fu">cut</span>(0.5) <span class="kw">scatter</span> <span class="kw">line</span>(0.5) <span class="kw">lfit</span></span>
<span id="cb71-5"><a href="ch5.html#cb71-5" aria-hidden="true"></a>cmogram <span class="kw">score</span> lagdemvoteshare, <span class="fu">cut</span>(0.5) <span class="kw">scatter</span> <span class="kw">line</span>(0.5) <span class="kw">lowess</span></span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/lmb_7.R"><code>lmb_7.R</code></a></em></p>
<div class="sourceCode" id="cb72"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">#aggregating the data</span>
<span class="va">categories</span> <span class="op">&lt;-</span> <span class="va">lmb_data</span><span class="op">$</span><span class="va">lagdemvoteshare</span>

<span class="va">demmeans</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/split.html">split</a></span><span class="op">(</span><span class="va">lmb_data</span><span class="op">$</span><span class="va">score</span>, <span class="fu"><a href="https://rdrr.io/r/base/cut.html">cut</a></span><span class="op">(</span><span class="va">lmb_data</span><span class="op">$</span><span class="va">lagdemvoteshare</span>, <span class="fl">100</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">lapply</a></span><span class="op">(</span><span class="va">mean</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/base/unlist.html">unlist</a></span><span class="op">(</span><span class="op">)</span>

<span class="va">agg_lmb_data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>score <span class="op">=</span> <span class="va">demmeans</span>, lagdemvoteshare <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0.01</span>,<span class="fl">1</span>, by <span class="op">=</span> <span class="fl">0.01</span><span class="op">)</span><span class="op">)</span>

<span class="co">#plotting</span>
<span class="va">lmb_data</span> <span class="op">&lt;-</span> <span class="va">lmb_data</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>gg_group <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span><span class="va">lagdemvoteshare</span> <span class="op">&gt;</span> <span class="fl">0.5</span> <span class="op">~</span> <span class="fl">1</span>, <span class="cn">TRUE</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span><span class="op">)</span>
         
<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">lmb_data</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">lagdemvoteshare</span>, <span class="va">score</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">lagdemvoteshare</span>, y <span class="op">=</span> <span class="va">score</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">agg_lmb_data</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">stat_smooth</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">lagdemvoteshare</span>, <span class="va">score</span>, group <span class="op">=</span> <span class="va">gg_group</span><span class="op">)</span>, method <span class="op">=</span> <span class="st">"lm"</span>, 
              formula <span class="op">=</span> <span class="va">y</span> <span class="op">~</span> <span class="va">x</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">x</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/lims.html">xlim</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">1</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/lims.html">ylim</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">100</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_vline</a></span><span class="op">(</span>xintercept <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span>

<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">lmb_data</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">lagdemvoteshare</span>, <span class="va">score</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">lagdemvoteshare</span>, y <span class="op">=</span> <span class="va">score</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">agg_lmb_data</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">stat_smooth</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">lagdemvoteshare</span>, <span class="va">score</span>, group <span class="op">=</span> <span class="va">gg_group</span><span class="op">)</span>, method <span class="op">=</span> <span class="st">"loess"</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/lims.html">xlim</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">1</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/lims.html">ylim</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">100</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_vline</a></span><span class="op">(</span>xintercept <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span>

<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">lmb_data</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">lagdemvoteshare</span>, <span class="va">score</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">lagdemvoteshare</span>, y <span class="op">=</span> <span class="va">score</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">agg_lmb_data</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">stat_smooth</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">lagdemvoteshare</span>, <span class="va">score</span>, group <span class="op">=</span> <span class="va">gg_group</span><span class="op">)</span>, method <span class="op">=</span> <span class="st">"lm"</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/lims.html">xlim</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">1</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/lims.html">ylim</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">100</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_vline</a></span><span class="op">(</span>xintercept <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></code></pre></div>
<div class="figure">
<span id="fig:cmo-fig1"></span>
<img src="graphics/cmogram_lee2004_fig1.jpg" alt="Using `cmogram` with quadratic fit and confidence intervals. Reprinted from @lmb2004" width="100%"><p class="caption">
Figure 6.20: Using <code>cmogram</code> with quadratic fit and confidence intervals. Reprinted from <span class="citation">Lee, Moretti, and Butler (<a href="references.html#ref-lmb2004" role="doc-biblioref">2004</a>)</span>
</p>
</div>
<p>Figure <a href="ch5.html#fig:cmo-fig1">6.20</a> shows the output from this program. Notice the similarities between what we produced here and what <span class="citation">Lee, Moretti, and Butler (<a href="references.html#ref-lmb2004" role="doc-biblioref">2004</a>)</span> produced in their figure. The only differences are subtle changes in the binning used for the two figures.</p>
<p>We have options other than a quadratic fit, though, and it’s useful to compare this graph with one in which we only fit a linear model. Now, because there are strong trends in the running variable, we probably just want to use the quadratic, but let’s see what we get when we use simpler straight lines.</p>
<div class="figure">
<span id="fig:cmo-fig2"></span>
<img src="graphics/cmogram_lee2004_linear.jpg" alt="Using `cmogram` with linear fit. Reprinted from @lmb2004" width="100%"><p class="caption">
Figure 6.21: Using <code>cmogram</code> with linear fit. Reprinted from <span class="citation">Lee, Moretti, and Butler (<a href="references.html#ref-lmb2004" role="doc-biblioref">2004</a>)</span>
</p>
</div>
<p>Figure <a href="ch5.html#fig:cmo-fig2">6.21</a> shows what we get when we only use a linear fit of the data left and right of the cutoff. Notice the influence that outliers far from the actual cutoff play in the estimate of the causal effect at the cutoff. Some of this would go away if we restricted the bandwidth to be shorter distances to and from the cutoff, but I leave it to you to do that.</p>
<p>Finally, we can use a lowess fit. A lowess fit more or less crawls through the data and runs small regression on small cuts of data. This can give the figure a zigzag appearance. We nonetheless show it in Figure 40.</p>
<div class="figure">
<span id="fig:cmo-fig3"></span>
<img src="graphics/cmogram_lee2004_lowess.jpg" alt="Using `cmogram` with lowess fit. Reprinted from @lmb2004" width="100%"><p class="caption">
Figure 6.22: Using <code>cmogram</code> with lowess fit. Reprinted from <span class="citation">Lee, Moretti, and Butler (<a href="references.html#ref-lmb2004" role="doc-biblioref">2004</a>)</span>
</p>
</div>
<p>If there don’t appear to be any trends in the running variable, then the polynomials aren’t going to buy you much. Some very good papers only report a linear fit because there weren’t very strong trends to begin with. For instance, consider <span class="citation">Carrell, Hoekstra, and West (<a href="references.html#ref-Carrell2011" role="doc-biblioref">2011</a>)</span>. Those authors are interested in the causal effect of drinking on academic test outcomes for students at the Air Force Academy. Their running variable is the precise age of the student, which they have because they know the student’s date of birth and they know the date of every exam taken at the Air Force Academy. Because the Air Force Academy restricts students’ social life, there is a starker increase in drinking at age 21 on its campus than might be the case for a more a typical university campus. They examined the causal effect of drinking age on normalized grades using RDD, but because there weren’t strong trends in the data, they presented a graph with only a linear fit. Your choice should be in large part based on what, to your eyeball, is the best fit of the data.</p>
<p><span class="citation">Hahn, Todd, and Klaauw (<a href="references.html#ref-Hahn2001" role="doc-biblioref">2001</a>)</span> have shown that one-sided kernel estimation such as lowess may suffer from poor properties because the point of interest is at the boundary (i.e., the discontinuity). This is called the “boundary problem.” They propose using local linear nonparametric regressions instead. In these regressions, more weight is given to the observations at the center.</p>
<p>You can also estimate kernel-weighted local polynomial regressions. Think of it as a weighted regression restricted to a window like we’ve been doing (hence the word “local”) where the chosen kernel provides the weights. A rectangular kernel would give the same results as <span class="math inline">\(E[Y]\)</span> at a given bin on <span class="math inline">\(X\)</span>, but a triangular kernel would give more importance to observations closest to the center. This method will be sensitive to the size of the bandwidth chosen. But in that sense, it’s similar to what we’ve been doing. Figure <a href="ch5.html#fig:lpoly-1">6.23</a> shows this visually.</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/lmb_8.do"><code>lmb_8.do</code></a></em></p>
<div class="sourceCode" id="cb73"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb73-1"><a href="ch5.html#cb73-1" aria-hidden="true"></a>* Note kernel-weighted <span class="kw">local</span> polynomial regression is a smoothing method.</span>
<span id="cb73-2"><a href="ch5.html#cb73-2" aria-hidden="true"></a><span class="kw">capture</span> <span class="kw">drop</span> sdem* x1 x0</span>
<span id="cb73-3"><a href="ch5.html#cb73-3" aria-hidden="true"></a>lpoly <span class="kw">score</span> demvoteshare <span class="kw">if</span> democrat == 0, nograph kernel(triangle) <span class="kw">gen</span>(x0 sdem0) bwidth(0.1)}</span>
<span id="cb73-4"><a href="ch5.html#cb73-4" aria-hidden="true"></a>lpoly <span class="kw">score</span> demvoteshare <span class="kw">if</span> democrat == 1, nograph kernel(triangle) <span class="kw">gen</span>(x1 sdem1)  bwidth(0.1)}</span>
<span id="cb73-5"><a href="ch5.html#cb73-5" aria-hidden="true"></a><span class="kw">scatter</span> sdem1 x1, <span class="kw">color</span>(<span class="kw">red</span>) msize(small) || <span class="kw">scatter</span> sdem0 x0, msize(small) <span class="kw">color</span>(<span class="kw">red</span>) <span class="bn">xline</span>(0.5,lstyle(<span class="bn">dot</span>)) <span class="bn">legend</span>(<span class="kw">off</span>) <span class="bn">xtitle</span>(<span class="st">"Democratic vote share"</span>) <span class="bn">ytitle</span>(<span class="st">"ADA score"</span>)</span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/lmb_8.R"><code>lmb_8.R</code></a></em></p>
<div class="sourceCode" id="cb74"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">stats</span><span class="op">)</span>

<span class="va">smooth_dem0</span> <span class="op">&lt;-</span> <span class="va">lmb_data</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">democrat</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="va">score</span>, <span class="va">demvoteshare</span><span class="op">)</span>
<span class="va">smooth_dem0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/as_tibble.html">as_tibble</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/ksmooth.html">ksmooth</a></span><span class="op">(</span><span class="va">smooth_dem0</span><span class="op">$</span><span class="va">demvoteshare</span>, <span class="va">smooth_dem0</span><span class="op">$</span><span class="va">score</span>, 
                                 kernel <span class="op">=</span> <span class="st">"box"</span>, bandwidth <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span><span class="op">)</span>


<span class="va">smooth_dem1</span> <span class="op">&lt;-</span> <span class="va">lmb_data</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">democrat</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="va">score</span>, <span class="va">demvoteshare</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/na.fail.html">na.omit</a></span><span class="op">(</span><span class="op">)</span>
<span class="va">smooth_dem1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/as_tibble.html">as_tibble</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/ksmooth.html">ksmooth</a></span><span class="op">(</span><span class="va">smooth_dem1</span><span class="op">$</span><span class="va">demvoteshare</span>, <span class="va">smooth_dem1</span><span class="op">$</span><span class="va">score</span>, 
                                 kernel <span class="op">=</span> <span class="st">"box"</span>, bandwidth <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span><span class="op">)</span>

<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">geom_smooth</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">smooth_dem0</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">geom_smooth</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">smooth_dem1</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_vline</a></span><span class="op">(</span>xintercept <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></code></pre></div>
<div class="figure">
<span id="fig:lpoly-1"></span>
<img src="graphics/lpoly_1.jpg" alt="Local linear nonparametric regressions" width="100%"><p class="caption">
Figure 6.23: Local linear nonparametric regressions
</p>
</div>
<p>A couple of final things. First, recall the continuity assumption. Because the continuity assumption specifically involves continuous conditional expectation functions of the potential outcomes throughout the cutoff, it therefore is <em>untestable</em>. That’s right—it’s an untestable assumption. But, what we can do is check for whether there are changes in the conditional expectation functions for other exogenous covariates that cannot or should not be changing as a result of the cutoff. So it’s very common to look at things like race or gender around the cutoff. You can use these same methods to do that, but I do not do them here. Any RDD paper will always involve such placebos; even though they are not direct tests of the continuity assumption, they are indirect tests. Remember, when you are publishing, your readers aren’t as familiar with this thing you’re studying, so your task is explain to readers what you know. Anticipate their objections and the sources of their skepticism. Think like them. Try to put yourself in a stranger’s shoes. And then test those skepticisms to the best of your ability.</p>
<p>Second, we saw the importance of bandwidth selection, or window, for estimating the causal effect using this method, as well as the importance of selection of polynomial length. There’s always a trade-off when choosing the bandwidth between bias and variance—the shorter the window, the lower the bias, but because you have less data, the variance in your estimate increases. Recent work has been focused on optimal bandwidth selection, such as <span class="citation">Imbens and Kalyanaraman (<a href="references.html#ref-Imbens2011" role="doc-biblioref">2011</a>)</span> and <span class="citation">Calonico, Cattaneo, and Titiunik (<a href="references.html#ref-Calonico2014" role="doc-biblioref">2014</a>)</span>. The latter can be implemented with the user-created <code>rdrobust</code> command. These methods ultimately choose optimal bandwidths that may differ left and right of the cutoff based on some bias-variance trade-off. Let’s repeat our analysis using this nonparametric method. The coefficient is 46.48 with a standard error of 1.24.</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/lmb_9.do"><code>lmb_9.do</code></a></em></p>
<div class="sourceCode" id="cb75"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb75-1"><a href="ch5.html#cb75-1" aria-hidden="true"></a>* Local polynomial point estimators with bias correction</span>
<span id="cb75-2"><a href="ch5.html#cb75-2" aria-hidden="true"></a><span class="kw">ssc</span> install rdrobust, <span class="kw">replace</span></span>
<span id="cb75-3"><a href="ch5.html#cb75-3" aria-hidden="true"></a>rdrobust <span class="kw">score</span> demvoteshare, c(0.5)</span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/lmb_9.R"><code>lmb_9.R</code></a></em></p>
<div class="sourceCode" id="cb76"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">rdrobust</span><span class="op">)</span>

<span class="va">rdr</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rdrobust/man/rdrobust.html">rdrobust</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">lmb_data</span><span class="op">$</span><span class="va">score</span>,
                x <span class="op">=</span> <span class="va">lmb_data</span><span class="op">$</span><span class="va">demvoteshare</span>, c <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">rdr</span><span class="op">)</span></code></pre></div>
<p>This method, as we’ve repeatedly said, is data-greedy because it gobbles up data at the discontinuity. So ideally these kinds of methods will be used when you have large numbers of observations in the sample so that you have a sizable number of observations at the discontinuity. When that is the case, there should be some harmony in your findings across results. If there isn’t, then you may not have sufficient power to pick up this effect.</p>
<p>Finally, we look at the implementation of the McCrary density test. We will implement this test using local polynomial density estimation <span class="citation">(Cattaneo, Jansson, and Ma <a href="references.html#ref-Cattaneo2019" role="doc-biblioref">2019</a>)</span>. This requires installing two files in Stata. Visually inspecting the graph in Figure <a href="ch5.html#fig:mccrary">6.24</a>, we see no signs that there was manipulation in the running variable at the cutoff.</p>
<div class="figure">
<span id="fig:mccrary"></span>
<img src="graphics/rddensity.jpg" alt="McCrary density test using local linear nonparametric regressions" width="100%"><p class="caption">
Figure 6.24: McCrary density test using local linear nonparametric regressions
</p>
</div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/lmb_10.do"><code>lmb_10.do</code></a></em></p>
<div class="sourceCode" id="cb77"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb77-1"><a href="ch5.html#cb77-1" aria-hidden="true"></a>* McCrary density <span class="kw">test</span></span>
<span id="cb77-2"><a href="ch5.html#cb77-2" aria-hidden="true"></a>net install rddensity, from(https:<span class="co">//sites.google.com/site/rdpackages/rddensity/stata) replace</span></span>
<span id="cb77-3"><a href="ch5.html#cb77-3" aria-hidden="true"></a>net install lpdensity, from(https:<span class="co">//sites.google.com/site/nppackages/lpdensity/stata) replace</span></span>
<span id="cb77-4"><a href="ch5.html#cb77-4" aria-hidden="true"></a>rddensity demvoteshare, c(0.5) plot</span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/lmb_10.R"><code>lmb_10.R</code></a></em></p>
<div class="sourceCode" id="cb78"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">rddensity</span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">rdd</span><span class="op">)</span>

<span class="fu">DCdensity</span><span class="op">(</span><span class="va">lmb_data</span><span class="op">$</span><span class="va">demvoteshare</span>, cutpoint <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span>

<span class="va">density</span> <span class="op">&lt;-</span> <span class="fu">rddensity</span><span class="op">(</span><span class="va">lmb_data</span><span class="op">$</span><span class="va">demvoteshare</span>, c <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span>
<span class="fu">rdplotdensity</span><span class="op">(</span><span class="va">density</span>, <span class="va">lmb_data</span><span class="op">$</span><span class="va">demvoteshare</span><span class="op">)</span></code></pre></div>
</div>
<div id="concluding-remarks-about-close-election-designs" class="section level3" number="6.4.2">
<h3>
<span class="header-section-number">6.4.2</span> Concluding remarks about close-election designs<a class="anchor" aria-label="anchor" href="#concluding-remarks-about-close-election-designs"><i class="fas fa-link"></i></a>
</h3>
<p>Let’s circle back to the close-election design. The design has since become practically a cottage industry within economics and political science. It has been extended to other types of elections and outcomes. One paper I like a lot used close gubernatorial elections to examine the effect of Democratic governors on the wage gap between workers of different races <span class="citation">(Beland <a href="references.html#ref-Beland2015" role="doc-biblioref">2015</a>)</span>. There are dozens more.</p>
<p>But a critique from <span class="citation">Caughey and Sekhon (<a href="references.html#ref-Caughey2011" role="doc-biblioref">2011</a>)</span> called into question the validity of Lee’s analysis on the House elections. They found that bare winners and bare losers in US House elections differed considerably on pretreatment covariates, which had not been formally evaluated by <span class="citation">Lee, Moretti, and Butler (<a href="references.html#ref-lmb2004" role="doc-biblioref">2004</a>)</span>. And that covariate imbalance got even worse in the closest elections. Their conclusion is that the sorting problems got more severe, not less, in the closest of House races, suggesting that these races could not be used for an RDD.</p>
<p>At first glance, it appeared that this criticism by <span class="citation">Caughey and Sekhon (<a href="references.html#ref-Caughey2011" role="doc-biblioref">2011</a>)</span> threw cold water on the entire close-election design, but we since know that is not the case. It appears that the <span class="citation">Caughey and Sekhon (<a href="references.html#ref-Caughey2011" role="doc-biblioref">2011</a>)</span> criticism may have been only relevant for a subset of House races but did not characterize other time periods or other types of races. <span class="citation">Eggers et al. (<a href="references.html#ref-Eggers2014" role="doc-biblioref">2014</a>)</span> evaluated 40,000 close elections, including the House in other time periods, mayoral races, and other types of races for political offices in the US and nine other countries. No other case that they encountered exhibited the type of pattern described by <span class="citation">Caughey and Sekhon (<a href="references.html#ref-Caughey2011" role="doc-biblioref">2011</a>)</span>. Eggers et al. (2014) conclude that the assumptions behind RDD in the close-election design are likely to be met in a wide variety of electoral settings and is perhaps one of the best RD designs we have going forward.</p>
</div>
</div>
<div id="regression-kink-design" class="section level2" number="6.5">
<h2>
<span class="header-section-number">6.5</span> Regression Kink Design<a class="anchor" aria-label="anchor" href="#regression-kink-design"><i class="fas fa-link"></i></a>
</h2>
<p>Many times, the concept of a running variable shifting a unit into treatment and in turn causing a jump in some outcome is sufficient. But there are some instances in which the idea of a “jump” doesn’t describe what happens. A couple of papers by David Card and coauthors have extended the regression discontinuity design in order to handle these different types of situations. The most notable is <span class="citation">Card et al. (<a href="references.html#ref-Card2015" role="doc-biblioref">2015</a>)</span>, which introduced a new method called regression kink design, or RKD. The intuition is rather simple. Rather than the cutoff causing a discontinuous jump in the treatment variable at the cutoff, it changes the first derivative, which is known as a kink. Kinks are often embedded in policy rules, and thanks to <span class="citation">Card et al. (<a href="references.html#ref-Card2015" role="doc-biblioref">2015</a>)</span>, we can use kinks to identify the causal effect of a policy by exploiting the jump in the first derivative.</p>
<p><span class="citation">Card et al. (<a href="references.html#ref-Card2015" role="doc-biblioref">2015</a>)</span> paper applies the design to answer the question of whether the level of unemployment benefits affects the length of time spent unemployed in Austria. Unemployment benefits are based on income in a base period. There is then a minimum benefit level that isn’t binding for people with low earnings. Then benefits are 55% of the earnings in the base period. There is a maximum benefit level that is then adjusted every year, which creates a discontinuity in the schedule.</p>
<div class="figure">
<span id="fig:rkd-1"></span>
<img src="graphics/rkd_1.jpg" alt="RKD kinks. Reprinted from @Card2015." width="100%"><p class="caption">
Figure 6.25: RKD kinks. Reprinted from <span class="citation">Card et al. (<a href="references.html#ref-Card2015" role="doc-biblioref">2015</a>)</span>.
</p>
</div>
<p>Figure <a href="ch5.html#fig:rkd-1">6.25</a> shows the relationship between base earnings and unemployment benefits around the discontinuity. There’s a visible kink in the empirical relationship between average benefits and base earnings. You can see this in the sharp decline in the slope of the function as base-year earnings pass the threshold. Figure <a href="ch5.html#fig:rkd-3">6.26</a> presents a similar picture, but this time of unemployment duration. Again, there is a clear kink as base earnings pass the threshold. The authors conclude that increases in unemployment benefits in the Austrian context exert relatively large effects on unemployment duration.</p>
<div class="figure">
<span id="fig:rkd-3"></span>
<img src="graphics/rkd_3.jpg" alt="Unemployment duration. Reprinted from @Card2015." width="100%"><p class="caption">
Figure 6.26: Unemployment duration. Reprinted from <span class="citation">Card et al. (<a href="references.html#ref-Card2015" role="doc-biblioref">2015</a>)</span>.
</p>
</div>
</div>
<div id="conclusion-4" class="section level2" number="6.6">
<h2>
<span class="header-section-number">6.6</span> Conclusion<a class="anchor" aria-label="anchor" href="#conclusion-4"><i class="fas fa-link"></i></a>
</h2>
<p>The regression discontinuity design is often considered a winning design because of its upside in credibly identifying causal effects. As with all designs, its credibility only comes from deep institutional knowledge, particularly surrounding the relationship between the running variable, the cutoff, treatment assignment, and the outcomes themselves. Insofar as one can easily find a situation in which a running variable passing some threshold leads to units being siphoned off into some treatment, then if continuity is believable, you’re probably sitting on a great opportunity, assuming you can use it to do something theoretically interesting and policy relevant to others.</p>
<p>Regression discontinuity design opportunities abound, particularly within firms and government agencies, for no other reason than that these organizations face scarcity problems and must use some method to ration a treatment. Randomization is a fair way to do it, and that is often the method used. But a running variable is another method. Routinely, organizations will simply use a continuous score to assign treatments by arbitrarily picking a cutoff above which everyone receives the treatment. Finding these can yield a cheap yet powerfully informative natural experiment. This chapter attempted to lay out the basics of the design. But the area continues to grow at a lightning pace. So I encourage you to see this chapter as a starting point, not an ending point.</p>
<div class="cover-box">
<div class="row">
    <div class="col-xs-8 col-md-4 cover-img">
        <a href="https://www.amazon.com/dp/0300251688"><img src="images/cover.jpg" alt="Buy Today!"></a>
    </div>
    
    <div class="col-xs-12 col-md-8 cover-text-box">
            <h2> 
                Causal Inference: 
                <br><span style="font-style: italic; font-weight:bold; font-size: 20px;">The Mixtape.</span>
            </h2> 
            
        <div class="cover-text">
            <p>Buy the print version today:</p>
            
            <div class="chips">
                <a href="https://www.amazon.com/dp/0300251688" class="app-chip"> 
                    <i class="fab fa-amazon" aria-hidden="true"></i> Buy from Amazon 
                </a>
    
                <a href="https://yalebooks.yale.edu/book/9780300251685/causal-inference" class="app-chip"> 
                    <i class="fas fa-book" aria-hidden="true"></i> Buy from Yale Press 
                </a>
            </div>
        </div>
    </div>
</div>
</div>

</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="ch4.html"><span class="header-section-number">5</span> Matching and Subclassification</a></div>
<div class="next"><a href="ch6.html"><span class="header-section-number">7</span> Instrumental Variables</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#ch5"><span class="header-section-number">6</span> Regression Discontinuity</a></li>
<li>
<a class="nav-link" href="#huge-popularity-of-regression-discontinuity"><span class="header-section-number">6.1</span> Huge Popularity of Regression Discontinuity</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#waiting-for-life"><span class="header-section-number">6.1.1</span> Waiting for life</a></li>
<li><a class="nav-link" href="#graphical-representation-of-rdd"><span class="header-section-number">6.1.2</span> Graphical representation of RDD</a></li>
<li><a class="nav-link" href="#a-picture-is-worth-a-thousand-words"><span class="header-section-number">6.1.3</span> A picture is worth a thousand words</a></li>
<li><a class="nav-link" href="#data-requirements-for-rdd"><span class="header-section-number">6.1.4</span> Data requirements for RDD</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#estimation-using-an-rdd"><span class="header-section-number">6.2</span> Estimation Using an RDD</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#the-sharp-rd-design"><span class="header-section-number">6.2.1</span> The Sharp RD Design</a></li>
<li><a class="nav-link" href="#continuity-assumption"><span class="header-section-number">6.2.2</span> Continuity assumption</a></li>
<li><a class="nav-link" href="#estimation-using-local-and-global-least-squares-regressions"><span class="header-section-number">6.2.3</span> Estimation using local and global least squares regressions</a></li>
<li><a class="nav-link" href="#nonparametric-kernels"><span class="header-section-number">6.2.4</span> Nonparametric kernels</a></li>
<li><a class="nav-link" href="#medicare-and-universal-health-care"><span class="header-section-number">6.2.5</span> Medicare and universal health care</a></li>
<li><a class="nav-link" href="#inference"><span class="header-section-number">6.2.6</span> Inference</a></li>
<li><a class="nav-link" href="#the-fuzzy-rd-design"><span class="header-section-number">6.2.7</span> The Fuzzy RD Design</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#challenges-to-identification"><span class="header-section-number">6.3</span> Challenges to Identification</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#mccrarys-density-test"><span class="header-section-number">6.3.1</span> McCrary’s density test</a></li>
<li><a class="nav-link" href="#covariate-balance-and-other-placebos"><span class="header-section-number">6.3.2</span> Covariate balance and other placebos</a></li>
<li><a class="nav-link" href="#nonrandom-heaping-on-the-running-variable"><span class="header-section-number">6.3.3</span> Nonrandom heaping on the running variable</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#replicating-a-popular-design-the-close-election"><span class="header-section-number">6.4</span> Replicating a Popular Design: The Close Election</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#replication-exercise"><span class="header-section-number">6.4.1</span> Replication exercise</a></li>
<li><a class="nav-link" href="#concluding-remarks-about-close-election-designs"><span class="header-section-number">6.4.2</span> Concluding remarks about close-election designs</a></li>
</ul>
</li>
<li><a class="nav-link" href="#regression-kink-design"><span class="header-section-number">6.5</span> Regression Kink Design</a></li>
<li><a class="nav-link" href="#conclusion-4"><span class="header-section-number">6.6</span> Conclusion</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/scunning1975/mixtape/blob/master/05-Regression_Discontinuity.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/scunning1975/mixtape/edit/master/05-Regression_Discontinuity.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong><span style="font-weight:bold">Causal Inference</span></strong>: <i>The Mixtape</i>" was written by Scott Cunningham. It was last built on 2020-12-19.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
