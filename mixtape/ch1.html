<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>2 Probability and Regression Review | Causal Inference</title>
<meta name="author" content="Scott Cunningham">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.6/header-attrs.js"></script><script src="libs/jquery-3.5.1/jquery-3.5.1.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.5.3/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.5.3/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.3.9000/tabs.js"></script><script src="libs/bs3compat-0.2.3.9000/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><style>
    @import url('https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,400;0,700;1,400;1,700&family=Roboto:ital,wght@0,700;1,300&display=swap');
    </style>
<script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS --><link rel="stylesheet" href="css/style.css">
<link rel="stylesheet" href="css/toc.css">
<link rel="stylesheet" href="css/causal_inference_style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="&lt;i&gt;The Mixtape&lt;/i&gt;"><span style="font-weight:bold">Causal Inference</span></a>:
        <small class="text-muted"><i>The Mixtape</i></small>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="active" href="ch1.html"><span class="header-section-number">2</span> Probability and Regression Review</a></li>
<li><a class="" href="ch2.html"><span class="header-section-number">3</span> Directed Acyclic Graphs</a></li>
<li><a class="" href="ch3.html"><span class="header-section-number">4</span> Potential Outcomes Causal Model</a></li>
<li><a class="" href="ch4.html"><span class="header-section-number">5</span> Matching and Subclassification</a></li>
<li><a class="" href="ch5.html"><span class="header-section-number">6</span> Regression Discontinuity</a></li>
<li><a class="" href="ch6.html"><span class="header-section-number">7</span> Instrumental Variables</a></li>
<li><a class="" href="ch7.html"><span class="header-section-number">8</span> Panel Data</a></li>
<li><a class="" href="ch8.html"><span class="header-section-number">9</span> Difference-in-Differences</a></li>
<li><a class="" href="ch9.html"><span class="header-section-number">10</span> Synthetic Control</a></li>
<li><a class="" href="ch10.html"><span class="header-section-number">11</span> Conclusion</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/scunning1975/mixtape">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="ch1" class="section level1" number="2">
<h1>
<span class="header-section-number">2</span> Probability and Regression Review<a class="anchor" aria-label="anchor" href="#ch1"><i class="fas fa-link"></i></a>
</h1>
<div class="cover-box">
<div class="row">
    <div class="col-xs-8 col-md-4 cover-img">
        <a href="https://www.amazon.com/dp/0300251688"><img src="images/cover.jpg" alt="Buy Today!"></a>
    </div>
    
    <div class="col-xs-12 col-md-8 cover-text-box">
            <h2> 
                Causal Inference: 
                <br><span style="font-style: italic; font-weight:bold; font-size: 20px;">The Mixtape.</span>
            </h2> 
            
        <div class="cover-text">
            <p>Buy the print version today:</p>
            
            <div class="chips">
                <a href="https://www.amazon.com/dp/0300251688" class="app-chip"> 
                    <i class="fab fa-amazon" aria-hidden="true"></i> Buy from Amazon 
                </a>
    
                <a href="https://yalebooks.yale.edu/book/9780300251685/causal-inference" class="app-chip"> 
                    <i class="fas fa-book" aria-hidden="true"></i> Buy from Yale Press 
                </a>
            </div>
        </div>
    </div>
</div>
</div>
<div id="basic-probability-theory" class="section level2" number="2.1">
<h2>
<span class="header-section-number">2.1</span> Basic probability theory<a class="anchor" aria-label="anchor" href="#basic-probability-theory"><i class="fas fa-link"></i></a>
</h2>
<p>In practice, causal inference is based on statistical models that range from the very simple to extremely advanced. And building such models requires some rudimentary knowledge of probability theory, so let’s begin with some definitions. A random process is a process that can be repeated many times with different outcomes each time. The sample space is the set of all the possible outcomes of a random process. We distinguish between discrete and continuous random processes Table 1 below. Discrete processes produce, integers, whereas continuous processes produce fractions as well.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:random-process">Table 2.1: </span> Examples of Discrete and Continuous Random Processes</caption>
<thead><tr class="header">
<th align="left">Description</th>
<th align="left">Type</th>
<th align="left">Potential outcomes</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">12-sided die</td>
<td align="left">Discrete</td>
<td align="left">1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12</td>
</tr>
<tr class="even">
<td align="left">Coin</td>
<td align="left">Discrete</td>
<td align="left">Heads, Tails</td>
</tr>
<tr class="odd">
<td align="left">Deck of cards</td>
<td align="left">Discrete</td>
<td align="left">2 <span class="math inline">\(\diamondsuit\)</span>, 3 <span class="math inline">\(\diamondsuit\)</span>, …King <span class="math inline">\(\heartsuit\)</span>, Ace <span class="math inline">\(\heartsuit\)</span>
</td>
</tr>
<tr class="even">
<td align="left">Gas prices</td>
<td align="left">Continuous</td>
<td align="left"><span class="math inline">\(P\geq 0\)</span></td>
</tr>
</tbody>
</table></div>
<p>We define independent events two ways. The first refers to logical independence. For instance, two events occur but there is no reason to believe that the two events affect each other. When it is assumed that they <em>do</em> affect each other, this is a logical fallacy called <em>post hoc ergo propter hoc</em>, which is Latin for “after this, therefore because of this.” This fallacy recognizes that the temporal ordering of events is not sufficient to be able to say that the first thing caused the second.</p>
<p>The second definition of an independent event is statistical independence. We’ll illustrate the latter with an example from the idea of sampling with and without replacement. Let’s use a randomly shuffled deck of cards for an example. For a deck of 52 cards, what is the probability that the first card will be an ace?
<span class="math display">\[ 
\Pr(\text{Ace}) =\dfrac{\text{Count Aces}}{\text{Sample Space}}=\dfrac{4}{52}=
   \dfrac{1}{13}=0.077
\]</span>
There are 52 possible outcomes in the sample space, or the set of all possible outcomes of the random process. Of those 52 possible outcomes, we are concerned with the frequency of an ace occurring. There are four aces in the deck, so <span class="math inline">\(\dfrac{4}{52}=0.077\)</span>.</p>
<p>Assume that the first card was an ace. Now we ask the question again. If we shuffle the deck, what is the probability the next card drawn is also an ace? It is no longer <span class="math inline">\(\dfrac{1}{13}\)</span> because we did not sample with replacement. We sampled <em>without</em> replacement. Thus the new probability is
<span class="math display">\[ 
\Pr\Big(\text{Ace}\mid\text{Card }1 =\text{Ace}\Big) =
   \dfrac{3}{51}= 0.059
\]</span>
Under sampling without replacement, the two events—ace on <span class="math inline">\(\text{Card }1\)</span> and an ace on <span class="math inline">\(\text{Card }2\)</span> if <span class="math inline">\(\text{Card }1\)</span> was an ace—aren’t independent events. To make the two events independent, you would have to put the ace back and shuffle the deck. So two events, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, are independent if and only if:
<span class="math display">\[ 
\Pr(A\mid B)=\Pr(A)
\]</span>
An example of two independent events would be rolling a 5 with one die after having rolled a 3 with another die. The two events are independent, so the probability of rolling a 5 is always 0.17 regardless of what we rolled on the first die.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;The probability rolling a 5 using one six-sided die is &lt;span class="math inline"&gt;\(\dfrac{1}{6}=0.167\)&lt;/span&gt;.&lt;/p&gt;'><sup>11</sup></a></p>
<p>But what if we want to know the probability of some event occurring that requires that multiple events first to occur? For instance, let’s say we’re talking about the Cleveland Cavaliers winning the NBA championship. In 2016, the Golden State Warriors were 3–1 in a best-of-seven playoff. What had to happen for the Warriors to lose the playoff? The Cavaliers had to win three in a row. In this instance, to find the probability, we have to take the product of all marginal probabilities, or <span class="math inline">\(\Pr(\cdot)^n\)</span>, where <span class="math inline">\(\Pr(\cdot)\)</span> is the marginal probability of one event occurring, and <span class="math inline">\(n\)</span> is the number of repetitions of that one event. If the unconditional probability of a Cleveland win is 0.5, and each game is independent, then the probability that Cleveland could come back from a 3–1 deficit is the <em>product</em> of each game’s probability of winning:
<span class="math display">\[ 
\text{Win probability} =\Pr\big(W,W,W\big)= (0.5)^3= 0.125
\]</span>
Another example may be helpful. In Texas Hold’em poker, each player is dealt two cards facedown. When you are holding two of a kind, you say you have two “in the pocket.” So, what is the probability of being dealt pocket aces? It’s <span class="math inline">\(\dfrac{4}{52}\times\dfrac{3}{51}=0.0045\)</span>. That’s right: it’s <span class="math inline">\(0.45\%\)</span>.</p>
<p>Let’s formalize what we’ve been saying for a more generalized case. For independent events, to calculate <em>joint probabilities</em>, we multiply the marginal probabilities:
<span class="math display">\[ 
\Pr(A,B)=\Pr(A)\Pr(B)
\]</span>
where <span class="math inline">\(\Pr(A,B)\)</span> is the joint probability of both <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> occurring, and <span class="math inline">\(\Pr(A)\)</span> is the marginal probability of <span class="math inline">\(A\)</span> event occurring.</p>
<p>Now, for a slightly more difficult application. What is the probability of rolling a 7 using two six-sided dice, and is it the same as the probability of rolling a 3? To answer this, let’s compare the two probabilities. We’ll use a table to help explain the intuition. First, let’s look at all the ways to get a 7 using two six-sided dice. There are 36 total possible outcomes <span class="math inline">\((6^2=36)\)</span> when rolling two dice. In Table <a href="ch1.html#tab:7die">2.2</a> we see that there are six different ways to roll a 7 using only two dice. So the probability of rolling a 7 is <span class="math inline">\(6/36=16.67\)</span>%. Next, let’s look at all the ways to roll a 3 using two six-sided dice. Table <a href="ch1.html#tab:3die">2.3</a> shows that there are only two ways to get a 3 rolling two six-sided dice. So the probability of rolling a 3 is <span class="math inline">\(2/36=5.56\)</span>%. So, no, the probabilities of rolling a 7 and rolling a 3 are different.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:7die">Table 2.2: </span> Total number of ways to get a 7 with two six-sided dice.</caption>
<thead><tr class="header">
<th align="left">Die 1</th>
<th align="left">Die 2</th>
<th align="left">Outcome</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">6</td>
<td align="left">7</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="left">5</td>
<td align="left">7</td>
</tr>
<tr class="odd">
<td align="left">3</td>
<td align="left">4</td>
<td align="left">7</td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="left">3</td>
<td align="left">7</td>
</tr>
<tr class="odd">
<td align="left">5</td>
<td align="left">2</td>
<td align="left">7</td>
</tr>
<tr class="even">
<td align="left">6</td>
<td align="left">1</td>
<td align="left">7</td>
</tr>
</tbody>
</table></div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:3die">Table 2.3: </span> Total number of ways to get a 3 using two six-sided dice.</caption>
<thead><tr class="header">
<th align="left">Die 1</th>
<th align="left">Die 2</th>
<th align="left">Outcome</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">2</td>
<td align="left">3</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="left">1</td>
<td align="left">3</td>
</tr>
</tbody>
</table></div>
</div>
<div id="events-and-conditional-probability" class="section level2" number="2.2">
<h2>
<span class="header-section-number">2.2</span> Events and conditional probability<a class="anchor" aria-label="anchor" href="#events-and-conditional-probability"><i class="fas fa-link"></i></a>
</h2>
<p>First, before we talk about the three ways of representing a probability, I’d like to introduce some new terminology and concepts: <strong>events</strong> and <strong>conditional probabilities</strong>. Let <span class="math inline">\(A\)</span> be some event. And let <span class="math inline">\(B\)</span> be some other event. For two events, there are four possibilities.</p>
<ol style="list-style-type: decimal">
<li><p>A and B: Both A and B occur.</p></li>
<li><p><span class="math inline">\(\sim\)</span> A and B: A does not occur, but B occurs.</p></li>
<li><p>A and <span class="math inline">\(\sim\)</span> B: A occurs, but B does not occur.</p></li>
<li><p><span class="math inline">\(\sim\)</span> A and <span class="math inline">\(\sim\)</span> B: Neither A nor B occurs.</p></li>
</ol>
<p>I’ll use a couple of different examples to illustrate how to represent a probability.</p>
</div>
<div id="probability-tree" class="section level2" number="2.3">
<h2>
<span class="header-section-number">2.3</span> Probability tree<a class="anchor" aria-label="anchor" href="#probability-tree"><i class="fas fa-link"></i></a>
</h2>
<p>Let’s think about a situation in which you are trying to get your driver’s license. Suppose that in order to get a driver’s license, you have to pass the written exam and the driving exam. However, if you fail the written exam, you’re not allowed to take the driving exam. We can represent these two events in a probability tree.</p>
<div class="inline-figure"><img src="causal_inference_mixtape_files/figure-html/unnamed-chunk-5-1.png" width="100%" style="display: block; margin: auto;"></div>
<p>Probability trees are intuitive and easy to interpret.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;The set notation &lt;span class="math inline"&gt;\(\cup\)&lt;/span&gt; means “union” and refers to two events occurring together.&lt;/p&gt;'><sup>12</sup></a> First, we see that the probability of passing the written exam is 0.75 and the probability of failing the exam is 0.25. Second, at every branching off from a node, we can further see that the probabilities associated with a given branch are summing to 1.0. The joint probabilities are also all summing to 1.0. This is called the law of total probability and it is equal to the sum of all joint probability of A and B<span class="math inline">\(_n\)</span> events occurring:
<span class="math display">\[ 
\Pr(A)=\sum_n Pr(A\cup B_n)
\]</span></p>
<p>We also see the concept of a conditional probability in the driver’s license tree. For instance, the probability of failing the driving exam, conditional on having passed the written exam, is represented as <span class="math inline">\(\Pr(\text{Fail} \mid \text{Pass})=0.45\)</span>.</p>
</div>
<div id="venn-diagrams-and-sets" class="section level2" number="2.4">
<h2>
<span class="header-section-number">2.4</span> Venn diagrams and sets<a class="anchor" aria-label="anchor" href="#venn-diagrams-and-sets"><i class="fas fa-link"></i></a>
</h2>
<p>A second way to represent multiple events occurring is with a Venn diagram. Venn diagrams were first conceived by John Venn in 1880. They are used to teach elementary set theory, as well as to express set relationships in probability and statistics. This example will involve two sets, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>.</p>
<div class="inline-figure"><img src="causal_inference_mixtape_files/figure-html/unnamed-chunk-6-1.png" width="100%" style="display: block; margin: auto;"></div>
<p>The University of Texas’s football coach has been on the razor’s edge with the athletic director and regents all season. After several mediocre seasons, his future with the school is in jeopardy. If the Longhorns don’t make it to a great bowl game, he likely won’t be rehired. But if they do, then he likely will be rehired. Let’s discuss elementary set theory using this coach’s situation as our guiding example. But before we do, let’s remind ourselves of our terms. <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are events, and <span class="math inline">\(U\)</span> is the universal set of which <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are subsets. Let <span class="math inline">\(A\)</span> be the probability that the Longhorns get invited to a great bowl game and <span class="math inline">\(B\)</span> be the probability that their coach is rehired. Let <span class="math inline">\(\Pr(A)=0.6\)</span> and let <span class="math inline">\(\Pr(B)=0.8\)</span>. Let the probability that both <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> occur be <span class="math inline">\(\Pr(A,B)=0.5\)</span>.</p>
<p>Note, that <span class="math inline">\(A+\sim A=U\)</span>, where <span class="math inline">\(\sim A\)</span> is the complement of <span class="math inline">\(A\)</span>. The complement means that it is everything in the universal set that is not A. The same is said of B. The sum of <span class="math inline">\(B\)</span> and <span class="math inline">\(\sim B=U\)</span>. Therefore:
<span class="math display">\[ 
A+{\sim} A=B+ {\sim} B
\]</span></p>
<p>We can rewrite out the following definitions:</p>
<p><span class="math display">\[\begin{align}
   A &amp; = B+{\sim} B - {\sim} A \\
   B &amp; = A+{\sim} A - {\sim} B 
\end{align}\]</span></p>
<p>Whenever we want to describe a set of events in which either <span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span> could occur, it is: <span class="math inline">\(A \cup B\)</span>. And this is pronounced “<span class="math inline">\(A\)</span> union <span class="math inline">\(B\)</span>,” which means it is the new set that contains every element from <span class="math inline">\(A\)</span> and every element from <span class="math inline">\(B\)</span>. Any element that is in either set <span class="math inline">\(A\)</span> or set <span class="math inline">\(B\)</span>, then, is also in the new union set. And whenever we want to describe a set of events that occurred together—the joint set—it’s <span class="math inline">\(A \cap B\)</span>, which is pronounced “<span class="math inline">\(A\)</span> intersect <span class="math inline">\(B\)</span>.” This new set contains every element that is in both the <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> sets. That is, only things inside both <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> get added to the new set.</p>
<p>Now let’s look closely at a relationship involving the set <em>A</em>.</p>
<p><span class="math display">\[\begin{align}
   A=A \cup B+A \cup{\sim} B
\end{align}\]</span></p>
<p>Notice what this is saying: there are two ways to identify the <span class="math inline">\(A\)</span> set. First, you can look at all the instances where <span class="math inline">\(A\)</span> occurs with <span class="math inline">\(B\)</span>. But then what about the rest of <span class="math inline">\(A\)</span> that is not in <span class="math inline">\(B\)</span>? Well, that’s the <span class="math inline">\(A \cup B\)</span> situation, which covers the rest of the <span class="math inline">\(A\)</span> set.</p>
<p>A similar style of reasoning can help you understand the following expression.
<span class="math display">\[ 
A \cap B=A \cup{\sim}B+\sim A \cup B+A \cup B
\]</span>
To get the <span class="math inline">\(A\)</span> intersect <span class="math inline">\(B\)</span>, we need three objects: the set of <span class="math inline">\(A\)</span> units outside of <span class="math inline">\(B\)</span>, the set of <span class="math inline">\(B\)</span> units outside <span class="math inline">\(A\)</span>, and their joint set. You get all those, and you have <span class="math inline">\(A \cap B\)</span>.</p>
<p>Now it is just simple addition to find all missing values. Recall that <span class="math inline">\(A\)</span> is your team making playoffs and <span class="math inline">\(\Pr(A)=0.6\)</span>. And <span class="math inline">\(B\)</span> is the probability that the coach is rehired, <span class="math inline">\(\Pr(B)=0.8\)</span>. Also, <span class="math inline">\(\Pr(A,B)=0.5\)</span>, which is the probability of both <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> occurring. Then we have:</p>
<p><span class="math display">\[\begin{align}
   A              &amp; = A \cup B+A \cup{\sim}B \\
   A \cup{\sim}B  &amp; = A - A \cup B           \\
   \Pr(A, \sim B) &amp; = \Pr(A) - Pr(A,B)       \\
   \Pr(A,\sim B)  &amp; = 0.6 - 0.5              \\
   \Pr(A,\sim B)  &amp; = 0.1                    
\end{align}\]</span></p>
<p>When working with sets, it is important to understand that probability is calculated by considering the share of the set (for example <span class="math inline">\(A\)</span>) made up by the subset (for example <span class="math inline">\(A \cup B\)</span>). When we write down that the probability that <span class="math inline">\(A \cup B\)</span> occurs at all, it is with regards to <span class="math inline">\(U\)</span>. But what if we were to ask the question “What share of <em>A</em> is due to <span class="math inline">\(A \cup B\)</span>?” Notice, then, that we would need to do this:</p>
<p><span class="math display">\[\begin{align}
   ? &amp; = A \cup B \div A \\
   ? &amp; = 0.5 \div 0.6    \\
   ? &amp; = 0.83            
\end{align}\]</span></p>
<p>I left this intentionally undefined on the left side so as to focus on the calculation itself. But now let’s define what we are wanting to calculate: In a world where <span class="math inline">\(A\)</span> has occurred, what is the probability that <span class="math inline">\(B\)</span> will also occur? This is:</p>
<p><span class="math display">\[\begin{align}
   \Pr(B \mid A)  &amp; = \dfrac{\Pr(A,B)}{\Pr(A)}= \dfrac{0.5}{0.6}=0.83 \\
   \Pr(A \mid B) &amp; = \dfrac{\Pr(A,B)}{\Pr(B)}= \dfrac{0.5}{0.8}=0.63 
\end{align}\]</span>
Notice, these conditional probabilities are not as easy to see in the Venn diagram. We are essentially asking what percentage of a subset—e.g., <span class="math inline">\(\Pr(A)\)</span>—is due to the joint set, for example, <span class="math inline">\(\Pr(A,B)\)</span>. This reasoning is the very same reasoning used to define the concept of a conditional probability.</p>
</div>
<div id="contingency-tables" class="section level2" number="2.5">
<h2>
<span class="header-section-number">2.5</span> Contingency tables<a class="anchor" aria-label="anchor" href="#contingency-tables"><i class="fas fa-link"></i></a>
</h2>
<p>Another way that we can represent events is with a contingency table. Contingency tables are also sometimes called twoway tables. Table <a href="ch1.html#tab:twowaytable">2.4</a> is an example of a contingency table. We continue with our example about the worried Texas coach.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:twowaytable">Table 2.4: </span> Two way contingency table.</caption>
<thead><tr class="header">
<th align="left">Event labels</th>
<th align="left">Coach is not rehired <span class="math inline">\((B)\)</span>
</th>
<th align="left">Coach is rehired <span class="math inline">\((\sim B)\)</span>
</th>
<th align="left"><strong>Total</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">
<span class="math inline">\((A)\)</span> Bowl game</td>
<td align="left">
<span class="math inline">\(\Pr(A,\sim B)\)</span>=0.1</td>
<td align="left">
<span class="math inline">\(\Pr(A,B)\)</span>=0.5</td>
<td align="left">
<span class="math inline">\(\Pr(A)\)</span>=0.6</td>
</tr>
<tr class="even">
<td align="left">
<span class="math inline">\((\sim A)\)</span> no Bowl game</td>
<td align="left"><span class="math inline">\(\Pr(\sim A, \sim B)=0.1\)</span></td>
<td align="left"><span class="math inline">\(\Pr(\sim A,B)=0.3\)</span></td>
<td align="left"><span class="math inline">\(\Pr(B)=0.4\)</span></td>
</tr>
<tr class="odd">
<td align="left"><strong>Total</strong></td>
<td align="left"><span class="math inline">\(\Pr(\sim B)=0.2\)</span></td>
<td align="left"><span class="math inline">\(\Pr(B)=0.8\)</span></td>
<td align="left">1.0</td>
</tr>
</tbody>
</table></div>
<p>Recall that <span class="math inline">\(\Pr(A)=0.6\)</span>, <span class="math inline">\(\Pr(B)=0.8\)</span>, and <span class="math inline">\(Pr(A,B)=0.5\)</span>. Note that to calculate conditional probabilities, we must know the frequency of the element in question (e.g., <span class="math inline">\(\Pr(A,B)\)</span>) relative to some other larger event (e.g., <span class="math inline">\(\Pr(A)\)</span>). So if we want to know what the conditional probability of <span class="math inline">\(B\)</span> is given <span class="math inline">\(A\)</span>, then it’s:
<span class="math display">\[ 
\Pr(B\mid A)=\dfrac{\Pr(A,B)}{\Pr(A)}=\dfrac{0.5}{0.6}=0.83
\]</span>
But note that knowing the frequency of <span class="math inline">\(A\cup B\)</span> in a world where <span class="math inline">\(B\)</span> occurs is to ask the following:
<span class="math display">\[ 
\Pr(A\mid B)=\dfrac{\Pr(A,B)}{\Pr(B)}= \dfrac{0.5}{0.8}=0.63
\]</span></p>
<p>So, we can use what we have done so far to write out a definition of joint probability. Let’s start with a definition of conditional probability first. Given two events, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>:</p>
<p><span class="math display" id="eq:cond5">\[\begin{align}
   \Pr(A\mid B) &amp; =\dfrac{\Pr(A,B)}{\Pr(B)} \tag{2.1} \\
   \Pr(B\mid A) &amp; =\dfrac{\Pr(B,A)}{\Pr(A)} \tag{2.2} \\
   \Pr(A,B)     &amp; =\Pr(B,A) \tag{2.3}                 \\
   \Pr(A)       &amp; =\Pr(A,\sim B)+\Pr(A,B)                    
   \tag{2.4}
   \\
   \Pr(B)       &amp; =\Pr(A,B)+\Pr(\sim A, B)                   
   \tag{2.5}
\end{align}\]</span>
Using equations <a href="ch1.html#eq:cond1">(2.1)</a> and <a href="ch1.html#eq:cond2">(2.2)</a>, I can simply write down a definition of joint probabilities.
<span class="math display" id="eq:joing2">\[\begin{align}
   \Pr(A,B) = \Pr(A \mid B) \Pr(B) \tag{2.6}\\
   \Pr(B,A) = \Pr(B \mid A) \Pr(A) \tag{2.7}
\end{align}\]</span>
And this is the formula for joint probability. Given equation <a href="ch1.html#eq:cond3">(2.3)</a>, and using the definitions of <span class="math inline">\((\Pr(A,B\)</span> and <span class="math inline">\(\Pr(B,A))\)</span>, I can also rearrange terms, make a substitution, and rewrite it as:
<span class="math display" id="eq:bayes1">\[\begin{align}
   \Pr(A\mid B)\Pr(B) &amp; = \Pr(B\mid A)\Pr(A)                  
   \\
   \Pr(A\mid B)       &amp; = \dfrac{\Pr(B\mid A) \Pr(A)}{\Pr(B)} 
\end{align}\]</span>
\tag{2.8}</p>
<p>Equation <a href="ch1.html#eq:bayes1">(2.8)</a> is sometimes called the naive version of Bayes’s rule. We will now decompose this equation more fully, though, by substituting equation <a href="ch1.html#eq:cond5">(2.5)</a> into equation <a href="ch1.html#eq:bayes1">(2.8)</a>.</p>
<p><span class="math display" id="eq:cond6">\[\begin{align}
   \Pr(A\mid B)=\dfrac{\Pr(B\mid A)\Pr(A)}{\Pr(A,B)+ \Pr(\sim A,B)} \tag{2.9}
\end{align}\]</span>
Substituting equation <a href="ch1.html#eq:joint1">(2.6)</a> into the denominator for equation <a href="ch1.html#eq:cond6">(2.9)</a> yields:</p>
<p><span class="math display" id="eq:bayes2">\[\begin{align}
   \Pr(A\mid B)=\dfrac{\Pr(B\mid A)\Pr(A)}{\Pr(B\mid A)\Pr(A)+\Pr(\sim A, B)} \tag{2.10}
\end{align}\]</span>
Finally, we note that using the definition of joint probability, that <span class="math inline">\(\Pr(B,\sim A)= \Pr(B\mid\sim A)\Pr(\sim A)\)</span>, which we substitute into the denominator of equation <a href="ch1.html#eq:bayes2">(2.10)</a> to get:</p>
<p><span class="math display" id="eq:bayes3">\[\begin{align}
   \Pr(A\mid B)=\dfrac{\Pr(B\mid A)\Pr(A)}{\Pr(B\mid A)\Pr(A)+\Pr(B\mid \sim A)\Pr(\sim A)} \tag{2.11}
\end{align}\]</span></p>
<p>That’s a mouthful of substitutions, so what does equation <a href="ch1.html#eq:bayes3">(2.11)</a> mean? This is the Bayesian decomposition version of Bayes’s rule. Let’s use our example again of Texas making a great bowl game. <span class="math inline">\(A\)</span> is Texas making a great bowl game, and <span class="math inline">\(B\)</span> is the coach getting rehired. And <span class="math inline">\(A\cap B\)</span> is the joint probability that both events occur. We can make each calculation using the contingency tables. The questions here is this: If the Texas coach is rehired, what’s the probability that the Longhorns made a great bowl game? Or formally, <span class="math inline">\(\Pr(A\mid B)\)</span>. We can use the Bayesian decomposition to find this probability.</p>
<p><span class="math display">\[\begin{align}
   \Pr(A\mid B) &amp; = \dfrac{\Pr(B\mid A)\Pr(A)}{\Pr(B\mid A)\Pr(A)+\Pr(B\mid \sim A)\Pr(\sim A)} 
   \\
                &amp; =\dfrac{0.83\cdot 0.6}{0.83\cdot 0.6+0.75\cdot 0.4}                           
   \\
                &amp; =\dfrac{0.498}{0.498+0.3}                                                     \\
                &amp; =\dfrac{0.498}{0.798}                                                         \\
   \Pr(A\mid B) &amp; =0.624                                                                        
\end{align}\]</span>
Check this against the contingency table using the definition of joint probability:</p>
<p><span class="math display">\[\begin{align}
   \Pr(A\mid B)=\dfrac{\Pr(A,B)}{\Pr(B)}= \dfrac{0.5}{0.8}=0.625
\end{align}\]</span>
So, if the coach is rehired, there is a 63 percent chance we made a great bowl game.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Why are they different? Because 0.83 is an approximation of &lt;span class="math inline"&gt;\(\Pr(B\mid A)\)&lt;/span&gt;, which was technically 0.833…trailing.&lt;/p&gt;'><sup>13</sup></a></p>
</div>
<div id="monty-hall-example" class="section level2" number="2.6">
<h2>
<span class="header-section-number">2.6</span> Monty Hall example<a class="anchor" aria-label="anchor" href="#monty-hall-example"><i class="fas fa-link"></i></a>
</h2>
<p>Let’s use a different example, the Monty Hall example. This is a fun one, because most people find itcounterintuitive. It even is used to stump mathematicians and statisticians.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;There’s an ironic story in which someone posed the Monty Hall question to the columnist, Marilyn vos Savant. Vos Savant had an extremely high IQ and so people would send in puzzles to stump her. Without the Bayesian decomposition, using only logic, she got the answer right. Her column enraged people, though. Critics wrote in to mansplain how wrong she was, but in fact it was they who were wrong.&lt;/p&gt;"><sup>14</sup></a> But Bayes’s rule makes the answer very clear—so clear, in fact, that it’s somewhat surprising that Bayes’s rule was actually once controversial <span class="citation">(Mcgrayne <a href="references.html#ref-McGrayne2012" role="doc-biblioref">2012</a>)</span>.</p>
<p>Let’s assume three closed doors: door 1 <span class="math inline">\((D_1)\)</span>, door 2 <span class="math inline">\((D_2)\)</span>, and door 3 <span class="math inline">\((D_3)\)</span>. Behind one of the doors is a million dollars. Behind each of the other two doors is a goat. Monty Hall, the game-show host in this example, asks the contestants to pick a door. After they pick the door, but before he opens the door they picked, he opens one of the other doors to reveal a goat. He then asks the contestant, “Would you like to switch doors?”</p>
<p>A common response to Monty Hall’s offer is to say it makes no sense to change doors, because there’s an equal chance that the million dollars is behind either door. Therefore, why switch? There’s a 50–50 chance it’s behind the door picked and there’s a 50–50 chance it’s behind the remaining door, so it makes no rational sense to switch. Right? Yet, a little intuition should tell you that’s not the right answer, because it would seem that when Monty Hall opened that third door, he made a statement. But what exactly did he say?</p>
<p>Let’s formalize the problem using our probability notation. Assume that you chose door 1, <span class="math inline">\(D_1\)</span>. The probability that <span class="math inline">\(D_1\)</span> had a million dollars when you made that choice is <span class="math inline">\(\Pr(D_1=1 \text{ million})=\dfrac{1}{3}\)</span>. We will call that event <span class="math inline">\(A_1\)</span>. And the probability that <span class="math inline">\(D_1\)</span> has a million dollars at the start of the game is <span class="math inline">\(\dfrac{1}{3}\)</span> because the sample space is 3 doors, of which one has a million dollars behind it. Thus, <span class="math inline">\(\Pr(A_1)=\dfrac{1}{3}\)</span>. Also, by the law of total probability, <span class="math inline">\(\Pr(\sim A_1)=\dfrac{2}{3}\)</span>. Let’s say that Monty Hall had opened door 2, <span class="math inline">\(D_2\)</span>, to reveal a goat. Then he asked, “Would you like to change to door number 3?”</p>
<p>We need to know the probability that door 3 has the million dollars and compare that to Door 1’s probability. We will call the opening of door 2 event <span class="math inline">\(B\)</span>. We will call the probability that the million dollars is behind door <span class="math inline">\(i\)</span>, <span class="math inline">\(A_i\)</span>. We now write out the question just asked formally and decompose it using the Bayesian decomposition. We are ultimately interested in knowing what the probability is that door 1 has a million dollars (event <span class="math inline">\(A_1\)</span>) given that Monty Hall opened door 2 (event <span class="math inline">\(B\)</span>), which is a conditional probability question. Let’s write out that conditional probability using the Bayesian decomposition from equation <a href="ch1.html#eq:bayes3">(2.11)</a>.</p>
<p><span class="math display" id="eq:bayes4">\[\begin{align}
   \small
   \Pr(A_1 \mid B)=\dfrac{\Pr(B\mid A_1) \Pr(A_1)}{\Pr(B\mid A_1) \Pr(A_1)+\Pr(B\mid A_2) \Pr(A_2)+\Pr(B\mid A_3) \Pr(A_3)}\\
   \tag{2.12}
\end{align}\]</span></p>
<p>There are basically two kinds of probabilities on the right side of the equation. There’s the marginal probability that the million dollars is behind a given door, <span class="math inline">\(\Pr(A_i)\)</span>. And there’s the conditional probability that Monty Hall would open door 2 given that the million dollars is behind door <span class="math inline">\(A_i\)</span>, <span class="math inline">\(\Pr(B\mid A_i)\)</span>.</p>
<p>The marginal probability that door <span class="math inline">\(i\)</span> has the million dollars behind it without our having any additional information is <span class="math inline">\(\dfrac{1}{3}\)</span>. We call this the <em>prior probability</em>, or <em>prior belief</em>. It may also be called the <em>unconditional probability</em>.</p>
<p>The conditional probability, <span class="math inline">\(\Pr(B|A_i)\)</span>, requires a little more careful thinking. Take the first conditional probability, <span class="math inline">\(\Pr(B\mid A_1)\)</span>. If door 1 has the million dollars behind it, what’s the probability that Monty Hall would open door 2?</p>
<p>Let’s think about the second conditional probability: <span class="math inline">\(\Pr(B\mid A_2)\)</span>. If the money is behind door 2, what’s the probability that Monty Hall would open door 2?</p>
<p>And then the last conditional probability, <span class="math inline">\(\Pr(B\mid A_3)\)</span>. In a world where the money is behind door 3, what’s the probability Monty Hall will open door 2?</p>
<p>Each of these conditional probabilities requires thinking carefully about the feasibility of the events in question. Let’s examine the easiest question: <span class="math inline">\(\Pr(B\mid A_2)\)</span>. If the money is behind door 2, how likely is it for Monty Hall to open that same door, door 2? Keep in mind: this is a game show. So that gives you some idea about how the game-show host will behave. Do you think Monty Hall would open a door that had the million dollars behind it? It makes no sense to think he’d ever open a door that actually had the money behind it—he will always open a door with a goat. So don’t you think he’s only opening doors with goats? Let’s see what happens if take that intuition to its logical extreme and conclude that Monty Hall <em>never</em> opens a door if it has a million dollars. He <em>only</em> opens a door if the door has a goat. Under that assumption, we can proceed to estimate <span class="math inline">\(\Pr(A_1\mid B)\)</span> by substituting values for <span class="math inline">\(\Pr(B\mid A_i)\)</span> and <span class="math inline">\(\Pr(A_i)\)</span> into the right side of equation <a href="ch1.html#eq:bayes4">(2.12)</a>.</p>
<p>What then is <span class="math inline">\(\Pr(B\mid A_1)\)</span>? That is, in a world where <em>you</em> have chosen door 1, and the money is behind door 1, what is the probability that he would open door 2? There are two doors he could open if the money is behind door 1—he could open either door 2 or door 3, as both have a goat behind them. So <span class="math inline">\(\Pr(B\mid A_1)=0.5\)</span>.</p>
<p>What about the second conditional probability, <span class="math inline">\(\Pr(B\mid A_2)\)</span>? If the money is behind door 2, what’s the probability he will open it? Under our assumption that he never opens the door if it has a million dollars, we know this probability is 0.0. And finally, what about the third probability, <span class="math inline">\(\Pr(B\mid A_3)\)</span>? What is the probability he opens door 2 given that the money is behind door 3? Now consider this one carefully—the contestant has already chosen door 1, so he can’t open that one. And he can’t open door 3, because that has the money behind it. The only door, therefore, he could open is door 2. Thus, this probability is 1.0. Furthermore, all marginal probabilities, <span class="math inline">\(\Pr(A_i)\)</span>, equal 1/3, allowing us to solve for the conditional probability on the left side through substitution, multiplication, and division.</p>
<p><span class="math display">\[\begin{align}
   \Pr(A_1\mid B) &amp; = \dfrac{\dfrac{1}{2}\cdot                       
   \dfrac{1}{3}}{\dfrac{1}{2}\cdot
   \dfrac{1}{3}+0\cdot\dfrac{1}{3}+1.0 \cdot \dfrac{1}{3}}
   \\
                  &amp; =\dfrac{\dfrac{1}{6}}{\dfrac{1}{6}+\dfrac{2}{6}} 
   \\
                  &amp; = \dfrac{1}{3}                                   
\end{align}\]</span>
Aha. Now isn’t that just a little bit surprising? The probability that the contestant chose the correct door is <span class="math inline">\(\dfrac{1}{3}\)</span>, just as it was before Monty Hall opened door 2.</p>
<p>But what about the probability that door 3, the door you’re holding, has the million dollars? Have your beliefs about that likelihood changed now that door 2 has been removed from the equation? Let’s crank through our Bayesian decomposition and see whether we learned anything.</p>
<p><span class="math display">\[\begin{align}
   \Pr(A_3\mid B) &amp; = \dfrac{ \Pr(B\mid A_3)\Pr(A_3) }{ \Pr(B\mid A_3)\Pr(A_3)+\Pr(B\mid A_2)\Pr(A_2)+ \Pr(B\mid A_1)\Pr(A_1) }      
   \\
                  &amp; = \dfrac{ 1.0 \cdot \dfrac{1}{3} }{ 1.0 \cdot \dfrac{1}{3}+0 \cdot \dfrac{1}{3}+\dfrac{1}{2} \cdot \dfrac{1}{3}} 
   \\
                  &amp; = \dfrac{2}{3}                                                                                                   
\end{align}\]</span></p>
<p>Interestingly, while your beliefs about the door you originally chose haven’t changed, your beliefs about the other door have changed. The prior probability, <span class="math inline">\(\Pr(A_3)=\dfrac{1}{3}\)</span>, increased through a process called <em>updating</em> to a new probability of <span class="math inline">\(\Pr(A_3\mid B)=\dfrac{2}{3}\)</span>. This new conditional probability is called the <em>posterior probability</em>, or <em>posterior belief</em>. And it simply means that having witnessed <span class="math inline">\(B\)</span>, you learned information that allowed you to form a new belief about which door the money might be behind.</p>
<p>As was mentioned in footnote 14 regarding the controversy around vos Sant’s correct reasoning about the need to switch doors, deductions based on Bayes’s rule are often surprising even to smart people—probably because we lack coherent ways to correctly incorporate information into probabilities. Bayes’s rule shows us how to do that in a way that is logical and accurate. But besides being insightful, Bayes’s rule also opens the door for a different kind of reasoning about cause and effect. Whereas most of this book has to do with estimating effects from known causes, Bayes’s rule reminds us that we can form reasonable beliefs about causes from known effects.</p>
</div>
<div id="summation-operator" class="section level2" number="2.7">
<h2>
<span class="header-section-number">2.7</span> Summation operator<a class="anchor" aria-label="anchor" href="#summation-operator"><i class="fas fa-link"></i></a>
</h2>
<p>The tools we use to reason about causality rest atop a bedrock of probabilities. We are often working with mathematical tools and concepts from statistics such as expectations and probabilities. One of the most common tools we will use in this book is the linear regression model, but before we can dive into that, we have to build out some simple notation.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;For a more complete review of regression, see &lt;span class="citation"&gt;Wooldridge (&lt;a href="references.html#ref-Wooldridge2010" role="doc-biblioref"&gt;2010&lt;/a&gt;)&lt;/span&gt; and &lt;span class="citation"&gt;Wooldridge (&lt;a href="references.html#ref-Wooldridge2015" role="doc-biblioref"&gt;2015&lt;/a&gt;)&lt;/span&gt;. I stand on the shoulders of giants.&lt;/p&gt;'><sup>15</sup></a> We’ll begin with the summation operator. The Greek letter <span class="math inline">\(\Sigma\)</span> (the capital Sigma) denotes the summation operator. Let <span class="math inline">\(x_1, x_2, \ldots, x_n\)</span> be a sequence of numbers. We can compactly write a sum of numbers using the summation operator as:</p>
<p><span class="math display">\[\begin{align}
   \sum_{i=1}^nx_i \equiv x_1+x_2+\ldots+x_n
\end{align}\]</span>
The letter <span class="math inline">\(i\)</span> is called the index of summation. Other letters, such as <span class="math inline">\(j\)</span> or <span class="math inline">\(k\)</span>, are sometimes used as indices of summation. The subscript variable simply represents a specific value of a random variable, <span class="math inline">\(x\)</span>. The numbers 1 and <span class="math inline">\(n\)</span> are the lower limit and the upper limit, respectively, of the summation. The expression <span class="math inline">\(\Sigma_{i=1}^nx_i\)</span> can be stated in words as “sum the numbers <span class="math inline">\(x_i\)</span> for all values of <span class="math inline">\(i\)</span> from 1 to <span class="math inline">\(n\)</span>.” An example can help clarify:</p>
<p><span class="math display">\[\begin{align}
\sum_{i=6}^9 x_i= x_6+x_7+x_8+x_9
\end{align}\]</span>
The summation operator has three properties. The first property is called the constant rule. Formally, it is:
<span class="math display">\[ 
\text{For any constant }
   c{:}\quad \sum_{i=1}^nc=nc
\]</span>
Let’s consider an example. Say that we are given:
<span class="math display">\[ 
\sum_{i=1}^35=(5+5+5)=3 \cdot 5=15
\]</span>
A second property of the summation operator is:
<span class="math display">\[ 
\sum_{i=1}^ncx_i=c\sum_{i=1}^nx_i
\]</span>
Again let’s use an example. Say we are given:</p>
<p><span class="math display">\[\begin{align}
   \sum_{i=1}^3 5x_i &amp; =5x_1+5x_2+5x_3   
   \\
    &amp; =5 (x_1+x_2+x_3)  
   \\
    &amp; =5\sum_{i=1}^3x_i 
\end{align}\]</span>
We can apply both of these properties to get the following third property:</p>
<p><span class="math display">\[\begin{align}
   \text{For any constant $a$ and $b$:}\quad \sum_{i=1}^n(ax_i+by_i) =a\sum_{i=1}^n x_i + b\sum_{j=1}^n y_i
\end{align}\]</span>
Before leaving the summation operator, it is useful to also note things which are not properties of this operator. First, the summation of a ratio is not the ratio of the summations themselves.</p>
<p><span class="math display">\[\begin{align}
   \sum_i^n \dfrac{x_i}{y_i} \ne \dfrac{ \sum_{i=1}^n x_i}{\sum_{i=1}^ny_i}
\end{align}\]</span>
Second, the summation of some squared variable is not equal to the squaring of its summation.</p>
<p><span class="math display">\[\begin{align}
   \sum_{i=1}^nx_i^2 \ne
   \bigg(\sum_{i=1}^nx_i \bigg)^2
\end{align}\]</span></p>
<p>We can use the summation indicator to make a number of calculations, some of which we will do repeatedly over the course of this book. For instance, we can use the summation operator to calculate the average:
<span class="math display">\[\begin{align}
   \overline{x} &amp; = \dfrac{1}{n} \sum_{i=1}^n x_i 
   \\
&amp; =\dfrac{x_1+x_2+\dots+x_n}{n}   
\end{align}\]</span>
where <span class="math inline">\(\overline{x}\)</span> is the average (mean) of the random variable <span class="math inline">\(x_i\)</span>. Another calculation we can make is a random variable’s deviations from its own mean. The sum of the deviations from the mean is always equal to 0:
<span class="math display">\[ 
\sum_{i=1}^n (x_i - \overline{x})=0
\]</span>
You can see this in Table <a href="ch1.html#tab:deviation1">2.5</a>.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:deviation1">Table 2.5: </span> Sum of deviations equalling zero</caption>
<thead><tr class="header">
<th align="center"><span class="math inline">\(x\)</span></th>
<th align="right"><span class="math inline">\(x-\overline{x}\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="center">10</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="right"><span class="math inline">\(-4\)</span></td>
</tr>
<tr class="odd">
<td align="center">13</td>
<td align="right">5</td>
</tr>
<tr class="even">
<td align="center">5</td>
<td align="right"><span class="math inline">\(-3\)</span></td>
</tr>
<tr class="odd">
<td align="center">Mean=8</td>
<td align="right">Sum=0</td>
</tr>
</tbody>
</table></div>
<p>Consider a sequence of two numbers {<span class="math inline">\(y_1, y_2, \ldots, y_n\)</span>} and {<span class="math inline">\(x_1, x_2, \ldots, x_n\)</span>}. Now we can consider double summations over possible values of <span class="math inline">\(x\)</span>’s and <span class="math inline">\(y\)</span>’s. For example, consider the case where <span class="math inline">\(n=m=2\)</span>. Then, <span class="math inline">\(\sum_{i=1}^2\sum_{j=1}^2x_iy_j\)</span> is equal to <span class="math inline">\(x_1y_1+x_1y_2+x_2y_1+x_2y_2\)</span>. This is because</p>
<p><span class="math display">\[\begin{align}
   x_1y_1+x_1y_2+x_2y_1+x_2y_2
     &amp; = x_1(y_1+y_2)+x_2(y_1+y_2)                     \\
     &amp; = \sum_{i=1}^2x_i(y_1+y_2)                      \\
     &amp; = \sum_{i=1}^2x_i \bigg( \sum_{j=1}^2y_j \bigg) \\
     &amp; = \sum_{i=1}^2 \bigg( \sum_{j=1}^2x_iy_j \bigg) \\
     &amp; = \sum_{i=1}^2 \sum_{j=1}^2x_iy_j               
\end{align}\]</span>
One result that will be very useful throughout the book is:</p>
<p><span class="math display" id="eq:sqrddev1">\[ 
\sum_{i=1}^n(x_i - \overline{x})^2=\sum_{i=1}^n x_i^2 - n(\overline{x})^2 \tag{2.13}
\]</span></p>
<p>An overly long, step-by-step proof is below. Note that the summation index is suppressed after the first line for easier reading.</p>
<p><span class="math display">\[\begin{align}
   \sum_{i=1}^n(x_i-\overline{x})^2 &amp; =                                                                                               
   \sum_{i=1}^n (x_i^2-2x_i\overline{x}+\overline{x}^2)
   \\
            &amp; = \sum x_i^2 - 2\overline{x} \sum x_i +n\overline{x}^2                                          \\
            &amp; = \sum x_i^2 - 2 \dfrac{1}{n} \sum x_i \sum x_i +n\overline{x}^2                                \\
            &amp; = \sum x_i^2 +n\overline{x}^2 - \dfrac{2}{n} \bigg (\sum x_i \bigg )^2                          \\
            &amp; = \sum x_i^2+n\bigg (\dfrac{1}{n} \sum x_i \bigg)^2 - 2n \bigg (\dfrac{1}{n} \sum x_i \bigg )^2 \\
            &amp; = \sum x_i^2 - n \bigg (\dfrac{1}{n} \sum x_i \bigg )^2                                         \\
            &amp; = \sum x_i^2 - n \overline{x}^2                                                                 
\end{align}\]</span>
A more general version of this result is:
<span class="math display" id="eq:sqrddev2">\[\begin{align}
   \sum_{i=1}^n(x_i-\overline{x})(y_i-\overline{y}) &amp; = \sum_{i=1}^n x_i(y_i - \overline{y})   
   \\
            &amp; = \sum_{i=1}^n (x_i - \overline{x})y_i   
   \\
            &amp; = \sum_{i=1}^n x_iy_i - n(\overline{xy}) 
   \\
    \tag{2.14}
\end{align}\]</span></p>
<p>Or:
<span class="math display" id="eq:sqrddev3">\[ 
\sum_{i=1}^n (x_i -\overline{x})(y_i - \overline{y})= \sum_{i=1}^n x_i(y_i - \overline{y})=\sum_{i=1}^n (x_i - \overline{x})y_i=\sum_{i=1}^n x_iy_i - n(\overline{x}\overline{y}) \tag{2.15}
\]</span></p>
</div>
<div id="expected-value" class="section level2" number="2.8">
<h2>
<span class="header-section-number">2.8</span> Expected value<a class="anchor" aria-label="anchor" href="#expected-value"><i class="fas fa-link"></i></a>
</h2>
<p>The expected value of a random variable, also called the expectation and sometimes the population mean, is simply the weighted average of the possible values that the variable can take, with the weights being given by the probability of each value occurring in the population. Suppose that the variable <span class="math inline">\(X\)</span> can take on values <span class="math inline">\(x_1, x_2, \ldots, x_k\)</span>, each with probability <span class="math inline">\(f(x_1), f(x_2), \ldots, f(x_k)\)</span>, respectively. Then we define the expected value of <span class="math inline">\(X\)</span> as:
<span class="math display">\[\begin{align}
       E(X) &amp; = x_1f(x_1)+x_2f(x_2)+\dots+x_kf(x_k) \\
            &amp; = \sum_{j=1}^k x_jf(x_j)              
   \end{align}\]</span>
Let’s look at a numerical example. If <span class="math inline">\(X\)</span> takes on values of <span class="math inline">\(-1\)</span>, 0, and 2, with probabilities 0.3, 0.3, and 0.4, respectively.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;The law of total probability requires that all marginal probabilities sum to unity.&lt;/p&gt;"><sup>16</sup></a> Then the expected value of <span class="math inline">\(X\)</span> equals:</p>
<p><span class="math display">\[\begin{align}
   E(X) &amp; = (-1)(0.3)+(0)(0.3)+(2)(0.4) 
   \\
        &amp; = 0.5                         
\end{align}\]</span>
In fact, you could take the expectation of a function of that variable, too, such as <span class="math inline">\(X^2\)</span>. Note that <span class="math inline">\(X^2\)</span> takes only the values 1, 0, and 4, with probabilities 0.3, 0.3, and 0.4. Calculating the expected value of <span class="math inline">\(X^2\)</span> therefore is:</p>
<p><span class="math display">\[\begin{align}
   E(X^2) &amp; = (-1)^2(0.3)+(0)^2(0.3)+(2)^2(0.4) 
   \\
          &amp; = 1.9                               
\end{align}\]</span></p>
<p>The first property of expected value is that for any constant <span class="math inline">\(c\)</span>, <span class="math inline">\(E(c)=c\)</span>. The second property is that for any two constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, then <span class="math inline">\(E(aX+ b)=E(aX)+E(b)=aE(X)+b\)</span>. And the third property is that if we have numerous constants, <span class="math inline">\(a_1, \dots, a_n\)</span> and many random variables, <span class="math inline">\(X_1, \dots, X_n\)</span>, then the following is true:</p>
<p><span class="math display">\[\begin{align}
   E(a_1X_1+\dots+a_nX_n)=a_1E(X_1)+\dots+a_nE(X_n)
\end{align}\]</span>
We can also express this using the expectation operator:</p>
<p><span class="math display">\[\begin{align}
   E\bigg(\sum_{i=1}^na_iX_i\bigg)=\sum_{i=1}a_iE(X_i)
\end{align}\]</span>
And in the special case where <span class="math inline">\(a_i=1\)</span>, then</p>
<p><span class="math display">\[\begin{align}
   E\bigg(\sum_{i=1}^nX_i\bigg)=\sum_{i=1}^nE(X_i)
\end{align}\]</span></p>
</div>
<div id="variance" class="section level2" number="2.9">
<h2>
<span class="header-section-number">2.9</span> Variance<a class="anchor" aria-label="anchor" href="#variance"><i class="fas fa-link"></i></a>
</h2>
<p>The expectation operator, <span class="math inline">\(E(\cdot)\)</span>, is a population concept. It refers to the whole group of interest, not just to the sample available to us. Its meaning is somewhat similar to that of the average of a random variable in the population. Some additional properties for the expectation operator can be explained assuming two random variables, <span class="math inline">\(W\)</span> and <span class="math inline">\(H\)</span>.</p>
<p><span class="math display">\[\begin{align}
   E(aW+b)             &amp; = aE(W)+b\ \text{for any constants $a$, $b$} 
   \\
   E(W+H)              &amp; = E(W)+E(H)                                  \\
   E\Big(W - E(W)\Big) &amp; = 0                                          
\end{align}\]</span>
Consider the variance of a random variable, <span class="math inline">\(W\)</span>:</p>
<p><span class="math display">\[\begin{align}
   V(W)=\sigma^2=E\Big[\big(W-E(W)\big)^2\Big]\ \text{in the population}
\end{align}\]</span>
We can show
<span class="math display" id="eq:var1">\[ 
V(W)=E(W^2) - E(W)^2 \tag{2.16}
\]</span>
In a given sample of data, we can estimate the variance by the following calculation:</p>
<p><span class="math display">\[\begin{align}
   \widehat{S}^2=(n-1)^{-1}\sum_{i=1}^n(x_i - \overline{x})^2
\end{align}\]</span>
where we divide by <span class="math inline">\(n\ -\ 1\)</span> because we are making a degree-of-freedom adjustment from estimating the mean. But in large samples, this degree-of-freedom adjustment has no practical effect on the value of <span class="math inline">\(S^2\)</span> where <span class="math inline">\(S^2\)</span> is the average (after a degree of freedom correction) over the sum of all squared deviations from the mean.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Whenever possible, I try to use the “hat” to represent an estimated statistic. Hence &lt;span class="math inline"&gt;\(\widehat{S}^2\)&lt;/span&gt; instead of just &lt;span class="math inline"&gt;\(S^2\)&lt;/span&gt;. But it is probably more common to see the sample variance represented as &lt;span class="math inline"&gt;\(S^2\)&lt;/span&gt;.&lt;/p&gt;'><sup>17</sup></a></p>
<p>A few more properties of variance. First, the variance of a line is:
<span class="math display" id="eq:var2">\[ 
   V(aX+b)=a^2V(X) \tag{2.17}
\]</span></p>
<p>And the variance of a constant is 0 (i.e., <span class="math inline">\(V(c)=0\)</span> for any constant, <span class="math inline">\(c\)</span>). The variance of the sum of two random variables is equal to:
<span class="math display" id="eq:var3">\[ 
V(X+Y)=V(X)+V(Y)+2\Big(E(XY) - E(X)E(Y)\Big) \tag{2.18}
\]</span>
If the two variables are independent, then <span class="math inline">\(E(XY)=E(X)E(Y)\)</span> and <span class="math inline">\(V(X+Y)\)</span> is equal to the sum of <span class="math inline">\(V(X)+V(Y)\)</span>.</p>
</div>
<div id="covariance" class="section level2" number="2.10">
<h2>
<span class="header-section-number">2.10</span> Covariance<a class="anchor" aria-label="anchor" href="#covariance"><i class="fas fa-link"></i></a>
</h2>
<p>The last part of equation <a href="ch1.html#eq:var3">(2.18)</a> is called the covariance. The covariance measures the amount of linear dependence between two random variables. We represent it with the <span class="math inline">\(C(X,Y)\)</span> operator. The expression <span class="math inline">\(C(X,Y)&gt;0\)</span> indicates that two variables move in the same direction, whereas <span class="math inline">\(C(X,Y)&lt;0\)</span> indicates that they move in opposite directions. Thus we can rewrite equation <a href="ch1.html#eq:var3">(2.18)</a> as:</p>
<p><span class="math display">\[\begin{align}
   V(X+Y)=V(X)+V(Y)+2C(X,Y)
\end{align}\]</span>
While it’s tempting to say that a zero covariance means that two random variables are unrelated, that is incorrect. They could have a nonlinear relationship. The definition of covariance is
<span class="math display" id="eq:cov1">\[ 
C(X,Y)=E(XY) - E(X)E(Y) \tag{2.19}
\]</span>
As we said, if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(C(X,Y)=0\)</span> in the population. The covariance between two linear functions is:</p>
<p><span class="math display">\[\begin{align}
   C(a_1+b_1X, a_2+b_2Y)=b_1b_2C(X,Y)
\end{align}\]</span>
The two constants, <span class="math inline">\(a_1\)</span> and <span class="math inline">\(a_2\)</span>, zero out because their mean is themselves and so the difference equals 0.</p>
<p>Interpreting the magnitude of the covariance can be tricky. For that, we are better served by looking at correlation. We define correlation as follows. Let <span class="math inline">\(W=\dfrac{X-E(X)}{\sqrt{V(X)}}\)</span> and <span class="math inline">\(Z=\dfrac{Y - E(Y)}{\sqrt{V(Y)}}\)</span>. Then:
<span class="math display" id="eq:corr1">\[ 
\text{Corr}(W,Z)=\dfrac{C(X,Y)}{\sqrt{V(X)V(Y)}} \tag{2.20}
\]</span>
The correlation coefficient is bounded by <span class="math inline">\(-1\)</span> and 1. A positive (negative) correlation indicates that the variables move in the same (opposite) ways. The closer the coefficient is to 1 or <span class="math inline">\(-1\)</span>, the stronger the linear relationship is.</p>
</div>
<div id="population-model" class="section level2" number="2.11">
<h2>
<span class="header-section-number">2.11</span> Population model<a class="anchor" aria-label="anchor" href="#population-model"><i class="fas fa-link"></i></a>
</h2>
<p>We begin with cross-sectional analysis. We will assume that we can collect a random sample from the population of interest. Assume that there are two variables, <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, and we want to see how <span class="math inline">\(y\)</span> varies with changes in <span class="math inline">\(x\)</span>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;This is not necessarily causal language. We are speaking first and generally in terms of two random variables systematically moving together in some measurable way.&lt;/p&gt;"><sup>18</sup></a></p>
<p>There are three questions that immediately come up. One, what if <span class="math inline">\(y\)</span> is affected by factors other than <span class="math inline">\(x\)</span>? How will we handle that? Two, what is the functional form connecting these two variables? Three, if we are interested in the causal effect of <span class="math inline">\(x\)</span> on <span class="math inline">\(y\)</span>, then how can we distinguish that from mere correlation? Let’s start with a specific model.
<span class="math display" id="eq:beta1">\[ 
y=\beta_0+\beta_1x+u
   \tag{2.21}
\]</span>
This model is assumed to hold in the population. Equation <a href="ch1.html#eq:beta1">(2.21)</a> defines a linear bivariate regression model. For models concerned with capturing causal effects, the terms on the left side are usually thought of as the effect, and the terms on the right side are thought of as the causes.</p>
<p>Equation <a href="ch1.html#eq:beta1">(2.21)</a> explicitly allows for other factors to affect <span class="math inline">\(y\)</span> by including a random variable called the error term, <span class="math inline">\(u\)</span>. This equation also explicitly models the functional form by assuming that <span class="math inline">\(y\)</span> is linearly dependent on <span class="math inline">\(x\)</span>. We call the <span class="math inline">\(\beta_0\)</span> coefficient the intercept parameter, and we call the <span class="math inline">\(\beta_1\)</span> coefficient the slope parameter. These describe a population, and our goal in empirical work is to estimate their values. We never directly observe these parameters, because they are not data (I will emphasize this throughout the book). What we can do, though, is estimate these parameters using <em>data</em> and <em>assumptions</em>. To do this, we need credible assumptions to <em>accurately</em> estimate these parameters with data. We will return to this point later. In this simple regression framework, all unobserved variables that determine <span class="math inline">\(y\)</span> are subsumed by the error term <span class="math inline">\(u\)</span>.</p>
<p>First, we make a simplifying assumption without loss of generality. Let the expected value of <span class="math inline">\(u\)</span> be zero in the population. Formally:
<span class="math display" id="eq:beta2">\[ 
E(u)=0 \tag{2.22}
\]</span>
where <span class="math inline">\(E(\cdot)\)</span> is the expected value operator discussed earlier. If we normalize the <span class="math inline">\(u\)</span> random variable to be 0, it is of no consequence. Why? Because the presence of <span class="math inline">\(\beta_0\)</span> (the intercept term) always allows us this flexibility. If the average of <span class="math inline">\(u\)</span> is different from 0—for instance, say that it’s <span class="math inline">\(\alpha_0\)</span>—then we adjust the intercept. Adjusting the intercept has no effect on the <span class="math inline">\(\beta_1\)</span> slope parameter, though. For instance:</p>
<p><span class="math display">\[\begin{align}
   y=(\beta_0+\alpha_0)+\beta_1x+(u-\alpha_0)
\end{align}\]</span>
where <span class="math inline">\(\alpha_0=E(u)\)</span>. The new error term is <span class="math inline">\(u-\alpha_0\)</span>, and the new intercept term is <span class="math inline">\(\beta_0+ \alpha_0\)</span>. But while those two terms changed, notice what did <em>not</em> change: the slope, <span class="math inline">\(\beta_1\)</span>.</p>
</div>
<div id="mean-independence" class="section level2" number="2.12">
<h2>
<span class="header-section-number">2.12</span> Mean independence<a class="anchor" aria-label="anchor" href="#mean-independence"><i class="fas fa-link"></i></a>
</h2>
<p>An assumption that meshes well with our elementary treatment of statistics involves the mean of the error term for each “slice” of the population determined by values of <span class="math inline">\(x\)</span>:
<span class="math display" id="eq:exog1">\[ 
E(u\mid x)=E(u)\ \text{for all values $x$} \tag{2.23}
\]</span>
where <span class="math inline">\(E(u\mid x)\)</span> means the “expected value of <span class="math inline">\(u\)</span> given <span class="math inline">\(x\)</span>.” If equation <a href="ch1.html#eq:exog1">(2.23)</a> holds, then we say that <span class="math inline">\(u\)</span> is mean independent of <span class="math inline">\(x\)</span>.</p>
<p>An example might help here. Let’s say we are estimating the effect of schooling on wages, and <span class="math inline">\(u\)</span> is unobserved ability. Mean independence requires that <span class="math inline">\(E(\text{ability}\mid x=8)=E(\text{ability}\mid x=12)=E(\text{ability}\mid x=16)\)</span> so that the average ability is the same in the different portions of the population with an eighth-grade education, a twelfth-grade education, and a college education. Because people choose how much schooling to invest in based on their own unobserved skills and attributes, equation 2.27 is likely violated—at least in our example.</p>
<p>But let’s say we are willing to make this assumption. Then combining this new assumption, <span class="math inline">\(E(u\mid x)=E(u)\)</span> (the nontrivial assumption to make), with <span class="math inline">\(E(u)=0\)</span> (the normalization and trivial assumption), and you get the following new assumption:
<span class="math display" id="eq:exog2">\[ 
E(u\mid x)=0,\ \text{for all values $x$} \tag{2.24}
\]</span>
Equation <a href="ch1.html#eq:exog2">(2.24)</a> is called the zero conditional mean assumption and is a key identifying assumption in regression models. Because the conditional expected value is a linear operator, <span class="math inline">\(E(u\mid x)=0\)</span> implies that</p>
<p><span class="math display">\[\begin{align}
   E(y\mid x)=\beta_0+\beta_1x
\end{align}\]</span>
which shows the population regression function is a linear function of <span class="math inline">\(x\)</span>, or what <span class="citation">Angrist and Pischke (<a href="references.html#ref-Angrist2009" role="doc-biblioref">2009</a>)</span> call the conditional expectation function.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Notice that the conditional expectation passed through the linear function leaving a constant, because of the first property of the expectation operator, and a constant times &lt;span class="math inline"&gt;\(x\)&lt;/span&gt;. This is because the conditional expectation of &lt;span class="math inline"&gt;\(E[X\mid X]=X\)&lt;/span&gt;. This leaves us with &lt;span class="math inline"&gt;\(E[u\mid X]\)&lt;/span&gt; which under zero conditional mean is equal to 0.&lt;/p&gt;'><sup>19</sup></a> This relationship is crucial for the intuition of the parameter, <span class="math inline">\(\beta_1\)</span>, as a <em>causal</em> <em>parameter</em>.</p>
<div class="cover-box">
<div class="row">
    <div class="col-xs-8 col-md-4 cover-img">
        <a href="https://www.amazon.com/dp/0300251688"><img src="images/cover.jpg" alt="Buy Today!"></a>
    </div>
    
    <div class="col-xs-12 col-md-8 cover-text-box">
            <h2> 
                Causal Inference: 
                <br><span style="font-style: italic; font-weight:bold; font-size: 20px;">The Mixtape.</span>
            </h2> 
            
        <div class="cover-text">
            <p>Buy the print version today:</p>
            
            <div class="chips">
                <a href="https://www.amazon.com/dp/0300251688" class="app-chip"> 
                    <i class="fab fa-amazon" aria-hidden="true"></i> Buy from Amazon 
                </a>
    
                <a href="https://yalebooks.yale.edu/book/9780300251685/causal-inference" class="app-chip"> 
                    <i class="fas fa-book" aria-hidden="true"></i> Buy from Yale Press 
                </a>
            </div>
        </div>
    </div>
</div>
</div>
</div>
<div id="ordinary-least-squares" class="section level2" number="2.13">
<h2>
<span class="header-section-number">2.13</span> Ordinary least squares<a class="anchor" aria-label="anchor" href="#ordinary-least-squares"><i class="fas fa-link"></i></a>
</h2>
<p>Given data on <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, how can we estimate the population parameters, <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>? Let the pairs of <span class="math inline">\(\big\{(x_i,\ \textrm{and}\ y_i): i=1,2,\dots,n \big\}\)</span> be random samples of size from the population. Plug any observation into the population equation:</p>
<p><span class="math display">\[\begin{align}
   y_i=\beta_0+\beta_1x_i+u_i
\end{align}\]</span>
where <span class="math inline">\(i\)</span> indicates a particular observation. We observe <span class="math inline">\(y_i\)</span> and <span class="math inline">\(x_i\)</span> but not <span class="math inline">\(u_i\)</span>. We just know that <span class="math inline">\(u_i\)</span> is there. We then use the two population restrictions that we discussed earlier:</p>
<p><span class="math display">\[\begin{align}
   E(u)       &amp; =0  \\
   E(u\mid x) &amp; = 0 
\end{align}\]</span>
to obtain estimating equations for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. We talked about the first condition already. The second one, though, means that the mean value of <span class="math inline">\(x\)</span> does not change with different slices of the error term. This independence assumption implies <span class="math inline">\(E(xu)=0\)</span>, we get <span class="math inline">\(E(u)=0\)</span>, and <span class="math inline">\(C(x,u)=0\)</span>. Notice that if <span class="math inline">\(C(x,u)=0\)</span>, then that implies <span class="math inline">\(x\)</span> and <span class="math inline">\(u\)</span> are independent.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;See equation &lt;a href="ch1.html#eq:cov1"&gt;(2.19)&lt;/a&gt;.&lt;/p&gt;'><sup>20</sup></a> Next we plug in for <span class="math inline">\(u\)</span>, which is equal to <span class="math inline">\(y-\beta_0-\beta_1x\)</span>:</p>
<p><span class="math display">\[\begin{align}
   E(y-\beta_0-\beta_1x)           &amp; =0 \\
   \Big(x[y-\beta_0-\beta_1x]\Big) &amp; =0 
\end{align}\]</span>
These are the two conditions in the population that effectively determine <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. And again, note that the notation here is population concepts. We don’t have access to populations, though we do have their sample counterparts:</p>
<p><span class="math display" id="eq:mm2">\[\begin{align}
   \dfrac{1}{n}\sum_{i=1}^n\Big(y_i-\widehat{\beta_0}-\widehat{\beta_1}x_i\Big) &amp; =0 \tag{2.25} \\
   \dfrac{1}{n}\sum_{i=1}^n
   \Big(x_i\Big[y_i-\widehat{\beta_0}-\widehat{\beta_1}x_i
   \Big]\Big)         &amp; =0                
   \tag{2.26}
\end{align}\]</span>
where <span class="math inline">\(\widehat{\beta}_0\)</span> and <span class="math inline">\(\widehat{\beta}_1\)</span> are the estimates from the data.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Notice that we are dividing by &lt;span class="math inline"&gt;\(n\)&lt;/span&gt;, not &lt;span class="math inline"&gt;\(n-1\)&lt;/span&gt;. There is no degrees-of-freedom correction, in other words, when using samples to calculate means. There is a degrees-of-freedom correction when we start calculating higher moments.&lt;/p&gt;'><sup>21</sup></a> These are two linear equations in the two unknowns <span class="math inline">\(\widehat{\beta}_0\)</span> and <span class="math inline">\(\widehat{\beta}_1\)</span>. Recall the properties of the summation operator as we work through the following sample properties of these two equations. We begin with equation <a href="ch1.html#eq:mm1">(2.25)</a> and pass the summation operator through.</p>
<p><span class="math display">\[\begin{align}
   \dfrac{1}{n}\sum_{i=1}^n\Big(y_i-\widehat{\beta_0}-\widehat{\beta_1}x_i\Big) &amp; = \dfrac{1}{n}\sum_{i=1}^n(y_i) - \dfrac{1}{n}\sum_{i=1}^n\widehat{\beta_0} - \dfrac{1}{n}\sum_{i=1}^n\widehat{\beta_1}x_i \\
    &amp; = \dfrac{1}{n}\sum_{i=1}^n y_i - \widehat{\beta_0} - \widehat{\beta_1} \bigg( \dfrac{1}{n}\sum_{i=1}^n x_i \bigg)          \\
    &amp; = \overline{y} - \widehat{\beta_0} - \widehat{\beta_1} \overline{x}                                                        
\end{align}\]</span>
where <span class="math inline">\(\overline{y}=\dfrac{1}{n}\sum_{i=1}^n y_i\)</span> which is the average of the <span class="math inline">\(n\)</span> numbers <span class="math inline">\(\{y_i:1,\dots,n\}\)</span>. For emphasis we will call <span class="math inline">\(\overline{y}\)</span> the sample average. We have already shown that the first equation equals zero (equation <a href="ch1.html#eq:mm1">(2.25)</a>), so this implies <span class="math inline">\(\overline{y}=\widehat{\beta_0}+\widehat{\beta_1} \overline{x}\)</span>. So we now use this equation to write the intercept in terms of the slope:</p>
<p><span class="math display">\[\begin{align}
   \widehat{\beta_0}=\overline{y}-\widehat{\beta_1} \overline{x}
\end{align}\]</span>
We now plug <span class="math inline">\(\widehat{\beta_0}\)</span> into the second equation, <span class="math inline">\(\sum_{i=1}^n x_i (y_i-\widehat{\beta_0} - \widehat{\beta_1}x_i)=0\)</span>. This gives us the following (with some simple algebraic manipulation):</p>
<p><span class="math display">\[\begin{align}
   \sum_{i=1}^n x_i\Big[y_i-(\overline{y}- \widehat{\beta_1} \overline{x})-\widehat{\beta_1} x_i\Big] &amp; =0                                                                   
   \\
   \sum_{i=1}^n x_i(y_i-\overline{y})       &amp; = \widehat{\beta_1} \bigg[ \sum_{i=1}^n x_i(x_i- \overline{x})\bigg] 
\end{align}\]</span>
So the equation to solve is<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Recall from much earlier that: &lt;span class="math display"&gt;\[\begin{align}
       \sum_{i=1}^n (x_i -\overline{x})(y_i-\overline{y}) &amp;amp; = \sum_{i=1}^n x_i(y_i-\overline{y})                       
       \\
&amp;amp; =\sum_{i=1}^n(x_i-\overline{x})y_i                         
       \\
&amp;amp; =\sum_{i=1}^n x_iy_i-n(\overline{x}\overline{y})
\end{align}\]&lt;/span&gt;&lt;/p&gt;'><sup>22</sup></a></p>
<p><span class="math display">\[\begin{align}
   \sum_{i=1}^n (x_i-\overline{x}) (y_i- \overline{y})=\widehat{\beta_1} \bigg[ \sum_{i=1}^n (x_i - \overline{x})^2 \bigg]
\end{align}\]</span>
If <span class="math inline">\(\sum_{i=1}^n(x_i-\overline{x})^2\ne0\)</span>, we can write:
<span class="math display">\[\begin{align}
       \widehat{\beta}_1 &amp; =                                                                        
       \dfrac{\sum_{i=1}^n (x_i-\overline{x})
       (y_i-\overline{y})}{\sum_{i=1}^n(x_i-\overline{x})^2 }
       \\
 &amp; =\dfrac{\text{Sample covariance}(x_i,y_i) }{\text{Sample variance}(x_i)} 
   \end{align}\]</span></p>
<p>The previous formula for <span class="math inline">\(\widehat{\beta}_1\)</span> is important because it shows us how to take data that we have and compute the slope estimate. The estimate, <span class="math inline">\(\widehat{\beta}_1\)</span>, is commonly referred to as the ordinary least squares (OLS) slope estimate. It can be computed whenever the sample variance of <span class="math inline">\(x_i\)</span> isn’t 0. In other words, it can be computed if <span class="math inline">\(x_i\)</span> is not constant across all values of <span class="math inline">\(i\)</span>. The intuition is that the variation in <span class="math inline">\(x\)</span> is what permits us to identify its impact in <span class="math inline">\(y\)</span>. This also means, though, that we cannot determine the slope in a relationship if we observe a sample in which everyone has the same years of schooling, or whatever causal variable we are interested in.</p>
<p>Once we have calculated <span class="math inline">\(\widehat{\beta}_1\)</span>, we can compute the intercept value, <span class="math inline">\(\widehat{\beta}_0\)</span>, as <span class="math inline">\(\widehat{\beta}_0=\overline{y} - \widehat{\beta}_1\overline{x}\)</span>. This is the OLS intercept estimate because it is calculated using sample averages. Notice that it is straightforward because <span class="math inline">\(\widehat{\beta}_0\)</span> is linear in <span class="math inline">\(\widehat{\beta}_1\)</span>. With computers and statistical programming languages and software, we let our computers do these calculations because even when <span class="math inline">\(n\)</span> is small, these calculations are quite tedious.</p>
<p>For any candidate estimates, <span class="math inline">\(\widehat{\beta}_0, \widehat{\beta}_1\)</span>, we define a fitted value for each <span class="math inline">\(i\)</span> as:</p>
<p><span class="math display">\[\begin{align}
   \widehat{y_i}=\widehat{\beta}_0+\widehat{\beta}_1x_i
\end{align}\]</span>
Recall that <span class="math inline">\(i=\{1, \ldots, n\}\)</span>, so we have <span class="math inline">\(n\)</span> of these equations. This is the value we predict for <span class="math inline">\(y_i\)</span> given that <span class="math inline">\(x=x_i\)</span>. But there is prediction error because <span class="math inline">\(y\ne y_i\)</span>. We call that mistake the residual, and here use the <span class="math inline">\(\widehat{u_i}\)</span> notation for it. So the residual equals:</p>
<p><span class="math display">\[\begin{align}
   \widehat{u_i} &amp; = y_i-\widehat{y_i}                          
   \\
   \widehat{u_i} &amp; = y_i-\widehat{\beta_0}-\widehat{\beta_1}x_i 
\end{align}\]</span>
While both the residual and the error term are represented with a <span class="math inline">\({u}\)</span>, it is important that you know the differences. The residual is the prediction error based on our fitted <span class="math inline">\(\widehat{y}\)</span> and the actual <span class="math inline">\(y\)</span>. The residual is therefore easily calculated with any sample of data. But <span class="math inline">\(u\)</span> without the hat is the <em>error term</em>, and it is by definition unobserved by the researcher. Whereas the residual will appear in the data set once generated from a few steps of regression and manipulation, the error term will never appear in the data set. It is all of the determinants of our outcome not captured by our model. This is a crucial distinction, and strangely enough it is so subtle that even some seasoned researchers struggle to express it.</p>
<p>Suppose we measure the size of the mistake, for each <span class="math inline">\(i\)</span>, by squaring it. Squaring it will, after all, eliminate all negative values of the mistake so that everything is a positive value. This becomes useful when summing the mistakes if we don’t want positive and negative values to cancel one another out. So let’s do that: square the mistake and add them all up to get <span class="math inline">\(\sum_{i=1}^n \widehat{u_i}^2\)</span>:</p>
<p><span class="math display">\[\begin{align}
   \sum_{i=1}^n \widehat{u_i}^2 &amp; =\sum_{i=1}^n (y_i - \widehat{y_i})^2                                 \\
        &amp; = \sum_{i=1}^n \Big(y_i-\widehat{\beta_0}-\widehat{\beta_1}x_i\Big)^2 
\end{align}\]</span>
This equation is called the sum of squared residuals because the residual is <span class="math inline">\(\widehat{u_i}=y_i-\widehat{y}\)</span>. But the residual is based on estimates of the slope and the intercept. We can imagine any number of estimates of those values. But what if our goal is to <em>minimize</em> the sum of squared residuals by choosing <span class="math inline">\(\widehat{\beta_0}\)</span> and <span class="math inline">\(\widehat{\beta_1}\)</span>? Using calculus, it can be shown that the solutions to that problem yield parameter estimates that are the same as what we obtained before.</p>
<p>Once we have the numbers <span class="math inline">\(\widehat{\beta}_0\)</span> and <span class="math inline">\(\widehat{\beta}_1\)</span> for a given data set, we write the OLS regression line:
<span class="math display">\[ 
\widehat{y}=\widehat{\beta_0}+\widehat{\beta_1}x
\]</span>
Let’s consider a short simulation.</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/ols.do"><code>ols.do</code></a></em></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb1-1"><a href="ch1.html#cb1-1" aria-hidden="true"></a><span class="kw">set</span> <span class="dv">seed</span> 1 </span>
<span id="cb1-2"><a href="ch1.html#cb1-2" aria-hidden="true"></a><span class="kw">clear</span> </span>
<span id="cb1-3"><a href="ch1.html#cb1-3" aria-hidden="true"></a><span class="kw">set</span> <span class="kw">obs</span> 10000 </span>
<span id="cb1-4"><a href="ch1.html#cb1-4" aria-hidden="true"></a><span class="kw">gen</span> x = rnormal() </span>
<span id="cb1-5"><a href="ch1.html#cb1-5" aria-hidden="true"></a><span class="kw">gen</span> u  = rnormal() </span>
<span id="cb1-6"><a href="ch1.html#cb1-6" aria-hidden="true"></a><span class="kw">gen</span> <span class="fu">y</span>  = 5.5*x + 12*u </span>
<span id="cb1-7"><a href="ch1.html#cb1-7" aria-hidden="true"></a><span class="kw">reg</span> <span class="fu">y</span> x </span>
<span id="cb1-8"><a href="ch1.html#cb1-8" aria-hidden="true"></a><span class="kw">predict</span> yhat1 </span>
<span id="cb1-9"><a href="ch1.html#cb1-9" aria-hidden="true"></a><span class="kw">gen</span> yhat2 = -0.0750109  + 5.598296*x <span class="co">// Compare yhat1 and yhat2</span></span>
<span id="cb1-10"><a href="ch1.html#cb1-10" aria-hidden="true"></a><span class="kw">sum</span> yhat* </span>
<span id="cb1-11"><a href="ch1.html#cb1-11" aria-hidden="true"></a><span class="kw">predict</span> uhat1, residual </span>
<span id="cb1-12"><a href="ch1.html#cb1-12" aria-hidden="true"></a><span class="kw">gen</span> uhat2=<span class="fu">y</span>-yhat2 </span>
<span id="cb1-13"><a href="ch1.html#cb1-13" aria-hidden="true"></a><span class="kw">sum</span> uhat* </span>
<span id="cb1-14"><a href="ch1.html#cb1-14" aria-hidden="true"></a><span class="kw">twoway</span> (<span class="kw">lfit</span> <span class="fu">y</span> x, lcolor(<span class="bn">black</span>) lwidth(medium)) (<span class="kw">scatter</span> <span class="fu">y</span> x, mcolor(<span class="bn">black</span>) <span class="co">///</span></span>
<span id="cb1-15"><a href="ch1.html#cb1-15" aria-hidden="true"></a>msize(tiny) <span class="bn">msymbol</span>(point)), <span class="bn">title</span>(OLS Regression Line) </span>
<span id="cb1-16"><a href="ch1.html#cb1-16" aria-hidden="true"></a><span class="kw">rvfplot</span>, <span class="kw">yline</span>(0) </span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/ols.R"><code>ols.R</code></a></em></p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span>
<span class="va">tb</span> <span class="op">&lt;-</span> <span class="fu">tibble</span><span class="op">(</span>
  x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">10000</span><span class="op">)</span>,
  u <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">10000</span><span class="op">)</span>,
  y <span class="op">=</span> <span class="fl">5.5</span><span class="op">*</span><span class="va">x</span> <span class="op">+</span> <span class="fl">12</span><span class="op">*</span><span class="va">u</span>
<span class="op">)</span> 

<span class="va">reg_tb</span> <span class="op">&lt;-</span> <span class="va">tb</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, <span class="va">.</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="op">)</span>

<span class="va">reg_tb</span><span class="op">$</span><span class="va">coefficients</span>

<span class="va">tb</span> <span class="op">&lt;-</span> <span class="va">tb</span> <span class="op">%&gt;%</span> 
  <span class="fu">mutate</span><span class="op">(</span>
    yhat1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, <span class="va">.</span><span class="op">)</span><span class="op">)</span>,
    yhat2 <span class="op">=</span> <span class="fl">0.0732608</span> <span class="op">+</span> <span class="fl">5.685033</span><span class="op">*</span><span class="va">x</span>, 
    uhat1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, <span class="va">.</span><span class="op">)</span><span class="op">)</span>,
    uhat2 <span class="op">=</span> <span class="va">y</span> <span class="op">-</span> <span class="va">yhat2</span>
  <span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">tb</span><span class="op">[</span><span class="op">-</span><span class="fl">1</span><span class="op">:</span><span class="op">-</span><span class="fl">3</span><span class="op">]</span><span class="op">)</span>

<span class="va">tb</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, <span class="va">.</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x<span class="op">=</span><span class="va">x</span>, y<span class="op">=</span><span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu">ggtitle</span><span class="op">(</span><span class="st">"OLS Regression Line"</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">geom_point</span><span class="op">(</span>size <span class="op">=</span> <span class="fl">0.05</span>, color <span class="op">=</span> <span class="st">"black"</span>, alpha <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">geom_smooth</span><span class="op">(</span>method <span class="op">=</span> <span class="va">lm</span>, color <span class="op">=</span> <span class="st">"black"</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">annotate</span><span class="op">(</span><span class="st">"text"</span>, x <span class="op">=</span> <span class="op">-</span><span class="fl">1.5</span>, y <span class="op">=</span> <span class="fl">30</span>, color <span class="op">=</span> <span class="st">"red"</span>, 
           label <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Intercept = "</span>, <span class="op">-</span><span class="fl">0.0732608</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">annotate</span><span class="op">(</span><span class="st">"text"</span>, x <span class="op">=</span> <span class="fl">1.5</span>, y <span class="op">=</span> <span class="op">-</span><span class="fl">30</span>, color <span class="op">=</span> <span class="st">"blue"</span>, 
           label <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Slope ="</span>, <span class="fl">5.685033</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p>Let’s look at the output from this. First, if you summarize the data, you’ll see that the fitted values are produced both using Stata’s Predict command and manually using the Generate command. I wanted the reader to have a chance to better understand this, so did it both ways. But second, let’s look at the data and paste on top of it the estimated coefficients, the y-intercept and slope on <span class="math inline">\(x\)</span> in Figure <a href="ch1.html#fig:resid1">2.1</a>. The estimated coefficients in both are close to the hard coded values built into the data-generating process.</p>
<div class="figure">
<span id="fig:resid1"></span>
<img src="graphics/ols.jpg" alt="Graphical representation of bivariate regression from $y$ on $x$" width="100%"><p class="caption">
Figure 2.1: Graphical representation of bivariate regression from <span class="math inline">\(y\)</span> on <span class="math inline">\(x\)</span>
</p>
</div>
<p>Once we have the estimated coefficients and we have the OLS regression line, we can predict <span class="math inline">\(y\)</span> (outcome) for any (sensible) value of <span class="math inline">\(x\)</span>. So plug in certain values of <span class="math inline">\(x\)</span>, and we can immediately calculate what <span class="math inline">\(y\)</span> will probably be with some error. The value of OLS here lies in how large that error is: OLS minimizes the error for a linear function. In fact, it is the best such guess at <span class="math inline">\(y\)</span> for all linear estimators because it minimizes the prediction error. There’s always prediction error, in other words, with any estimator, but OLS is the least worst.</p>
<p>Notice that the intercept is the predicted value of <span class="math inline">\(y\)</span> if and when <span class="math inline">\(x=0\)</span>. In this sample, that value is <span class="math inline">\(-0.0750109\)</span>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;It isn’t exactly 0 even though &lt;span class="math inline"&gt;\(u\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(x\)&lt;/span&gt; are independent. Think of it as &lt;span class="math inline"&gt;\(u\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(x\)&lt;/span&gt; are independent in the population, but not in the sample. This is because sample characteristics tend to be slightly different from population properties due to sampling error.&lt;/p&gt;'><sup>23</sup></a> The slope allows us to predict changes in <span class="math inline">\(y\)</span> for any reasonable change in <span class="math inline">\(x\)</span> according to:</p>
<p><span class="math display">\[\begin{align}
   \Delta \widehat{y}=\widehat{\beta}_1 \Delta x
\end{align}\]</span>
And if <span class="math inline">\(\Delta x=1\)</span>, then <span class="math inline">\(x\)</span> increases by one unit, and so <span class="math inline">\(\Delta \widehat{y}=5.598296\)</span> in our numerical example because <span class="math inline">\(\widehat{\beta}_1=5.598296\)</span>.</p>
<p>Now that we have calculated <span class="math inline">\(\widehat{\beta}_0\)</span> and <span class="math inline">\(\widehat{\beta}_1\)</span>, we get the OLS fitted values by plugging <span class="math inline">\(x_i\)</span> into the following equation for <span class="math inline">\(i=1,\dots,n\)</span>:</p>
<p><span class="math display">\[\begin{align}
   \widehat{y_i}=\widehat{\beta}_0+\widehat{\beta}_1x_i
\end{align}\]</span>
The OLS residuals are also calculated by:</p>
<p><span class="math display">\[\begin{align}
   \widehat{u_i}=y_i - \widehat{\beta}_0 - \widehat{\beta}_1 x_i
\end{align}\]</span>
Most residuals will be different from 0 (i.e., they do not lie on the regression line). You can see this in Figure <a href="ch1.html#fig:resid1">2.1</a>. Some are positive, and some are negative. A positive residual indicates that the regression line (and hence, the predicted values) underestimates the true value of <span class="math inline">\(y_i\)</span>. And if the residual is negative, then the regression line overestimates the true value.</p>
<p>Recall that we defined the fitted value as <span class="math inline">\(\widehat{y_i}\)</span> and the residual, <span class="math inline">\(\widehat{u_i}\)</span>, as <span class="math inline">\(y_i - \widehat{y_i}\)</span>. Notice that the scatter-plot relationship between the residuals and the fitted values created a spherical pattern, suggesting that they are not correlated (Figure <a href="ch1.html#fig:resid3">2.2</a>). This is mechanical—least squares produces residuals which are uncorrelated with fitted values. There’s no magic here, just least squares.</p>
<div class="figure">
<span id="fig:resid3"></span>
<img src="graphics/residuals.jpg" alt="Distribution of residuals around regression line" width="100%"><p class="caption">
Figure 2.2: Distribution of residuals around regression line
</p>
</div>
</div>
<div id="algebraic-properties-of-ols" class="section level2" number="2.14">
<h2>
<span class="header-section-number">2.14</span> Algebraic Properties of OLS<a class="anchor" aria-label="anchor" href="#algebraic-properties-of-ols"><i class="fas fa-link"></i></a>
</h2>
<p>Remember how we obtained <span class="math inline">\(\widehat{\beta}_0\)</span> and <span class="math inline">\(\widehat{\beta}_1\)</span>? When an intercept is included, we have:</p>
<p><span class="math display">\[\begin{align}
   \sum_{i=1}^n\Big(y_i-\widehat{\beta}_0 -\widehat{\beta}_1x_i\Big) =0
\end{align}\]</span>
The OLS residual <em>always</em> adds up to zero, by <em>construction</em>.
<span class="math display" id="eq:summ1">\[ 
\sum_{i=1}^n \widehat{u_i}=0
   \tag{2.27}
\]</span>
Sometimes seeing is believing, so let’s look at this together. Type the following into Stata verbatim.</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/ols2.do"><code>ols2.do</code></a></em></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb3-1"><a href="ch1.html#cb3-1" aria-hidden="true"></a><span class="kw">clear</span> </span>
<span id="cb3-2"><a href="ch1.html#cb3-2" aria-hidden="true"></a><span class="kw">set</span> <span class="dv">seed</span> 1234</span>
<span id="cb3-3"><a href="ch1.html#cb3-3" aria-hidden="true"></a><span class="kw">set</span> <span class="kw">obs</span> 10</span>
<span id="cb3-4"><a href="ch1.html#cb3-4" aria-hidden="true"></a><span class="kw">gen</span> x = 9*rnormal() </span>
<span id="cb3-5"><a href="ch1.html#cb3-5" aria-hidden="true"></a><span class="kw">gen</span> u  = 36*rnormal() </span>
<span id="cb3-6"><a href="ch1.html#cb3-6" aria-hidden="true"></a><span class="kw">gen</span> <span class="fu">y</span>  = 3 + 2*x + u</span>
<span id="cb3-7"><a href="ch1.html#cb3-7" aria-hidden="true"></a><span class="kw">reg</span> <span class="fu">y</span> x</span>
<span id="cb3-8"><a href="ch1.html#cb3-8" aria-hidden="true"></a><span class="kw">predict</span> yhat</span>
<span id="cb3-9"><a href="ch1.html#cb3-9" aria-hidden="true"></a><span class="kw">predict</span> residuals, residual</span>
<span id="cb3-10"><a href="ch1.html#cb3-10" aria-hidden="true"></a>su residuals</span>
<span id="cb3-11"><a href="ch1.html#cb3-11" aria-hidden="true"></a><span class="ot">list</span></span>
<span id="cb3-12"><a href="ch1.html#cb3-12" aria-hidden="true"></a><span class="kw">collapse</span> (<span class="kw">sum</span>) x u <span class="fu">y</span> yhat residuals</span>
<span id="cb3-13"><a href="ch1.html#cb3-13" aria-hidden="true"></a><span class="ot">list</span></span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/ols2.R"><code>ols2.R</code></a></em></p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span>

<span class="va">tb</span> <span class="op">&lt;-</span> <span class="fu">tibble</span><span class="op">(</span>
  x <span class="op">=</span> <span class="fl">9</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">10</span><span class="op">)</span>,
  u <span class="op">=</span> <span class="fl">36</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">10</span><span class="op">)</span>,
  y <span class="op">=</span> <span class="fl">3</span> <span class="op">+</span> <span class="fl">2</span><span class="op">*</span><span class="va">x</span> <span class="op">+</span> <span class="va">u</span>,
  yhat <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span><span class="op">)</span><span class="op">)</span>,
  uhat <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span><span class="op">)</span><span class="op">)</span>
<span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">tb</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/colSums.html">colSums</a></span><span class="op">(</span><span class="va">tb</span><span class="op">)</span></code></pre></div>
<p>Output from this can be summarized as in the following table (Table <a href="ch1.html#tab:ols-residuals">2.6</a>).</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:ols-residuals">Table 2.6: </span> Simulated data showing the sum of residuals equals zero</caption>
<thead><tr class="header">
<th align="left">no.</th>
<th align="center"><span class="math inline">\(x\)</span></th>
<th align="center"><span class="math inline">\(u\)</span></th>
<th align="center"><span class="math inline">\(y\)</span></th>
<th align="left"><span class="math inline">\(\widehat{y}\)</span></th>
<th align="left"><span class="math inline">\(\widehat{u}\)</span></th>
<th align="left"><span class="math inline">\(x\widehat{u}\)</span></th>
<th align="left"><span class="math inline">\(\widehat{y}\widehat{u}\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">1.</td>
<td align="center"><span class="math inline">\(-4.381653\)</span></td>
<td align="center"><span class="math inline">\(-32.95803\)</span></td>
<td align="center"><span class="math inline">\(-38.72134\)</span></td>
<td align="left"><span class="math inline">\(-3.256034\)</span></td>
<td align="left"><span class="math inline">\(-35.46531\)</span></td>
<td align="left">155.3967</td>
<td align="left">115.4762</td>
</tr>
<tr class="even">
<td align="left">2.</td>
<td align="center"><span class="math inline">\(-13.28403\)</span></td>
<td align="center"><span class="math inline">\(-8.028061\)</span></td>
<td align="center"><span class="math inline">\(-31.59613\)</span></td>
<td align="left"><span class="math inline">\(-26.30994\)</span></td>
<td align="left"><span class="math inline">\(-5.28619\)</span></td>
<td align="left">70.22192</td>
<td align="left">139.0793</td>
</tr>
<tr class="odd">
<td align="left">3.</td>
<td align="center"><span class="math inline">\(-.0982034\)</span></td>
<td align="center">17.80379</td>
<td align="center">20.60738</td>
<td align="left">7.836532</td>
<td align="left">12.77085</td>
<td align="left"><span class="math inline">\(-1.254141\)</span></td>
<td align="left">100.0792</td>
</tr>
<tr class="even">
<td align="left">4.</td>
<td align="center"><span class="math inline">\(-.1238423\)</span></td>
<td align="center"><span class="math inline">\(-9.443188\)</span></td>
<td align="center"><span class="math inline">\(-6.690872\)</span></td>
<td align="left">7.770137</td>
<td align="left"><span class="math inline">\(-14.46101\)</span></td>
<td align="left">1.790884</td>
<td align="left"><span class="math inline">\(-112.364\)</span></td>
</tr>
<tr class="odd">
<td align="left">5.</td>
<td align="center">4.640209</td>
<td align="center">13.18046</td>
<td align="center">25.46088</td>
<td align="left">20.10728</td>
<td align="left">5.353592</td>
<td align="left">24.84179</td>
<td align="left">107.6462</td>
</tr>
<tr class="even">
<td align="left">6.</td>
<td align="center"><span class="math inline">\(-1.252096\)</span></td>
<td align="center"><span class="math inline">\(-34.64874\)</span></td>
<td align="center"><span class="math inline">\(-34.15294\)</span></td>
<td align="left">4.848374</td>
<td align="left"><span class="math inline">\(-39.00131\)</span></td>
<td align="left">48.83337</td>
<td align="left"><span class="math inline">\(-189.0929\)</span></td>
</tr>
<tr class="odd">
<td align="left">7.</td>
<td align="center">11.58586</td>
<td align="center">9.118524</td>
<td align="center">35.29023</td>
<td align="left">38.09396</td>
<td align="left"><span class="math inline">\(-2.80373\)</span></td>
<td align="left"><span class="math inline">\(-32.48362\)</span></td>
<td align="left"><span class="math inline">\(-106.8052\)</span></td>
</tr>
<tr class="even">
<td align="left">8.</td>
<td align="center"><span class="math inline">\(-5.289957\)</span></td>
<td align="center">82.23296</td>
<td align="center">74.65305</td>
<td align="left"><span class="math inline">\(-5.608207\)</span></td>
<td align="left">80.26126</td>
<td align="left"><span class="math inline">\(-424.5786\)</span></td>
<td align="left"><span class="math inline">\(-450.1217\)</span></td>
</tr>
<tr class="odd">
<td align="left">9.</td>
<td align="center"><span class="math inline">\(-.2754041\)</span></td>
<td align="center">11.60571</td>
<td align="center">14.0549</td>
<td align="left">7.377647</td>
<td align="left">6.677258</td>
<td align="left"><span class="math inline">\(-1.838944\)</span></td>
<td align="left">49.26245</td>
</tr>
<tr class="even">
<td align="left">10.</td>
<td align="center"><span class="math inline">\(-19.77159\)</span></td>
<td align="center"><span class="math inline">\(-14.61257\)</span></td>
<td align="center"><span class="math inline">\(-51.15575\)</span></td>
<td align="left"><span class="math inline">\(-43.11034\)</span></td>
<td align="left"><span class="math inline">\(-8.045414\)</span></td>
<td align="left">159.0706</td>
<td align="left">346.8405</td>
</tr>
<tr class="odd">
<td align="left">Sum</td>
<td align="center"><span class="math inline">\(-28.25072\)</span></td>
<td align="center">34.25085</td>
<td align="center">7.749418</td>
<td align="left">7.749418</td>
<td align="left">1.91e-06</td>
<td align="left"><span class="math inline">\(-6.56e-06\)</span></td>
<td align="left">.0000305</td>
</tr>
</tbody>
</table></div>
<p>Notice the difference between the <span class="math inline">\(u\)</span>, <span class="math inline">\(\widehat{y}\)</span>, and <span class="math inline">\(\widehat{u}\)</span> columns. When we sum these ten lines, neither the error term nor the fitted values of <span class="math inline">\(y\)</span> sum to zero. But the residuals <em>do</em> sum to zero. This is, as we said, one of the algebraic properties of OLS—coefficients were optimally chosen to ensure that the residuals sum to zero.</p>
<p>Because <span class="math inline">\(y_i=\widehat{y_i}+\widehat{u_i}\)</span> by definition (which we can also see in Table 6), we can take the sample average of both sides:</p>
<p><span class="math display">\[\begin{align}
   \dfrac{1}{n} \sum_{i=1}^n y_i=\dfrac{1}{n} \sum_{i=1}^n \widehat{y_i}+\dfrac{1}{n} \sum_{i=1}^n \widehat{u_i}
\end{align}\]</span>
and so <span class="math inline">\(\overline{y}=\overline{\widehat{y}}\)</span> because the residuals sum to zero. Similarly, the way that we obtained our estimates yields</p>
<p><span class="math display">\[\begin{align}
   \sum_{i=1}^n x_i \Big(y_i-\widehat{\beta}_0 - \widehat{\beta}_1 x_i\Big)=0
\end{align}\]</span>
The sample covariance (and therefore the sample correlation) between the explanatory variables and the residuals is always zero (see Table <a href="ch1.html#tab:ols-residuals">2.6</a>).</p>
<p><span class="math display">\[\begin{align}
   \sum_{i=1}^n x_i \widehat{u_i}=0
\end{align}\]</span>
Because the <span class="math inline">\(\widehat{y_i}\)</span> are linear functions of the <span class="math inline">\(x_i\)</span>, the fitted values and residuals are uncorrelated too (see Table <a href="ch1.html#tab:ols-residuals">2.6</a>):</p>
<p><span class="math display" id="eq:fitted1">\[\begin{align}
   \sum_{i=1}^n \widehat{y_i} \widehat{u_i}=0 \tag{2.28}
\end{align}\]</span>
Both properties hold by construction. In other words, <span class="math inline">\(\widehat{\beta}_0\)</span> and <span class="math inline">\(\widehat{\beta}_1\)</span> were selected <em>to make them true</em>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Using the Stata code from Table &lt;a href="ch1.html#tab:ols-residuals"&gt;2.6&lt;/a&gt;, you can show these algebraic properties yourself. I encourage you to do so by creating new variables equaling the product of these terms and collapsing as we did with the other variables. That sort of exercise may help convince you that the aforementioned algebraic properties always hold.&lt;/p&gt;'><sup>24</sup></a></p>
<p>A third property is that if we plug in the average for <span class="math inline">\(x\)</span>, we predict the sample average for <span class="math inline">\(y\)</span>. That is, the point <span class="math inline">\((\overline{x}, \overline{y})\)</span> is on the OLS regression line, or:</p>
<p><span class="math display">\[\begin{align}
   \overline{y}=\widehat{\beta}_0+\widehat{\beta}_1 \overline{x}
\end{align}\]</span></p>
</div>
<div id="goodness-of-fit" class="section level2" number="2.15">
<h2>
<span class="header-section-number">2.15</span> Goodness-of-fit<a class="anchor" aria-label="anchor" href="#goodness-of-fit"><i class="fas fa-link"></i></a>
</h2>
<p>For each observation, we write</p>
<p><span class="math display">\[\begin{align}
   y_i=\widehat{y_i}+\widehat{u_i}
\end{align}\]</span>
Define the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR) as</p>
<p><span class="math display" id="eq:ssr">\[\begin{align}
   SST &amp; = \sum_{i=1}^n (y_i - \overline{y})^2 \tag{2.29}           
   \\
   SSE &amp; = \sum_{i=1}^n (\widehat{y_i} - \overline{y})^2 \tag{2.30} 
   \\
   SSR &amp; = \sum_{i=1}^n \widehat{u_i}^2                                 
   \tag{2.31}
\end{align}\]</span>
These are sample variances when divided by <span class="math inline">\(n-1\)</span>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Recall the earlier discussion about degrees-of-freedom correction.&lt;/p&gt;"><sup>25</sup></a> <span class="math inline">\(\dfrac{SST}{n-1}\)</span> is the sample variance of <span class="math inline">\(y_i\)</span>, <span class="math inline">\(\dfrac{SSE}{n-1}\)</span> is the sample variance of <span class="math inline">\(\widehat{y_i}\)</span>, and <span class="math inline">\(\dfrac{SSR}{n-1}\)</span> is the sample variance of <span class="math inline">\(\widehat{u_i}\)</span>. With some simple manipulation rewrite equation <a href="ch1.html#eq:sst">(2.29)</a>:</p>
<p><span class="math display">\[\begin{align}
   SST &amp; = \sum_{i=1}^n (y_i - \overline{y})^2                                               
   \\
       &amp; = \sum_{i=1}^n \Big[ (y_i - \widehat{y_i}) - (\widehat{y_i} - \overline{y}) \Big]^2 
   \\
       &amp; = \sum_{i=1}^n \Big[ \widehat{u_i} - (\widehat{y_i} - \overline{y})\Big]^2          
\end{align}\]</span>
Since equation <a href="ch1.html#eq:fitted1">(2.28)</a> shows that the fitted values are uncorrelated with the residuals, we can write the following equation:</p>
<p><span class="math display">\[\begin{align}
   SST=SSE+SSR
\end{align}\]</span>
Assuming <span class="math inline">\(SST&gt;0\)</span>, we can define the fraction of the total variation in <span class="math inline">\(y_i\)</span> that is explained by <span class="math inline">\(x_i\)</span> (or the OLS regression line) as</p>
<p><span class="math display">\[\begin{align}
   R^2=\dfrac{SSE}{SST}=1-\dfrac{SSR}{SST}
\end{align}\]</span>
which is called the R-squared of the regression. It can be shown to be equal to the <em>square</em> of the correlation between <span class="math inline">\(y_i\)</span> and <span class="math inline">\(\widehat{y_i}\)</span>. Therefore <span class="math inline">\(0\leq R^2 \leq 1\)</span>. An R-squared of zero means no linear relationship between <span class="math inline">\(y_i\)</span> and <span class="math inline">\(x_i\)</span>, and an <span class="math inline">\(R\)</span>-squared of one means a perfect linear relationship (e.g., <span class="math inline">\(y_i=x_i+2\)</span>). As <span class="math inline">\(R^2\)</span> increases, the <span class="math inline">\(y_i\)</span> are closer and closer to falling on the OLS regression line.</p>
<p>I would encourage you not to fixate on <span class="math inline">\(R\)</span>-squared in research projects where the aim is to estimate some causal effect, though. It’s a useful summary measure, but it does not tell us about causality. Remember, you aren’t trying to explain variation in <span class="math inline">\(y\)</span> if you are trying to estimate some causal effect. The <span class="math inline">\(R^2\)</span> tells us how much of the variation in <span class="math inline">\(y_i\)</span> is explained by the explanatory variables. But if we are interested in the causal effect of a single variable, <span class="math inline">\(R^2\)</span> is irrelevant. For causal inference, we need equation <a href="ch1.html#eq:exog2">(2.24)</a>.</p>
</div>
<div id="expected-value-of-ols" class="section level2" number="2.16">
<h2>
<span class="header-section-number">2.16</span> Expected value of OLS<a class="anchor" aria-label="anchor" href="#expected-value-of-ols"><i class="fas fa-link"></i></a>
</h2>
<p>Up until now, we motivated simple regression using a population model. But our analysis has been purely algebraic, based on a sample of data. So residuals always average to zero when we apply OLS to a sample, regardless of any underlying model. But our job gets tougher. Now we have to study the statistical properties of the OLS estimator, referring to a population model and assuming random sampling.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;This section is a review of traditional econometrics pedagogy. We cover it for the sake of completeness. Traditionally, econometricians motivated their discuss of causality through ideas like unbiasedness and consistency.&lt;/p&gt;"><sup>26</sup></a></p>
<p>The field of mathematical statistics is concerned with questions. How do estimators behave across different samples of data? On average, for instance, will we get the right answer if we repeatedly sample? We need to find the expected value of the OLS estimators—in effect, the average outcome across all possible random samples—and determine whether we are right, on average. This leads naturally to a characteristic called unbiasedness, which is desirable of all estimators.
<span class="math display" id="eq:unbias">\[ 
E(\widehat{\beta})=\beta \tag{2.32}
\]</span>
Remember, our objective is to estimate <span class="math inline">\(\beta_1\)</span>, which is the slope <span class="math inline">\({population}\)</span> parameter that describes the relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span>. Our estimate, <span class="math inline">\(\widehat{\beta_1}\)</span>, is an estimator of that parameter obtained for a specific sample. Different samples will generate different estimates (<span class="math inline">\(\widehat{\beta_1}\)</span>) for the “true” (and unobserved) <span class="math inline">\(\beta_1\)</span>. Unbiasedness means that if we could take as many random samples on <span class="math inline">\(Y\)</span> as we want from the population and compute an estimate each time, the average of the estimates would be equal to <span class="math inline">\(\beta_1\)</span>.</p>
<p>There are several assumptions required for OLS to be unbiased. The first assumption is called linear in the parameters. Assume a population model</p>
<p><span class="math display">\[\begin{align}
   y=\beta_0+\beta_1 x+u
\end{align}\]</span>
where <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are the unknown population parameters. We view <span class="math inline">\(x\)</span> and <span class="math inline">\(u\)</span> as outcomes of random variables generated by some data-generating process. Thus, since <span class="math inline">\(y\)</span> is a function of <span class="math inline">\(x\)</span> and <span class="math inline">\(u\)</span>, both of which are random, then <span class="math inline">\(y\)</span> is also random. Stating this assumption formally shows that our goal is to estimate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</p>
<p>Our second assumption is random sampling. We have a random sample of size <span class="math inline">\(n\)</span>, <span class="math inline">\(\{ (x_i, y_i){:} i=1, \dots, n\}\)</span>, following the population model. We know how to use this data to estimate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> by OLS. Because each <span class="math inline">\(i\)</span> is a draw from the population, we can write, for each <span class="math inline">\(i\)</span>:</p>
<p><span class="math display">\[\begin{align}
   y_i=\beta_0+\beta_1x_i+u_i
\end{align}\]</span>
Notice that <span class="math inline">\(u_i\)</span> here is the unobserved error for observation <span class="math inline">\(i\)</span>. It is <em>not</em> the residual that we compute from the data.</p>
<p>The third assumption is called sample variation in the explanatory variable. That is, the sample outcomes on <span class="math inline">\(x_i\)</span> are not all the same value. This is the same as saying that the sample variance of <span class="math inline">\(x\)</span> is not zero. In practice, this is no assumption at all. If the <span class="math inline">\(x_i\)</span> all have the same value (i.e., are constant), we cannot learn how <span class="math inline">\(x\)</span> affects <span class="math inline">\(y\)</span> in the population. Recall that OLS is the covariance of <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> divided by the variance in <span class="math inline">\(x\)</span>, and so if <span class="math inline">\(x\)</span> is constant, then we are dividing by zero, and the OLS estimator is undefined.</p>
<p>With the fourth assumption our assumptions start to have real teeth. It is called the zero conditional mean assumption and is probably the most critical assumption in causal inference. In the population, the error term has zero mean given any value of the explanatory variable:</p>
<p><span class="math display">\[\begin{align}
   E(u\mid x)=E(u)=0
\end{align}\]</span>
This is the key assumption for showing that OLS is unbiased, with the zero value being of no importance once we assume that <span class="math inline">\(E(u\mid x)\)</span> does not change with <span class="math inline">\(x\)</span>. Note that we can compute OLS estimates whether or not this assumption holds, even if there is an underlying population model.</p>
<p>So, how do we show that <span class="math inline">\(\widehat{\beta_1}\)</span> is an unbiased estimate of <span class="math inline">\(\beta_1\)</span> (equation <a href="ch1.html#eq:unbias">(2.32)</a>)? We need to show that under the four assumptions we just outlined, the expected value of <span class="math inline">\(\widehat{\beta_1}\)</span>, when averaged across random samples, will center on the true value of <span class="math inline">\(\beta_1\)</span>. This is a subtle yet critical concept. Unbiasedness in this context means that if we repeatedly sample data from a population and run a regression on each new sample, the average over all those estimated coefficients will equal the true value of <span class="math inline">\(\beta_1\)</span>. We will discuss the answer as a series of steps.</p>
<p><strong>Step 1</strong>: Write down a formula for <span class="math inline">\(\widehat{\beta_1}\)</span>. It is convenient to use the <span class="math inline">\(\dfrac{C(x,y)}{V(x)}\)</span> form:</p>
<p><span class="math display">\[\begin{align}
   \widehat{\beta_1}=\dfrac{\sum_{i=1}^n (x_i - \overline{x})y_i}{\sum_{i=1}^n (x_i - \overline{x})^2}
\end{align}\]</span>
Let’s get rid of some of this notational clutter by defining <span class="math inline">\(\sum_{i=1}^n (x_i - \overline{x})^2=SST_x\)</span> (i.e., total variation in the <span class="math inline">\(x_i\)</span>) and rewrite this as:</p>
<p><span class="math display">\[\begin{align}
   \widehat{\beta_1}=\dfrac{ \sum_{i=1}^n (x_i - \overline{x})y_i}{SST_x}
\end{align}\]</span></p>
<p><strong>Step 2</strong>: Replace each <span class="math inline">\(y_i\)</span> with <span class="math inline">\(y_i= \beta_0+\beta_1 x_i+u_i\)</span>, which uses the first linear assumption and the fact that we have sampled data (our second assumption). The numerator becomes:</p>
<p><span class="math display">\[\begin{align}
   \sum_{i=1}^n (x_i - \overline{x})y_i &amp; =\sum_{i=1}^n (x_i - \overline{x})(\beta_0+\beta_1 x_i+u_i)                                                                  
   \\
                &amp; = \beta_0 \sum_{i=1}^n (x_i - \overline{x})+\beta_1 \sum_{i=1}^n (x_i - \overline{x})x_i+\sum_{i=1}^n (x_i+\overline{x}) u_i 
   \\
                &amp; =0+\beta_1 \sum_{i=1}^n (x_i - \overline{x})^2+ \sum_{i=1}^n (x_i - \overline{x})u_i                                         
   \\
                &amp; = \beta_1 SST_x+\sum_{i=1}^n (x_i - \overline{x}) u_i                                                                        
\end{align}\]</span>
Note, we used <span class="math inline">\(\sum_{i=1}^n (x_i-\overline{x})=0\)</span> and <span class="math inline">\(\sum_{i=1}^n (x_i - \overline{x})x_i=\sum_{i=1}^n (x_i - \overline{x})^2\)</span> to do this.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Told you we would use this result a lot.&lt;/p&gt;"><sup>27</sup></a></p>
<p>We have shown that:</p>
<p><span class="math display">\[\begin{align}
   \widehat{\beta_1} &amp; = \dfrac{ \beta_1 SST_x+ \sum_{i=1}^n (x_i - \overline{x})u_i }{SST_x} \nonumber \\
    &amp; = \beta_1+\dfrac{ \sum_{i=1}^n (x_i - \overline{x})u_i }{SST_x}                  
\end{align}\]</span>
Note that the last piece is the slope coefficient from the OLS regression of <span class="math inline">\(u_i\)</span> on <span class="math inline">\(x_i\)</span>, <span class="math inline">\(i\)</span>: <span class="math inline">\(1, \dots, n\)</span>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;I find it interesting that we see so many &lt;span class="math inline"&gt;\(\dfrac{cov}{var}\)&lt;/span&gt; terms when working with regres-&lt;br&gt;
sion. They show up constantly. Keep your eyes peeled.&lt;/p&gt;'><sup>28</sup></a> We cannot do this regression because the <span class="math inline">\(u_i\)</span> are not observed. Now define <span class="math inline">\(w_i=\dfrac{(x_i - \overline{x})}{SST_x}\)</span> so that we have the following:</p>
<p><span class="math display">\[\begin{align}
   \widehat{\beta_1}=\beta_1+\sum_{i=1}^n w_i u_i
\end{align}\]</span>
This has showed us the following: First, <span class="math inline">\(\widehat{\beta_1}\)</span> is a linear function of the unobserved errors, <span class="math inline">\(u_i\)</span>. The <span class="math inline">\(w_i\)</span> are all functions of <span class="math inline">\(\{ x_1, \dots, x_n \}\)</span>. Second, the random difference between <span class="math inline">\(\beta_1\)</span> and the estimate of it, <span class="math inline">\(\widehat{\beta_1}\)</span>, is due to this linear function of the unobservables.</p>
<p><strong>Step 3</strong>: Find <span class="math inline">\(E(\widehat{\beta_1})\)</span>. Under the random sampling assumption and the zero conditional mean assumption, <span class="math inline">\(E(u_i \mid x_1, \dots, x_n)=0\)</span>, that means conditional on each of the <span class="math inline">\(x\)</span> variables:</p>
<p><span class="math display">\[\begin{align}
   E\big(w_iu_i\mid x_1, \dots, x_n\big) =
   w_i E\big(u_i \mid x_1, \dots, x_n\big)=0
\end{align}\]</span>
because <span class="math inline">\(w_i\)</span> is a function of <span class="math inline">\(\{x_1, \dots, x_n\}\)</span>. This would be true if in the population <span class="math inline">\(u\)</span> and <span class="math inline">\(x\)</span> are correlated.</p>
<p>Now we can complete the proof: conditional on <span class="math inline">\(\{x_1, \dots, x_n\}\)</span>,</p>
<p><span class="math display">\[\begin{align}
   E(\widehat{\beta_1}) &amp; = E \bigg(\beta_1+\sum_{i=1}^n w_i u_i \bigg) \\
    &amp; =\beta_1+\sum_{i=1}^n E(w_i u_i)              \\
    &amp; = \beta_1+\sum_{i=1}^n w_i E(u_i)             \\
    &amp; =\beta_1 +0                                   \\
    &amp; =\beta_1                                      
\end{align}\]</span>
Remember, <span class="math inline">\(\beta_1\)</span> is the fixed constant in the population. The estimator, <span class="math inline">\(\widehat{\beta_1}\)</span>, varies across samples and is the random outcome: before we collect our data, we do not know what <span class="math inline">\(\widehat{\beta_1}\)</span> will be. Under the four aforementioned assumptions, <span class="math inline">\(E(\widehat{\beta_0})=\beta_0\)</span> and <span class="math inline">\(E(\widehat{\beta_1})=\beta_1\)</span>.</p>
<p>I find it helpful to be concrete when we work through exercises like this. So let’s visualize this. Let’s create a Monte Carlo simulation. We have the following population model:</p>
<p><span class="math display" id="eq:ols2">\[\begin{align}
y=3+2x+u \tag{2.33}
\end{align}\]</span>
where <span class="math inline">\(x\sim Normal(0,9)\)</span>, <span class="math inline">\(u\sim Normal(0,36)\)</span>. Also, <span class="math inline">\(x\)</span> and <span class="math inline">\(u\)</span> are independent. The following Monte Carlo simulation will estimate OLS on a sample of data 1,000 times. The true <span class="math inline">\(\beta\)</span> parameter equals 2. But what will the average <span class="math inline">\(\widehat{\beta}\)</span> equal when we use repeated sampling?</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/ols3.do"><code>ols3.do</code></a></em></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb5-1"><a href="ch1.html#cb5-1" aria-hidden="true"></a><span class="kw">clear</span> <span class="ot">all</span> </span>
<span id="cb5-2"><a href="ch1.html#cb5-2" aria-hidden="true"></a><span class="kw">program</span> <span class="kw">define</span> ols, rclass </span>
<span id="cb5-3"><a href="ch1.html#cb5-3" aria-hidden="true"></a><span class="kw">version</span> 14.2 </span>
<span id="cb5-4"><a href="ch1.html#cb5-4" aria-hidden="true"></a><span class="kw">syntax</span> [, <span class="kw">obs</span>(integer 1) mu(<span class="fu">real</span> 0) sigma(<span class="fu">real</span> 1) ] </span>
<span id="cb5-5"><a href="ch1.html#cb5-5" aria-hidden="true"></a></span>
<span id="cb5-6"><a href="ch1.html#cb5-6" aria-hidden="true"></a>    <span class="kw">clear</span> </span>
<span id="cb5-7"><a href="ch1.html#cb5-7" aria-hidden="true"></a>    <span class="kw">drop</span> <span class="dt">_all</span> </span>
<span id="cb5-8"><a href="ch1.html#cb5-8" aria-hidden="true"></a>    <span class="kw">set</span> <span class="kw">obs</span> 10000 </span>
<span id="cb5-9"><a href="ch1.html#cb5-9" aria-hidden="true"></a>    <span class="kw">gen</span> x = 9*rnormal()  </span>
<span id="cb5-10"><a href="ch1.html#cb5-10" aria-hidden="true"></a>    <span class="kw">gen</span> u  = 36*rnormal()  </span>
<span id="cb5-11"><a href="ch1.html#cb5-11" aria-hidden="true"></a>    <span class="kw">gen</span> <span class="fu">y</span>  = 3 + 2*x + u </span>
<span id="cb5-12"><a href="ch1.html#cb5-12" aria-hidden="true"></a>    <span class="kw">reg</span> <span class="fu">y</span> x </span>
<span id="cb5-13"><a href="ch1.html#cb5-13" aria-hidden="true"></a>    <span class="kw">end</span> </span>
<span id="cb5-14"><a href="ch1.html#cb5-14" aria-hidden="true"></a></span>
<span id="cb5-15"><a href="ch1.html#cb5-15" aria-hidden="true"></a><span class="kw">simulate</span> beta=_b[x], reps(1000): ols </span>
<span id="cb5-16"><a href="ch1.html#cb5-16" aria-hidden="true"></a>su </span>
<span id="cb5-17"><a href="ch1.html#cb5-17" aria-hidden="true"></a><span class="kw">hist</span> beta</span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/ols3.R"><code>ols3.R</code></a></em></p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>

<span class="va">lm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">lapply</a></span><span class="op">(</span>
  <span class="fl">1</span><span class="op">:</span><span class="fl">1000</span>,
  <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu">tibble</span><span class="op">(</span>
    x <span class="op">=</span> <span class="fl">9</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">10000</span><span class="op">)</span>,
    u <span class="op">=</span> <span class="fl">36</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">10000</span><span class="op">)</span>,
    y <span class="op">=</span> <span class="fl">3</span> <span class="op">+</span> <span class="fl">2</span><span class="op">*</span><span class="va">x</span> <span class="op">+</span> <span class="va">u</span>
  <span class="op">)</span> <span class="op">%&gt;%</span> 
    <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, <span class="va">.</span><span class="op">)</span>
<span class="op">)</span>

<span class="fu">as_tibble</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">lm</span>, <span class="va">coef</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>

<span class="fu">as_tibble</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">lm</span>, <span class="va">coef</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span><span class="op">+</span>
  <span class="fu">geom_histogram</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, binwidth <span class="op">=</span> <span class="fl">0.01</span><span class="op">)</span></code></pre></div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:ols">Table 2.7: </span> Monte Carlo simulation of OLS.</caption>
<thead><tr class="header">
<th align="left">Variable</th>
<th align="center">Obs</th>
<th align="center">Mean</th>
<th align="center">St. Dev.</th>
</tr></thead>
<tbody><tr class="odd">
<td align="left">beta</td>
<td align="center">1,000</td>
<td align="center">1.998317</td>
<td align="center">0.0398413</td>
</tr></tbody>
</table></div>
<p>Table <a href="ch1.html#tab:ols">2.7</a> gives us the mean value of <span class="math inline">\(\widehat{\beta-1}\)</span> over the 1,000 repetitions (repeated sampling). Your results will differ from mine here only in the randomness involved in the simulation. But your results should be similar to what is shown here. While each sample had a different estimated slope, the average for <span class="math inline">\(\widehat{\beta-1}\)</span> over all the samples was 1.998317, which is close to the true value of 2 (see equation <a href="ch1.html#eq:ols2">(2.33)</a>). The standard deviation in this estimator was 0.0398413, which is close to the standard error recorded in the regression itself.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;The standard error I found from running this on one sample of data was 0.0403616.&lt;/p&gt;"><sup>29</sup></a> Thus, we see that the estimate is the mean value of the coefficient from repeated sampling, and the standard error is the standard deviation from that repeated estimation. We can see the distribution of these coefficient estimates in Figure <a href="ch1.html#fig:beta-mc">2.3</a>.</p>
<div class="figure">
<span id="fig:beta-mc"></span>
<img src="graphics/beta_mc.jpg" alt="Distribution of coefficients from Monte Carlo simulation." width="100%"><p class="caption">
Figure 2.3: Distribution of coefficients from Monte Carlo simulation.
</p>
</div>
<p>The problem is, we don’t know which kind of sample we have. Do we have one of the “almost exactly 2” samples, or do we have one of the “pretty different from 2” samples? We can never know whether we are close to the population value. We hope that our sample is “typical” and produces a slope estimate close to <span class="math inline">\(\widehat{\beta_1}\)</span>, but we can’t know. Unbiasedness is a property of the procedure of the rule. It is not a property of the estimate itself. For example, say we estimated an that 8.2% return on schooling. It is tempting to say that 8.2% is an unbiased estimate of the return to schooling, but that’s technically incorrect. The rule used to get <span class="math inline">\(\widehat{\beta_1}=0.082\)</span> is unbiased (if we believe that <span class="math inline">\(u\)</span> is unrelated to schooling), not the actual estimate itself.</p>
</div>
<div id="law-of-iterated-expectations" class="section level2" number="2.17">
<h2>
<span class="header-section-number">2.17</span> Law of iterated expectations<a class="anchor" aria-label="anchor" href="#law-of-iterated-expectations"><i class="fas fa-link"></i></a>
</h2>
<p>The conditional expectation function (CEF) is the mean of some outcome <span class="math inline">\(y\)</span> with some covariate <span class="math inline">\(x\)</span> held fixed. Let’s focus more intently on this function.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;I highly encourage the interested reader to study &lt;span class="citation"&gt;Angrist and Pischke (&lt;a href="references.html#ref-Angrist2009" role="doc-biblioref"&gt;2009&lt;/a&gt;)&lt;/span&gt;, who have an excellent discussion of LIE there.&lt;/p&gt;'><sup>30</sup></a> Let’s get the notation and some of the syntax out of the way. As noted earlier, we write the CEF as <span class="math inline">\(E(y_i\mid x_i)\)</span>. Note that the CEF is explicitly a function of <span class="math inline">\(x_i\)</span>. And because <span class="math inline">\(x_i\)</span> is random, the CEF is random—although sometimes we work with particular values for <span class="math inline">\(x_i\)</span>, like <span class="math inline">\(E(y_i\mid x_i=8\text{ years schooling})\)</span> or <span class="math inline">\(E(y_i\mid x_i=\text{Female})\)</span>. When there are treatment variables, then the CEF takes on two values: <span class="math inline">\(E(y_i\mid d_i=0)\)</span> and <span class="math inline">\(E(y_i\mid d_i= 1)\)</span>. But these are special cases only.</p>
<p>An important complement to the CEF is the law of iterated expectations (LIE). This law says that an unconditional expectation can be written as the unconditional average of the CEF. In other words, <span class="math inline">\(E(y_i)=E \{E(y_i\mid x_i)\}\)</span>. This is a fairly simple idea: if you want to know the unconditional expectation of some random variable <span class="math inline">\(y\)</span>, you can simply calculate the weighted sum of all conditional expectations with respect to some covariate <span class="math inline">\(x\)</span>. Let’s look at an example. Let’s say that average grade-point for females is 3.5, average GPA for males is a 3.2, half the population is female, and half is male. Then:</p>
<p><span class="math display">\[\begin{align}
   E[GPA] &amp; = E \big\{E(GPA_i\mid \text{Gender}_i) \big\} 
   \\
          &amp; =(0.5 \times 3.5)+(3.2 \times 0.5)            
   \\
          &amp; = 3.35                                        
\end{align}\]</span>
You probably use LIE all the time and didn’t even know it. The proof is not complicated. Let <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_i\)</span> each be continuously distributed. The joint density is defined as <span class="math inline">\(f_{xy}(u,t)\)</span>. The conditional distribution of <span class="math inline">\(y\)</span> given <span class="math inline">\(x=u\)</span> is defined as <span class="math inline">\(f_y(t\mid x_i=u)\)</span>. The marginal densities are <span class="math inline">\(g_y(t)\)</span> and <span class="math inline">\(g_x(u)\)</span>.</p>
<p><span class="math display">\[\begin{align}
   E \{E(y\mid x)\} &amp; = \int E(y\mid x=u) g_x(u) du                                            \\
    &amp; = \int \bigg[ \int tf_{y\mid x} (t\mid x=u) dt \bigg] g_x(u) du          \\
    &amp; = \int \int t f_{y\mid x} (t\mid x=u) g_x(u) du dt                       \\
    &amp; = \int t \bigg[ \int f_{y\mid x} (t\mid x=u) g_x(u) du \bigg] dt \text{} \\
    &amp; = \int t [ f_{x,y}du] dt                                                 \\
    &amp; = \int t g_y(t) dt                                                       \\
    &amp; =E(y)                                                                    
\end{align}\]</span>
Check out how easy this proof is. The first line uses the definition of expectation. The second line uses the definition of conditional expectation. The third line switches the integration order. The fourth line uses the definition of joint density. The fifth line replaces the prior line with the subsequent expression. The sixth line integrates joint density over the support of x which is equal to the marginal density of <span class="math inline">\(y\)</span>. So restating the law of iterated expectations: <span class="math inline">\(E(y_i)= E\{E(y\mid x_i)\}\)</span>.</p>
</div>
<div id="cef-decomposition-property" class="section level2" number="2.18">
<h2>
<span class="header-section-number">2.18</span> CEF decomposition property<a class="anchor" aria-label="anchor" href="#cef-decomposition-property"><i class="fas fa-link"></i></a>
</h2>
<p>The first property of the CEF we will discuss is the CEF decomposition property. The power of LIE comes from the way it breaks a random variable into two pieces—the CEF and a residual with special properties. The CEF decomposition property states that
<span class="math display">\[ 
y_i=E(y_i\mid x_i)+\varepsilon_i\
\]</span></p>
<p>where (i) <span class="math inline">\(\varepsilon_i\)</span> is mean independent of <span class="math inline">\(x_i\)</span>, That is,</p>
<p><span class="math display">\[ 
E(\varepsilon_i\mid x_i)=0
\]</span></p>
<p>and (ii) <span class="math inline">\(\varepsilon_i\)</span> is not correlated with any function of <span class="math inline">\(x_i\)</span>.</p>
<p>The theorem says that any random variable <span class="math inline">\(y_i\)</span> can be decomposed into a piece that is explained by <span class="math inline">\(x_i\)</span> (the CEF) and a piece that is left over and orthogonal to any function of <span class="math inline">\(x_i\)</span>. I’ll prove the (<em>i</em>) part first. Recall that <span class="math inline">\(\varepsilon_i=y_i - E(y_i\mid x_i)\)</span> as we will make a substitution in the second line below.</p>
<p><span class="math display">\[\begin{align}
   E(\varepsilon_i\mid x_i)
     &amp; =E\Big(y_i- E(y_i\mid x_i)\mid x_i\Big) 
   \\
     &amp; =E(y_i\mid x_i) - E(y_i\mid x_i)        
   \\
     &amp; = 0                                     
\end{align}\]</span></p>
<p>The second part of the theorem states that <span class="math inline">\(\varepsilon_i\)</span> is uncorrelated with any function of <span class="math inline">\(x_i\)</span>. Let <span class="math inline">\(h(x_i)\)</span> be any function of <span class="math inline">\(x_i\)</span>. Then <span class="math inline">\(E( h(x_i) \varepsilon_i)=E \{ h(x_i) E(\varepsilon_i\mid x_i)\}\)</span> The second term in the interior product is equal to zero by mean independence.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Let’s take a concrete example of this proof. Let &lt;span class="math inline"&gt;\(h(x_i)=\alpha+\gamma x_i\)&lt;/span&gt;. Then take the joint expectation &lt;span class="math inline"&gt;\(E(h(x_i)\varepsilon_i)=E([\alpha+\gamma x_i] \varepsilon_i)\)&lt;/span&gt;. Then take conditional expectations &lt;span class="math inline"&gt;\(E (\alpha\mid x_i )+E(\gamma\mid x_i) E(x_i\mid x_i)E(\varepsilon\mid x_i)\}=\alpha+ x_iE(\varepsilon_i\mid x_i)=0\)&lt;/span&gt; after we pass the conditional expectation through.&lt;/p&gt;'><sup>31</sup></a></p>
</div>
<div id="cef-prediction-property" class="section level2" number="2.19">
<h2>
<span class="header-section-number">2.19</span> CEF prediction property<a class="anchor" aria-label="anchor" href="#cef-prediction-property"><i class="fas fa-link"></i></a>
</h2>
<p>The second property is the CEF prediction property. This states that <span class="math inline">\(E(y_i\mid x_i)=\arg\min_{m(x_i)}E[(y-m(x_i))^2]\)</span>, where <span class="math inline">\(m(x_i)\)</span> is any function of <span class="math inline">\(x_i\)</span>. In words, this states that the CEF is the minimum mean squared error of <span class="math inline">\(y_i\)</span> given <span class="math inline">\(x_i\)</span>. By adding <span class="math inline">\(E(y_i\mid x_i) - E(y_i\mid x_i)=0\)</span> to the right side we get
<span class="math display">\[ 
\Big[y_i-m(x_i)\Big]^2=\Big[\big(y_i-E[y_i\mid x_i]\big)
   +\big(E(y_i\mid x_i)- m(x_i)\big)\Big]^2
\]</span>
I personally find this easier to follow with simpler notation. So replace this expression with the following terms:
<span class="math display">\[ 
(a-b+b-c)^2
\]</span>
Distribute the terms, rearrange them, and replace the terms with their original values until you get the following:</p>
<p><span class="math display">\[\begin{align}
   \arg\min \Big(y_i- E(y_i\mid x_i)\Big)^2 &amp; + 2\Big(E(y_i\mid x_i)- m(x_i)\Big)\times 
   \Big(y_i - E(y_i\mid x_i)\Big)\\
    &amp; +\Big(E(y_i\mid x_i)+m(x_i)\Big)^2        
\end{align}\]</span></p>
<p>Now minimize the function with respect to <span class="math inline">\(m(x_i)\)</span>. When minimizing this function with respect to <span class="math inline">\(m(x_i)\)</span>, note that the first term <span class="math inline">\((y_i-E(y_i\mid x_i))^2\)</span> doesn’t matter because it does not depend on <span class="math inline">\(m(x_i)\)</span>. So it will zero out. The second and third terms, though, do depend on <span class="math inline">\(m(x_i)\)</span>. So rewrite <span class="math inline">\(2(E (y_i\mid x_i)-m(x_i))\)</span> as <span class="math inline">\(h(x_i)\)</span>. Also set <span class="math inline">\(\varepsilon_i\)</span> equal to <span class="math inline">\([y_i-E(y_i\mid x_i)]\)</span> and substitute
<span class="math display">\[ 
\arg\min\varepsilon_i^2+h(x_i)\varepsilon_i+ \Big[ E(y_i\mid x_i)+m(x_i)\Big]^2
\]</span>
Now minimizing this function and setting it equal to zero we get
<span class="math display">\[ 
h'(x_i)\varepsilon_i
\]</span>
which equals zero by the decomposition property.</p>
</div>
<div id="anova-theory" class="section level2" number="2.20">
<h2>
<span class="header-section-number">2.20</span> ANOVA theory<a class="anchor" aria-label="anchor" href="#anova-theory"><i class="fas fa-link"></i></a>
</h2>
<p>The final property of the CEF that we will discuss is the analysis of variance theorem, or ANOVA. According to this theorem, the unconditional variance in some random variable is equal to the variance in the conditional expectation plus the expectation of the conditional variance, or
<span class="math display">\[ 
V(y_i)=V\Big[E(y_i\mid x_i)\Big]+
   E\Big[V(y_i\mid x_i)\Big]
\]</span>
where <span class="math inline">\(V\)</span> is the variance and <span class="math inline">\(V(y_i\mid x_i)\)</span> is the conditional variance.</p>
</div>
<div id="linear-cef-theorem" class="section level2" number="2.21">
<h2>
<span class="header-section-number">2.21</span> Linear CEF theorem<a class="anchor" aria-label="anchor" href="#linear-cef-theorem"><i class="fas fa-link"></i></a>
</h2>
<p>As you probably know by now, the use of least squares in applied work is extremely common. That’s because regression has several justifications. We discussed one—unbiasedness under certain assumptions about the error term. But I’d like to present some slightly different arguments. <span class="citation">Angrist and Pischke (<a href="references.html#ref-Angrist2009" role="doc-biblioref">2009</a>)</span> argue that linear regression may be useful even if the underlying CEF itself is not linear, because regression is a good approximation of the CEF. So keep an open mind as I break this down a little bit more.</p>
<p><span class="citation">Angrist and Pischke (<a href="references.html#ref-Angrist2009" role="doc-biblioref">2009</a>)</span> give several arguments for using regression, and the linear CEF theorem is probably the easiest. Let’s assume that we are sure that the CEF itself is linear. So what? Well, if the CEF is linear, then the linear CEF theorem states that the population regression is equal to that linear CEF. And if the CEF is linear, and if the population regression equals it, then of course you should use the population regression to estimate CEF. If you need a proof for what could just as easily be considered common sense, I provide one. If <span class="math inline">\(E(y_i\mid x_i)\)</span> is linear, then <span class="math inline">\(E(y_i\mid x_i)= x'\widehat{\beta}\)</span> for some vector <span class="math inline">\(\widehat{\beta}\)</span>. By the decomposition property, you get:
<span class="math display">\[ 
E\Big(x(y-E(y\mid x)\Big)=E\Big(x(y-x'\widehat{\beta})\Big)=0
\]</span>
And then when you solve this, you get <span class="math inline">\(\widehat{\beta}=\beta\)</span>. Hence <span class="math inline">\(E(y\mid x)=x'\beta\)</span>.</p>
</div>
<div id="best-linear-predictor-theorem" class="section level2" number="2.22">
<h2>
<span class="header-section-number">2.22</span> Best linear predictor theorem<a class="anchor" aria-label="anchor" href="#best-linear-predictor-theorem"><i class="fas fa-link"></i></a>
</h2>
<p>There are a few other linear theorems that are worth bringing up in this context. For instance, recall that the CEF is the minimum mean squared error predictor of <span class="math inline">\(y\)</span> given <span class="math inline">\(x\)</span> in the class of all functions, according to the CEF prediction property. Given this, the population regression function is the best that we can do in the class of all linear functions.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;See &lt;span class="citation"&gt;Angrist and Pischke (&lt;a href="references.html#ref-Angrist2009" role="doc-biblioref"&gt;2009&lt;/a&gt;)&lt;/span&gt; for a proof.&lt;/p&gt;'><sup>32</sup></a></p>
</div>
<div id="regression-cef-theorem" class="section level2" number="2.23">
<h2>
<span class="header-section-number">2.23</span> Regression CEF theorem<a class="anchor" aria-label="anchor" href="#regression-cef-theorem"><i class="fas fa-link"></i></a>
</h2>
<p>I would now like to cover one more attribute of regression. The function <span class="math inline">\(X\beta\)</span> provides the minimum mean squared error linear approximation to the CEF. That is,
<span class="math display">\[ 
\beta=\arg\min_b E\Big\{\big[E(y_i\mid x_i) - x_i'b\big]^2 \Big\}
\]</span>
<em>So?</em> Let’s try and back up for a second, though, and get the big picture, as all these linear theorems can leave the reader asking, “So what?” I’m telling you all of this because I want to present to you an argument that regression is appealing; even though it’s linear, it can still be justified when the CEF itself isn’t. And since we don’t know with certainty that the CEF is linear, this is actually a nice argument to at least consider. Regression is ultimately nothing more than a crank turning data into estimates, and what I’m saying here is that crank produces something desirable even under bad situations. Let’s look a little bit more at this crank, though, by reviewing another theorem which has become popularly known as the regression anatomy theorem.</p>
</div>
<div id="regression-anatomy-theorem" class="section level2" number="2.24">
<h2>
<span class="header-section-number">2.24</span> Regression anatomy theorem<a class="anchor" aria-label="anchor" href="#regression-anatomy-theorem"><i class="fas fa-link"></i></a>
</h2>
<p>In addition to our discussion of the CEF and regression theorems, we now dissect the regression itself. Here we discuss the regression anatomy theorem. The regression anatomy theorem is based on earlier work by <span class="citation">Frisch and Waugh (<a href="references.html#ref-Frisch1933" role="doc-biblioref">1933</a>)</span> and <span class="citation">Lovell (<a href="references.html#ref-Lovell1963" role="doc-biblioref">1963</a>)</span>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;A helpful proof of the Frisch-Waugh-Lovell theorem can be found in &lt;span class="citation"&gt;Lovell (&lt;a href="references.html#ref-Lovell2008" role="doc-biblioref"&gt;2008&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;'><sup>33</sup></a> I find the theorem more intuitive when I think through a specific example and offer up some data visualization. In my opinion, the theorem helps us interpret the individual coefficients of a multiple linear regression model. Say that we are interested in the causal effect of family size on labor supply. We want to regress labor supply on family size:</p>
<p><span class="math display">\[\begin{align}
   Y_i=\beta_0+\beta_1 X_i+u_i
\end{align}\]</span>
where <span class="math inline">\(Y\)</span> is labor supply, and <span class="math inline">\(X\)</span> is family size.</p>
<p>If family size is truly random, then the number of kids in a family is uncorrelated with the unobserved error term.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;While randomly having kids may sound fun, I encourage you to have kids when you want to have them. Contact your local high school health teacher to learn more about a number of methods that can reasonably minimize the number of random children you create.&lt;/p&gt;"><sup>34</sup></a> This implies that when we regress labor supply on family size, our estimate, <span class="math inline">\(\widehat{\beta}_1\)</span>, can be interpreted as the causal effect of family size on labor supply. We could just plot the regression coefficient in a scatter plot showing all <span class="math inline">\(i\)</span> pairs of data; the slope coefficient would be the best linear fit of the data for this data cloud. Furthermore, under randomized number of children, the slope would also tell us the average causal effect of family size on labor supply.</p>
<p>But most likely, family size isn’t random, because so many people choose the number of children to have in their family—instead of, say, flipping a coin. So how do we interpret <span class="math inline">\(\widehat{\beta}_1\)</span> if the family size is <em>not</em> random? Often, people choose their family size according to something akin to an optimal stopping rule. People pick both how many kids to have, when to have them, and when to stop having them. In some instances, they may even attempt to pick the gender. All of these choices are based on a variety of unobserved and observed economic factors that may themselves be associated with one’s decision to enter the labor market. In other words, using the language we’ve been using up until now, it’s unlikely that <span class="math inline">\(E(u\mid X)=E(u)=0\)</span>.</p>
<p>But let’s say that we have reason to think that the number of kids in a family is <em>conditionally</em> random. To make this tractable for the sake of pedagogy, let’s say that a particular person’s family size is as good as randomly chosen once we condition on race and age.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Almost certainly a ridiculous assumption, but stick with me.&lt;/p&gt;"><sup>35</sup></a> While unrealistic, I include it to illustrate an important point regarding multivariate regressions. If this assumption were to be true, then we could write the following equation:</p>
<p><span class="math display">\[\begin{align}
   Y_i=\beta_0+\beta_1 X_i+\gamma_1 R_i+\gamma_2A_i+u_i
\end{align}\]</span>
where <span class="math inline">\(Y\)</span> is labor supply, <span class="math inline">\(X\)</span> is number of kids, <span class="math inline">\(R\)</span> is race, <span class="math inline">\(A\)</span> is age, and <span class="math inline">\(u\)</span> is the population error term.</p>
<p>If we want to estimate the average causal effect of family size on labor supply, then we need two things. First, we need a sample of <em>data</em> containing all four of those variables. Without all four of the variables, we cannot estimate this regression model. Second, we need for the number of kids, <span class="math inline">\(X\)</span>, to be randomly assigned for a given set of race and age.</p>
<p>Now, how do we interpret <span class="math inline">\(\widehat{\beta}_1\)</span>? And how might we visualize this coefficient given that there are six dimensions to the data? The regression anatomy theorem both tells us what this coefficient estimate actually means and also lets us visualize the data in only two dimensions.</p>
<p>To explain the intuition of the regression anatomy theorem, let’s write down a population model with multiple variables. Assume that your main multiple regression model of interest has K covariates. We can then write it as:
<span class="math display">\[ 
y_i=\beta_0+\beta_1 x_{1i}+\dots+\beta_k x_{ki}+ \dots+\beta_K x_{Ki}+e_i
\]</span>
Now assume an <em>auxiliary</em> regression in which the variable <span class="math inline">\(x_{1i}\)</span> is regressed on all the remaining independent variables:
<span class="math display">\[ 
x_{1i}=\gamma_0+\gamma_{k-1}x_{k-1i}+ \gamma_{k+1}x_{k+1i}+\dots+\gamma_Kx_{Ki}+f_i
\]</span>
and <span class="math inline">\(\tilde{x}_{1i}=x_{1i} - \widehat{x}_{1i}\)</span> is the residual from that auxiliary regression. Then the parameter <span class="math inline">\(\beta_1\)</span> can be rewritten as:
<span class="math display">\[ 
\beta_1=\dfrac{C(y_i,\tilde{x}_i)}{V(\tilde{x}_i)}
\]</span>
Notice that again we see that the coefficient estimate is a scaled covariance, only here, the covariance is with respect to the outcome and residual from the auxiliary regression, and the scale is the variance of that same residual.</p>
<p>To prove the theorem, note that <span class="math inline">\(E[\tilde{x}_{ki}]=E[x_{ki}] - E[\widehat{x}_{ki}]=E[f_i]\)</span>, and plug <span class="math inline">\(y_i\)</span> and residual <span class="math inline">\(\tilde{x}_{ki}\)</span> from <span class="math inline">\(x_{ki}\)</span> auxiliary regression into the covariance <span class="math inline">\(\mathop{\mathrm{cov}}(y_i,x_{ki})\)</span>:</p>
<p><span class="math display">\[\begin{align}
   \beta_k &amp; = \dfrac{\mathop{\mathrm{cov}}( \beta_0+\beta_1 x_{1i}+\dots+ \beta_kx_{ki}+\dots+\beta_Kx_{Ki}+e_i, \tilde{x}_{ki})}{\mathop{\mathrm{var}}(\tilde{x}_{ki})} 
   \\
           &amp; = \dfrac{\mathop{\mathrm{cov}}(\beta_0+\beta_1 x_{1i}+\dots+\beta_k x_{ki}+\dots+\beta_K x_{Ki}+e_i, f_i)}{\mathop{\mathrm{var}}(f_i)}                       
\end{align}\]</span></p>
<p>Since by construction <span class="math inline">\(E[f_i]=0\)</span>, it follows that the term <span class="math inline">\(\beta_0 E[f_i]=0\)</span>. Since <span class="math inline">\(f_i\)</span> is a linear combination of all the independent variables with the exception of <span class="math inline">\(x_{ki}\)</span>, it must be that</p>
<p><span class="math display">\[\begin{align}
   \beta_1E[f_ix_{1i}]=\dots=\beta_{k-1}E[f_ix_{k-1i}]= \beta_{k+1} E[f_ix_{k+1i}]=\dots=\beta_K E[f_ix_{Ki}]=0
\end{align}\]</span></p>
<p>Consider now the term <span class="math inline">\(E[e_if_i]\)</span>. This can be written as</p>
<p><span class="math display">\[\begin{align}
   E[e_if_i] &amp; = E[e_if_i]                               \\
             &amp; = E[e_i \tilde{x}_{ki}]                   \\
             &amp; = E\Big[e_i(x_{ki}-\widehat{x}_{ki})\Big] \\
             &amp; = E[e_ix_{ki}]-E[e_i\tilde{x}_{ki}]       
\end{align}\]</span></p>
<p>Since <span class="math inline">\(e_i\)</span> is uncorrelated with any independent variable, it is also uncorrelated with <span class="math inline">\(x_{ki}\)</span>. Accordingly, we have <span class="math inline">\(E[e_ix_{ki}]=0\)</span>. With regard to the second term of the subtraction, substituting the predicted value from the <span class="math inline">\(x_{ki}\)</span> auxiliary regression, we get
<span class="math display">\[ 
E[e_i\tilde{x}_{ki}]=E\Big[e_i(\widehat{\gamma}_0+ \widehat{\gamma}_1x_{1i}+\dots+ \widehat{\gamma}_{k-1i}+ \widehat{\gamma}_{k+1}x_{k+1i}+\dots+ \widehat{x}_Kx_{Ki})\Big]
\]</span></p>
<p>Once again, since <span class="math inline">\(e_i\)</span> is uncorrelated with any independent variable, the expected value of the terms is equal to zero. It follows that <span class="math inline">\(E[e_if_i]=0\)</span>.</p>
<p>The only remaining term, then, is <span class="math inline">\([\beta_kx_{ki}f_i]\)</span>, which equals <span class="math inline">\(E[\beta_kx_{ki}\tilde{x}_{ki}]\)</span>, since <span class="math inline">\(f_i=\tilde{x}_{ki}\)</span>. The term <span class="math inline">\(x_{ki}\)</span> can be substituted by rewriting the auxiliary regression model, <span class="math inline">\(x_{ki}\)</span>, such that</p>
<p><span class="math display">\[\begin{align}
   x_{ki}=E[x_{ki}\mid X_{-k}]+\tilde{x}_{ki}
\end{align}\]</span></p>
<p>This gives</p>
<p><span class="math display">\[\begin{align}
   E[\beta_{k}x_{ki}\tilde{x}_{ki}] &amp; = \beta_k E\Big[\tilde{x}_{ki} ( E[x_{ki}\mid X_{-k}]+ \tilde{x}_{ki})\Big]         
   \\
            &amp; = \beta_k \Big\{ E[\tilde{x}_{ki}^2]+ E[(E[x_{ki}\mid x_{-k}]\tilde{x}_{ki})]\Big\} 
   \\
            &amp; = \beta_{k}\mathop{\mathrm{var}}(\tilde{x}_{ki})                                                     
\end{align}\]</span></p>
<p>which follows directly from the orthogonality between <span class="math inline">\(E[x_{ki}\mid X_{-k}]\)</span> and <span class="math inline">\(\tilde{x}_{ki}\)</span>. From previous derivations we finally get
<span class="math display">\[ 
\mathop{\mathrm{cov}}(y_i,\tilde{x}_{ki})=\beta_k\mathop{\mathrm{var}}(\tilde{x}_{ki})
\]</span>
which completes the proof.</p>
<p>I find it helpful to visualize things. Let’s look at an example in Stata using its popular automobile data set. I’ll show you:</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/reganat.do"><code>reganat.do</code></a></em></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb7-1"><a href="ch1.html#cb7-1" aria-hidden="true"></a><span class="kw">ssc</span> install reganat, <span class="kw">replace</span></span>
<span id="cb7-2"><a href="ch1.html#cb7-2" aria-hidden="true"></a><span class="kw">sysuse</span> auto.dta, <span class="kw">replace</span></span>
<span id="cb7-3"><a href="ch1.html#cb7-3" aria-hidden="true"></a><span class="kw">regress</span> price <span class="fu">length</span></span>
<span id="cb7-4"><a href="ch1.html#cb7-4" aria-hidden="true"></a><span class="kw">regress</span> price <span class="fu">length</span> <span class="kw">weight</span> headroom mpg</span>
<span id="cb7-5"><a href="ch1.html#cb7-5" aria-hidden="true"></a>reganat price <span class="fu">length</span> <span class="kw">weight</span> headroom mpg, dis(<span class="fu">length</span>) biline</span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/reganat.R"><code>reganat.R</code></a></em></p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://haven.tidyverse.org">haven</a></span><span class="op">)</span>

<span class="va">read_data</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">df</span><span class="op">)</span>
<span class="op">{</span>
  <span class="va">full_path</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"https://raw.github.com/scunning1975/mixtape/master/"</span>, 
                     <span class="va">df</span>, sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span>
  <span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://haven.tidyverse.org/reference/read_dta.html">read_dta</a></span><span class="op">(</span><span class="va">full_path</span><span class="op">)</span>
  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">df</span><span class="op">)</span>
<span class="op">}</span>


<span class="va">auto</span> <span class="op">&lt;-</span> <span class="fu">read_data</span><span class="op">(</span><span class="st">"auto.dta"</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu">mutate</span><span class="op">(</span>length <span class="op">=</span> <span class="va">length</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">length</span><span class="op">)</span><span class="op">)</span>

<span class="va">lm1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">price</span> <span class="op">~</span> <span class="va">length</span>, <span class="va">auto</span><span class="op">)</span>
<span class="va">lm2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">price</span> <span class="op">~</span> <span class="va">length</span> <span class="op">+</span> <span class="va">weight</span> <span class="op">+</span> <span class="va">headroom</span> <span class="op">+</span> <span class="va">mpg</span>, <span class="va">auto</span><span class="op">)</span>


<span class="va">coef_lm1</span> <span class="op">&lt;-</span> <span class="va">lm1</span><span class="op">$</span><span class="va">coefficients</span>
<span class="va">coef_lm2</span> <span class="op">&lt;-</span> <span class="va">lm2</span><span class="op">$</span><span class="va">coefficients</span>
<span class="va">resid_lm2</span> <span class="op">&lt;-</span> <span class="va">lm2</span><span class="op">$</span><span class="va">residuals</span> 

<span class="va">y_single</span> <span class="op">&lt;-</span> <span class="fu">tibble</span><span class="op">(</span>price <span class="op">=</span> <span class="va">coef_lm1</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">+</span> <span class="va">coef_lm1</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">*</span><span class="va">auto</span><span class="op">$</span><span class="va">length</span>, 
                   length <span class="op">=</span> <span class="va">auto</span><span class="op">$</span><span class="va">length</span><span class="op">)</span>

<span class="va">y_multi</span> <span class="op">&lt;-</span> <span class="fu">tibble</span><span class="op">(</span>price <span class="op">=</span> <span class="va">coef_lm1</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">+</span> <span class="va">coef_lm2</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">*</span><span class="va">auto</span><span class="op">$</span><span class="va">length</span>, 
                  length <span class="op">=</span> <span class="va">auto</span><span class="op">$</span><span class="va">length</span><span class="op">)</span>


<span class="fu">ggplot</span><span class="op">(</span><span class="va">auto</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu">geom_point</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">length</span>, y <span class="op">=</span> <span class="va">price</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">geom_smooth</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">length</span>, y <span class="op">=</span> <span class="va">price</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">y_multi</span>, color <span class="op">=</span> <span class="st">"blue"</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">geom_smooth</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">length</span>, y <span class="op">=</span> <span class="va">price</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">y_single</span>, color<span class="op">=</span><span class="st">"red"</span><span class="op">)</span></code></pre></div>
<p>Let’s walk through both the regression output that I’ve reproduced in Table <a href="ch1.html#tab:reganat">2.8</a> as well as a nice visualization of the slope parameters in what I’ll call the short bivariate regression and the longer multivariate regression. The short regression of price on car length yields a coefficient of 57.20 on length. For every additional inch, a car is $57 more expensive, which is shown by the upward-sloping, dashed line in Figure <a href="ch1.html#fig:reganat4">2.4</a>. The slope of that line is 57.20.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:reganat">Table 2.8: </span> Regression estimates of automobile price on length and other characteristics.</caption>
<thead><tr class="header">
<th align="left">Covariates</th>
<th></th>
<th></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Length</td>
<td>57.20</td>
<td><span class="math inline">\(-94.50\)</span></td>
</tr>
<tr class="even">
<td align="left"></td>
<td>(14.08)</td>
<td>(40.40)</td>
</tr>
<tr class="odd">
<td align="left">Weight</td>
<td></td>
<td>4.34</td>
</tr>
<tr class="even">
<td align="left"></td>
<td></td>
<td>(1.16)</td>
</tr>
<tr class="odd">
<td align="left">Headroom</td>
<td></td>
<td><span class="math inline">\(-490.97\)</span></td>
</tr>
<tr class="even">
<td align="left"></td>
<td></td>
<td>(388.49)</td>
</tr>
<tr class="odd">
<td align="left">MPG</td>
<td></td>
<td><span class="math inline">\(-87.96\)</span></td>
</tr>
<tr class="even">
<td align="left"></td>
<td></td>
<td>(83.59)</td>
</tr>
</tbody>
</table></div>
<div class="figure">
<span id="fig:reganat4"></span>
<img src="graphics/reganat_3.jpg" alt="Regression anatomy display." width="100%"><p class="caption">
Figure 2.4: Regression anatomy display.
</p>
</div>
<p>It will eventually become second nature for you to talk about including more variables on the right side of a regression as “controlling for” those variables. But in this regression anatomy exercise, I hope to give a different interpretation of what you’re doing when you in fact “control for” variables in a regression. First, notice how the coefficient on length changed signs and increased in magnitude once we controlled for the other variables. Now, the effect on length is <span class="math inline">\(-94.5\)</span>. It appears that the length was confounded by several other variables, and once we conditioned on them, longer cars actually were cheaper. You can see a visual representation of this in Figure <a href="ch1.html#fig:reganat4">2.4</a>, where the multivariate slope is negative.</p>
<p>So what exactly going on in this visualization? Well, for one, it has condensed the number of dimensions (variables) from four to only two. It did this through the regression anatomy process that we described earlier. Basically, we ran the auxiliary regression, used the residuals from it, and then calculated the slope coefficient as <span class="math inline">\(\dfrac{\mathop{\mathrm{cov}}(y_i,\tilde{x}_i)}{\mathop{\mathrm{var}}{(\tilde{x}_i)}}\)</span>. This allowed us to show scatter plots of the auxiliary residuals paired with their outcome observations and to slice the slope through them (Figure <a href="ch1.html#fig:reganat4">2.4</a>). Notice that this is a useful way to preview the multidimensional correlation between two variables from a multivariate regression. Notice that the solid black line is negative and the slope from the bivariate regression is positive. The regression anatomy theorem shows that these two estimators—one being a multivariate OLS and the other being a bivariate regression price and a residual—are identical.</p>
</div>
<div id="variance-of-the-ols-estimators" class="section level2" number="2.25">
<h2>
<span class="header-section-number">2.25</span> Variance of the OLS estimators<a class="anchor" aria-label="anchor" href="#variance-of-the-ols-estimators"><i class="fas fa-link"></i></a>
</h2>
<p>That more or less summarizes what we want to discuss regarding the linear regression. Under a zero conditional mean assumption, we could epistemologically infer that the rule used to produce the coefficient from a regression in our sample was unbiased. That’s nice because it tells us that we have good reason to believe that result. But now we need to build out this epistemological justification so as to capture the inherent uncertainty in the sampling process itself. This added layer of uncertainty is often called inference. Let’s turn to it now.</p>
<p>Remember the simulation we ran earlier in which we resampled a population and estimated regression coefficients a thousand times? We produced a histogram of those 1,000 estimates in Figure <a href="ch1.html#fig:beta-mc">2.3</a>. The mean of the coefficients was around 1.998, which was very close to the true effect of 2 (hard-coded into the data-generating process). But the standard deviation was around 0.04. This means that, basically, in repeated sampling of some population, we got different estimates. But the average of those estimates was close to the true effect, and their spread had a standard deviation of 0.04. This concept of spread in repeated sampling is probably the most useful thing to keep in mind as we move through this section.</p>
<p>Under the four assumptions we discussed earlier, the OLS estimators are unbiased. But these assumptions are not sufficient to tell us anything about the variance in the estimator itself. The assumptions help inform our beliefs that the estimated coefficients, on average, equal the parameter values themselves. But to speak intelligently about the variance of the estimator, we need a measure of dispersion in the sampling distribution of the estimators. As we’ve been saying, this leads us to the variance and ultimately to the standard deviation. We could characterize the variance of the OLS estimators under the four assumptions. But for now, it’s easiest to introduce an assumption that simplifies the calculations. We’ll keep the assumption ordering we’ve been using and call this the fifth assumption.</p>
<p>The fifth assumption is the homoskedasticity or constant variance assumption. This assumption stipulates that our population error term, <span class="math inline">\(u\)</span>, has the same variance given any value of the explanatory variable, <span class="math inline">\(x\)</span>. Formally, this is:
<span class="math display">\[ 
V(u\mid x)=\sigma^2
\]</span>
When I was first learning this material, I always had an unusually hard time wrapping my head around <span class="math inline">\(\sigma^2\)</span>. Part of it was because of my humanities background; I didn’t really have an appreciation for random variables that were dispersed. I wasn’t used to taking a lot of numbers and trying to measure distances between them, so things were slow to click. So if you’re like me, try this. Think of <span class="math inline">\(\sigma^2\)</span> as just a positive number like 2 or 8. That number is measuring the spreading out of underlying errors themselves. In other words, the variance of the errors conditional on the explanatory variable is simply some finite, positive number. And that number is measuring the variance of the stuff other than <span class="math inline">\(x\)</span> that influence the value of <span class="math inline">\(y\)</span> itself. And because we assume the zero conditional mean assumption, whenever we assume homoskedasticity, we can also write:
<span class="math display">\[ 
E(u^2\mid x)=\sigma^2=E(u^2)
\]</span>
Now, under the first, fourth, and fifth assumptions, we can write:
<span class="math display">\[\begin{align}
       E(y\mid x) &amp; = \beta_0+\beta_1 x \\
       V(y\mid x) &amp; = \sigma^2          
   \end{align}\]</span>
So the average, or expected, value of <span class="math inline">\(y\)</span> is allowed to change with <span class="math inline">\(x\)</span>, but if the errors are homoskedastic, then the variance does not change with <span class="math inline">\(x\)</span>. The constant variance assumption may not be realistic; it must be determined on a case-by-case basis.</p>
<p><em>Theorem: Sampling variance of OLS</em>. Under assumptions 1 and 2, we get:</p>
<p><span class="math display">\[\begin{align}
   V(\widehat{\beta_1}\mid x)&amp;=\dfrac{\sigma^2}{\sum_{i=1}^n (x_i-\overline{x})^2}\\
   &amp;= \dfrac{ \sigma^2}{SST_x}
\end{align}\]</span></p>
<p><span class="math display">\[   
   V(\widehat{\beta_0}\mid x)=
   \dfrac{\sigma^2\left(\dfrac{1}{n} \sum_{i=1}^n x_i^2\right)}{SST_x}
\]</span>
To show this, write, as before,
<span class="math display">\[ 
\widehat{\beta_1}=\beta_1+ \sum_{i=1}^n w_i u_i
\]</span>
where <span class="math inline">\(w_i=\dfrac{(x_i-\overline{x})}{SST_x}\)</span>. We are treating this as nonrandom in the derivation. Because <span class="math inline">\(\beta_1\)</span> is a constant, it does not affect <span class="math inline">\(V(\widehat{\beta_1})\)</span>. Now, we need to use the fact that, for uncorrelated random variables, the variance of the sum is the sum of the variances. The <span class="math inline">\(\{ u_i: i=1, \dots, n \}\)</span> are actually independent across <span class="math inline">\(i\)</span> and uncorrelated. Remember: if we know <span class="math inline">\(x\)</span>, we know <span class="math inline">\(w\)</span>. So:</p>
<p><span class="math display">\[\begin{align}
   V(\widehat{\beta_1}\mid x) &amp; =\mathop{\mathrm{Var\,}}(\beta_1+ \sum_{i=1}^n w_i u_i\mid x)    
   \\
      &amp; = \mathop{\mathrm{Var\,}}\bigg(\sum_{i=1}^n w_i u_i\mid x\bigg) 
   \\
      &amp; = \sum_{i=1}^n\mathop{\mathrm{Var\,}}(w_i u_i\mid x)            
   \\
      &amp; = \sum_{i=1}^n w_i^2\mathop{\mathrm{Var\,}}(u_i\mid x)          
   \\
      &amp; = \sum_{i=1}^n w_i^2\sigma^2                  
   \\
      &amp; = \sigma^2 \sum_{i=1}^n w_i^2                 
\end{align}\]</span>
where the penultimate equality condition used the fifth assumption so that the variance of <span class="math inline">\(u_i\)</span> does not depend on <span class="math inline">\(x_i\)</span>. Now we have:</p>
<p><span class="math display">\[\begin{align}
   \sum_{i=1}^n w_i^2 &amp; = \sum_{i=1}^n                                          
   \dfrac{(x_i-\overline{x})^2}{SST_x^2}\\
    &amp; =\dfrac{ \sum_{i=1}^n (x_i - \overline{x})^2}{ SST_x^2} \\
    &amp; = \dfrac{ SST_x}{SST_x^2}                               \\
    &amp; = \dfrac{1}{SST_x}                                      
\end{align}\]</span>
We have shown:
<span class="math display">\[ 
V(\widehat{\beta_1})=\dfrac{\sigma^2}{SST_x}
\]</span>
A couple of points. First, this is the “standard” formula for the variance of the OLS slope estimator. It is <em>not</em> valid if the fifth assumption, of homoskedastic errors, doesn’t hold. The homoskedasticity assumption is needed, in other words, to derive this standard formula. But the homoskedasticity assumption is <em>not</em> used to show unbiasedness of the OLS estimators. That requires only the first four assumptions.</p>
<p>Usually, we are interested in <span class="math inline">\(\beta_1\)</span>. We can easily study the two factors that affect its variance: the numerator and the denominator.
<span class="math display">\[ 
V(\widehat{\beta_1})=\dfrac{ \sigma^2}{SST_x}
\]</span>
As the error variance increases—that is, as <span class="math inline">\(\sigma^2\)</span> increases—so does the variance in our estimator. The more “noise” in the relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> (i.e., the larger the variability in <span class="math inline">\(u\)</span>), the harder it is to learn something about <span class="math inline">\(\beta_1\)</span>. In contrast, more variation in <span class="math inline">\(\{x_i\}\)</span> is a <em>good</em> thing. As <span class="math inline">\(SST_x\)</span> rises, <span class="math inline">\(V(\widehat{\beta_1}) \downarrow\)</span>.</p>
<p>Notice that <span class="math inline">\(\dfrac{ SST_x}{n}\)</span> is the sample variance in <span class="math inline">\(x\)</span>. We can think of this as getting close to the population variance of <span class="math inline">\(x\)</span>, <span class="math inline">\(\sigma_x^2\)</span>, as <span class="math inline">\(n\)</span> gets large. This means:
<span class="math display">\[ 
SST_x \approx n\sigma_x^2
\]</span>
which means that as <span class="math inline">\(n\)</span> grows, <span class="math inline">\(V(\widehat{\beta_1})\)</span> shrinks at the rate of <span class="math inline">\(\dfrac{1}{n}\)</span>. This is why more data is a good thing: it shrinks the sampling variance of our estimators.</p>
<p>The standard deviation of <span class="math inline">\(\widehat{\beta_1}\)</span> is the square root of the variance. So:
<span class="math display">\[ 
sd(\widehat{\beta_1})=\dfrac{ \sigma}{\sqrt{SST_x}}
\]</span>
This turns out to be the measure of variation that appears in confidence intervals and test statistics.</p>
<p>Next we look at estimating the error variance. In the formula, <span class="math inline">\(V(\widehat{\beta_1})= \dfrac{\sigma^2}{SST_x}\)</span>, we can compute <span class="math inline">\(SST_x\)</span> from <span class="math inline">\(\{x_i: i=1, \dots, n\}\)</span>. But we need to estimate <span class="math inline">\(\sigma^2\)</span>. Recall that <span class="math inline">\(\sigma^2=E(u^2)\)</span>. Therefore, if we could observe a sample on the errors, <span class="math inline">\(\{u_i: i=1, \dots, n\}\)</span>, an unbiased estimator of <span class="math inline">\(\sigma^2\)</span> would be the sample average:
<span class="math display">\[ 
\dfrac{1}{n} \sum_{i=1}^n u_i^2
\]</span>
But this isn’t an estimator that we can compute from the data we observe, because <span class="math inline">\(u_i\)</span> are unobserved. How about replacing each <span class="math inline">\(u_i\)</span> with its “estimate,” the OLS residual <span class="math inline">\(\widehat{u_i}\)</span>?</p>
<p><span class="math display">\[\begin{align}
   u_i           &amp; = y_i - \beta_0 - \beta_1 x_i                    
   \\
   \widehat{u_i} &amp; = y_i - \widehat{\beta_0} - \widehat{\beta_1}x_i 
\end{align}\]</span>
Whereas <span class="math inline">\(u_i\)</span> <em>cannot</em> be computed, <span class="math inline">\(\widehat{u_i}\)</span> can be computed from the data because it depends on the estimators, <span class="math inline">\(\widehat{\beta_0}\)</span> and <span class="math inline">\(\widehat{\beta_1}\)</span>. But, except by sheer coincidence, <span class="math inline">\(u_i\ne\widehat{u_i}\)</span> for any <span class="math inline">\(i\)</span>.</p>
<p><span class="math display">\[\begin{align}
   \widehat{u_i} &amp; = y_i-\widehat{\beta_0} - \widehat{\beta_1}x_i                     
   \\
                 &amp; =(\beta_0+\beta_1x_i+u_i)-\widehat{\beta_0} - \widehat{\beta_1}x_i 
   \\
                 &amp; =u_i-(\widehat{\beta_0}-\beta_0)-(\widehat{\beta_1}                
   -\beta_1)x_i
\end{align}\]</span>
Note that <span class="math inline">\(E(\widehat{\beta_0})=\beta_0\)</span> and <span class="math inline">\(E(\widehat{\beta_1})=\beta_1\)</span>, but the estimators almost always differ from the population values in a sample. So what about this as an estimator of <span class="math inline">\(\sigma^2\)</span>?
<span class="math display">\[ 
\dfrac{1}{n} \sum_{i=1}^n \widehat{u_i}^2= \dfrac{1}{n}SSR
\]</span>
It is a true estimator and easily computed from the data after OLS. As it turns out, this estimator is slightly biased: its expected value is a little less than <span class="math inline">\(\sigma^2\)</span>. The estimator does not account for the two restrictions on the residuals used to obtain <span class="math inline">\(\widehat{\beta_0}\)</span> and <span class="math inline">\(\widehat{\beta_1}\)</span>:</p>
<p><span class="math display">\[\begin{align}
   \sum_{i=1}^n \widehat{u_i}=0 \\
   \sum_{i=1}^n x_i \widehat{u_i}=0
\end{align}\]</span>
There is no such restriction on the unobserved errors. The unbiased estimator, therefore, of <span class="math inline">\(\sigma^2\)</span> uses a degrees-of-freedom adjustment. The residuals have only <span class="math inline">\(n-2\)</span>, not <span class="math inline">\(n\)</span>, degrees of freedom. Therefore:
<span class="math display">\[ 
\widehat{\sigma}^2=\dfrac{1}{n-2} SSR
\]</span></p>
<p>We now propose the following theorem. The unbiased estimator of <span class="math inline">\(\sigma^2\)</span> under the first five assumptions is:
<span class="math display">\[ 
E(\widehat{\sigma}^2)=\sigma^2
\]</span>
In most software packages, regression output will include:</p>
<p><span class="math display">\[\begin{align}
   \widehat{\sigma} &amp; = \sqrt{\widehat{\sigma}^2} 
   \\
    &amp; = \sqrt{\dfrac{SSR}{(n-2)}} 
\end{align}\]</span>
This is an estimator of <span class="math inline">\(sd(u)\)</span>, the standard deviation of the population error. One small glitch is that <span class="math inline">\(\widehat{\sigma}\)</span> is not unbiased for <span class="math inline">\(\sigma\)</span>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;There does exist an unbiased estimator of &lt;span class="math inline"&gt;\(\sigma\)&lt;/span&gt;, but it’s tedious and hardly anyone in economics seems to use it. See &lt;span class="citation"&gt;Holtzman (&lt;a href="references.html#ref-Holtzman1950" role="doc-biblioref"&gt;1950&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;'><sup>36</sup></a> This will not matter for our purposes: <span class="math inline">\(\widehat{\sigma}\)</span> is called the standard error of the regression, which means that it is an estimate of the standard deviation of the error in the regression. The software package Stata calls it the root mean squared error.</p>
<p>Given <span class="math inline">\(\widehat{\sigma}\)</span>, we can now estimate <span class="math inline">\(sd(\widehat{\beta_1})\)</span> and <span class="math inline">\(sd(\widehat{\beta_0})\)</span>. The estimates of these are called the standard errors of the <span class="math inline">\(\widehat{\beta_j}\)</span>. We will use these <em>a lot</em>. Almost all regression packages report the standard errors in a column next to the coefficient estimates. We can just plug <span class="math inline">\(\widehat{\sigma}\)</span> in for <span class="math inline">\(\sigma\)</span>:
<span class="math display">\[ 
se(\widehat{\beta_1})= \dfrac{\widehat{\sigma}}{\sqrt{SST_x}}
\]</span>
where both the numerator and the denominator are computed from the data. For reasons we will see, it is useful to report the standard errors below the corresponding coefficient, usually in parentheses.</p>
</div>
<div id="robust-standard-errors" class="section level2" number="2.26">
<h2>
<span class="header-section-number">2.26</span> Robust standard errors<a class="anchor" aria-label="anchor" href="#robust-standard-errors"><i class="fas fa-link"></i></a>
</h2>
<p>How realistic is it that the variance in the errors is the same for all slices of the explanatory variable, <span class="math inline">\(x\)</span>? The short answer here is that it is probably unrealistic. Heterogeneity is just something I’ve come to accept as the rule, not the exception, so if anything, we should be opting in to believing in homoskedasticity, not opting out. You can just take it as a given that errors are never homoskedastic and move forward to the solution.</p>
<p>This isn’t completely bad news, because the unbiasedness of our regressions based on repeated sampling never depended on assuming anything about the variance of the errors. Those four assumptions, and particularly the zero conditional mean assumption, guaranteed that the central tendency of the coefficients under repeated sampling would equal the true parameter, which for this book is a causal parameter. The problem is with the spread of the coefficients. Without homoskedasticity, OLS no longer has the minimum mean squared errors, which means that the estimated standard errors are biased. Using our sampling metaphor, then, the distribution of the coefficients is probably larger than we thought. Fortunately, there is a solution. Let’s write down the variance equation under heterogeneous variance terms:
<span class="math display">\[ 
\mathop{\mathrm{Var\,}}(\widehat{\beta_1})=\dfrac{\sum_{i=1}^n (x_i - \overline{x})^2 \sigma_i^2}{SST_x^2}
\]</span>
Notice the <span class="math inline">\(i\)</span> subscript in our <span class="math inline">\(\sigma_i^2\)</span> term; that means variance is not a constant. When <span class="math inline">\(\sigma_i^2=\sigma^2\)</span> for all <span class="math inline">\(i\)</span>, this formula reduces to the usual form, <span class="math inline">\(\dfrac{\sigma^2}{SST_x^2}\)</span>. But when that isn’t true, then we have a problem called heteroskedastic errors. A valid estimator of Var(<span class="math inline">\(\widehat{\beta_1}\)</span>) for heteroskedasticity of any form (including homoskedasticity) is
<span class="math display">\[ 
\mathop{\mathrm{Var\,}}(\widehat{\beta_1})=\dfrac{\sum_{i=1}^n (x_i - \overline{x})^2 \widehat{u_i}^2}{SST_x^2}
\]</span>
which is easily computed from the data after the OLS regression. We have Friedhelm Eicker, Peter J. Huber, and Halbert White to thank for this solution (<span class="citation">White (<a href="references.html#ref-White1980" role="doc-biblioref">1980</a>)</span>).<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;No one even bothers to cite &lt;span class="citation"&gt;White (&lt;a href="references.html#ref-White1980" role="doc-biblioref"&gt;1980&lt;/a&gt;)&lt;/span&gt; anymore, just like how no one cites Leibniz or Newton when using calculus. Eicker, Huber, and White created a solution so valuable that it got separated from the original papers when it was absorbed into the statistical toolkit.&lt;/p&gt;'><sup>37</sup></a> The solution for heteroskedasticity goes by several names, but the most common is “robust” standard error.</p>
</div>
<div id="cluster-robust-standard-errors" class="section level2" number="2.27">
<h2>
<span class="header-section-number">2.27</span> Cluster robust standard errors<a class="anchor" aria-label="anchor" href="#cluster-robust-standard-errors"><i class="fas fa-link"></i></a>
</h2>
<p>People will try to scare you by challenging how you constructed your standard errors. Heteroskedastic errors, though, aren’t the only thing you should be worried about when it comes to inference. Some phenomena do not affect observations individually, but they do affect groups of observations that involve individuals. And then they affect those individuals within the group in a common way. Say you want to estimate the effect of class size on student achievement, but you know that there exist unobservable things (like the teacher) that affect all the students equally. If we can commit to independence of these unobservables across classes, but individual student unobservables are correlated within a class, then we have a situation in which we need to cluster the standard errors. Before we dive into an example, I’d like to start with a simulation to illustrate the problem.</p>
<p>As a baseline for this simulation, let’s begin by simulating nonclustered data and analyze least squares estimates of that nonclustered data. This will help firm up our understanding of the problems that occur with least squares when data is clustered.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Hat tip to Ben Chidmi, who helped create this simulation in Stata.&lt;/p&gt;"><sup>38</sup></a></p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/cluster1.do"><code>cluster1.do</code></a></em></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb9-1"><a href="ch1.html#cb9-1" aria-hidden="true"></a><span class="kw">clear</span> <span class="ot">all</span></span>
<span id="cb9-2"><a href="ch1.html#cb9-2" aria-hidden="true"></a><span class="kw">set</span> <span class="dv">seed</span> 20140</span>
<span id="cb9-3"><a href="ch1.html#cb9-3" aria-hidden="true"></a>* Set the number <span class="kw">of</span> simulations</span>
<span id="cb9-4"><a href="ch1.html#cb9-4" aria-hidden="true"></a><span class="kw">local</span> n_sims  = 1000</span>
<span id="cb9-5"><a href="ch1.html#cb9-5" aria-hidden="true"></a><span class="kw">set</span> <span class="kw">obs</span> <span class="ot">`n_sims'</span></span>
<span id="cb9-6"><a href="ch1.html#cb9-6" aria-hidden="true"></a></span>
<span id="cb9-7"><a href="ch1.html#cb9-7" aria-hidden="true"></a>* Create the variables that will contain the results <span class="kw">of</span> each simulation</span>
<span id="cb9-8"><a href="ch1.html#cb9-8" aria-hidden="true"></a><span class="kw">generate</span> beta_0 = .</span>
<span id="cb9-9"><a href="ch1.html#cb9-9" aria-hidden="true"></a><span class="kw">generate</span> beta_0_l = .</span>
<span id="cb9-10"><a href="ch1.html#cb9-10" aria-hidden="true"></a><span class="kw">generate</span> beta_0_u = .</span>
<span id="cb9-11"><a href="ch1.html#cb9-11" aria-hidden="true"></a><span class="kw">generate</span> beta_1 = .</span>
<span id="cb9-12"><a href="ch1.html#cb9-12" aria-hidden="true"></a><span class="kw">generate</span> beta_1_l = .</span>
<span id="cb9-13"><a href="ch1.html#cb9-13" aria-hidden="true"></a><span class="kw">generate</span> beta_1_u = .</span>
<span id="cb9-14"><a href="ch1.html#cb9-14" aria-hidden="true"></a></span>
<span id="cb9-15"><a href="ch1.html#cb9-15" aria-hidden="true"></a></span>
<span id="cb9-16"><a href="ch1.html#cb9-16" aria-hidden="true"></a>* Provide the true population parameters</span>
<span id="cb9-17"><a href="ch1.html#cb9-17" aria-hidden="true"></a><span class="kw">local</span> beta_0_true = 0.4</span>
<span id="cb9-18"><a href="ch1.html#cb9-18" aria-hidden="true"></a><span class="kw">local</span> beta_1_true = 0</span>
<span id="cb9-19"><a href="ch1.html#cb9-19" aria-hidden="true"></a><span class="kw">local</span> rho = 0.5</span>
<span id="cb9-20"><a href="ch1.html#cb9-20" aria-hidden="true"></a></span>
<span id="cb9-21"><a href="ch1.html#cb9-21" aria-hidden="true"></a>* Run the linear regression 1000 times and <span class="kw">save</span> the parameters beta_0 and beta_1</span>
<span id="cb9-22"><a href="ch1.html#cb9-22" aria-hidden="true"></a><span class="kw">quietly</span> {</span>
<span id="cb9-23"><a href="ch1.html#cb9-23" aria-hidden="true"></a>    <span class="kw">forvalues</span> i = 1(1) <span class="ot">`n_sims'</span> {</span>
<span id="cb9-24"><a href="ch1.html#cb9-24" aria-hidden="true"></a>        <span class="kw">preserve</span>  </span>
<span id="cb9-25"><a href="ch1.html#cb9-25" aria-hidden="true"></a>        <span class="kw">clear</span></span>
<span id="cb9-26"><a href="ch1.html#cb9-26" aria-hidden="true"></a>        <span class="kw">set</span> <span class="kw">obs</span> 100</span>
<span id="cb9-27"><a href="ch1.html#cb9-27" aria-hidden="true"></a>        <span class="kw">generate</span> x = rnormal(0,1)</span>
<span id="cb9-28"><a href="ch1.html#cb9-28" aria-hidden="true"></a>        <span class="kw">generate</span> <span class="fu">e</span> = rnormal(0, <span class="fu">sqrt</span>(1 - <span class="ot">`rho'</span>))</span>
<span id="cb9-29"><a href="ch1.html#cb9-29" aria-hidden="true"></a>        <span class="kw">generate</span> <span class="fu">y</span> = <span class="ot">`beta_0_true'</span> + <span class="ot">`beta_1_true'</span>*x + <span class="fu">e</span></span>
<span id="cb9-30"><a href="ch1.html#cb9-30" aria-hidden="true"></a>        <span class="kw">regress</span> <span class="fu">y</span> x</span>
<span id="cb9-31"><a href="ch1.html#cb9-31" aria-hidden="true"></a>        <span class="kw">local</span> b0 = _b[<span class="dt">_cons</span>]</span>
<span id="cb9-32"><a href="ch1.html#cb9-32" aria-hidden="true"></a>        <span class="kw">local</span> b1 = _b[x]</span>
<span id="cb9-33"><a href="ch1.html#cb9-33" aria-hidden="true"></a>        <span class="kw">local</span> df = <span class="fu">e</span>(df_r)</span>
<span id="cb9-34"><a href="ch1.html#cb9-34" aria-hidden="true"></a>        <span class="kw">local</span> critical_value = invt(<span class="ot">`df'</span>, 0.975)</span>
<span id="cb9-35"><a href="ch1.html#cb9-35" aria-hidden="true"></a>        <span class="kw">restore</span></span>
<span id="cb9-36"><a href="ch1.html#cb9-36" aria-hidden="true"></a>        <span class="kw">replace</span> beta_0 = <span class="ot">`b0'</span> <span class="kw">in</span> <span class="ot">`i'</span></span>
<span id="cb9-37"><a href="ch1.html#cb9-37" aria-hidden="true"></a>        <span class="kw">replace</span> beta_0_l = beta_0 - <span class="ot">`critical_value'</span>*_se[<span class="dt">_cons</span>] </span>
<span id="cb9-38"><a href="ch1.html#cb9-38" aria-hidden="true"></a>        <span class="kw">replace</span> beta_0_u = beta_0 + <span class="ot">`critical_value'</span>*_se[<span class="dt">_cons</span>] </span>
<span id="cb9-39"><a href="ch1.html#cb9-39" aria-hidden="true"></a>        <span class="kw">replace</span> beta_1 = <span class="ot">`b1'</span> <span class="kw">in</span> <span class="ot">`i'</span></span>
<span id="cb9-40"><a href="ch1.html#cb9-40" aria-hidden="true"></a>        <span class="kw">replace</span> beta_1_l = beta_1 - <span class="ot">`critical_value'</span>*_se[x] </span>
<span id="cb9-41"><a href="ch1.html#cb9-41" aria-hidden="true"></a>        <span class="kw">replace</span> beta_1_u = beta_1 + <span class="ot">`critical_value'</span>*_se[x] </span>
<span id="cb9-42"><a href="ch1.html#cb9-42" aria-hidden="true"></a>        </span>
<span id="cb9-43"><a href="ch1.html#cb9-43" aria-hidden="true"></a>    }</span>
<span id="cb9-44"><a href="ch1.html#cb9-44" aria-hidden="true"></a>}</span>
<span id="cb9-45"><a href="ch1.html#cb9-45" aria-hidden="true"></a><span class="kw">gen</span> false = (beta_1_l &gt; 0 )</span>
<span id="cb9-46"><a href="ch1.html#cb9-46" aria-hidden="true"></a><span class="kw">replace</span> false = 2 <span class="kw">if</span> beta_1_u &lt; 0</span>
<span id="cb9-47"><a href="ch1.html#cb9-47" aria-hidden="true"></a><span class="kw">replace</span> false = 3 <span class="kw">if</span> false == 0</span>
<span id="cb9-48"><a href="ch1.html#cb9-48" aria-hidden="true"></a><span class="kw">tab</span> false</span>
<span id="cb9-49"><a href="ch1.html#cb9-49" aria-hidden="true"></a></span>
<span id="cb9-50"><a href="ch1.html#cb9-50" aria-hidden="true"></a>* Plot the parameter estimate</span>
<span id="cb9-51"><a href="ch1.html#cb9-51" aria-hidden="true"></a><span class="kw">hist</span> beta_1, <span class="kw">frequency</span> <span class="bn">addplot</span>(pci 0 0 100 0) <span class="bn">title</span>(<span class="st">"Least squares estimates of non-clustered data"</span>) <span class="bn">subtitle</span>(<span class="st">" Monte Carlo simulation of the slope"</span>) <span class="bn">legend</span>(<span class="kw">label</span>(1 <span class="st">"Distribution of least squares estimates"</span>) <span class="kw">label</span>(2 <span class="st">"True population parameter"</span>)) <span class="bn">xtitle</span>(<span class="st">"Parameter estimate"</span>) </span>
<span id="cb9-52"><a href="ch1.html#cb9-52" aria-hidden="true"></a></span>
<span id="cb9-53"><a href="ch1.html#cb9-53" aria-hidden="true"></a><span class="kw">sort</span> beta_1</span>
<span id="cb9-54"><a href="ch1.html#cb9-54" aria-hidden="true"></a><span class="kw">gen</span> <span class="kw">int</span> sim_ID = <span class="dt">_n</span></span>
<span id="cb9-55"><a href="ch1.html#cb9-55" aria-hidden="true"></a><span class="kw">gen</span> beta_1_True = 0</span>
<span id="cb9-56"><a href="ch1.html#cb9-56" aria-hidden="true"></a>* Plot <span class="kw">of</span> the Confidence Interval</span>
<span id="cb9-57"><a href="ch1.html#cb9-57" aria-hidden="true"></a><span class="kw">twoway</span> rcap beta_1_l beta_1_u sim_ID <span class="kw">if</span> beta_1_l &gt; 0 | beta_1_u &lt; 0  , horizontal lcolor(<span class="bn">pink</span>) || || <span class="co">///</span></span>
<span id="cb9-58"><a href="ch1.html#cb9-58" aria-hidden="true"></a>rcap beta_1_l beta_1_u sim_ID <span class="kw">if</span> beta_1_l &lt; 0 &amp; beta_1_u &gt; 0 , horizontal ysc(<span class="fu">r</span>(0)) || || <span class="co">///</span></span>
<span id="cb9-59"><a href="ch1.html#cb9-59" aria-hidden="true"></a>connected sim_ID beta_1 || || <span class="co">///</span></span>
<span id="cb9-60"><a href="ch1.html#cb9-60" aria-hidden="true"></a><span class="kw">line</span> sim_ID beta_1_True, lpattern(<span class="kw">dash</span>) lcolor(<span class="bn">black</span>) lwidth(1) <span class="co">///  </span></span>
<span id="cb9-61"><a href="ch1.html#cb9-61" aria-hidden="true"></a><span class="bn">title</span>(<span class="st">"Least squares estimates of non-clustered data"</span>) <span class="bn">subtitle</span>(<span class="st">" 95% Confidence interval of the slope"</span>) <span class="co">///</span></span>
<span id="cb9-62"><a href="ch1.html#cb9-62" aria-hidden="true"></a><span class="bn">legend</span>(<span class="kw">label</span>(1 <span class="st">"Missed"</span>) <span class="kw">label</span>(2 <span class="st">"Hit"</span>) <span class="kw">label</span>(3 <span class="st">"OLS estimates"</span>) <span class="kw">label</span>(4 <span class="st">"True population parameter"</span>)) <span class="bn">xtitle</span>(<span class="st">"Parameter estimates"</span>) <span class="co">///</span></span>
<span id="cb9-63"><a href="ch1.html#cb9-63" aria-hidden="true"></a><span class="bn">ytitle</span>(<span class="st">"Simulation"</span>)</span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/cluster1.R"><code>cluster1.R</code></a></em></p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">#- Analysis of Clustered Data</span>
<span class="co">#- Courtesy of Dr. Yuki Yanai, </span>
<span class="co">#- http://yukiyanai.github.io/teaching/rm1/contents/R/clustered-data-analysis.html</span>

<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">'arm'</span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="http://mvtnorm.R-forge.R-project.org">'mvtnorm'</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://github.com/lme4/lme4/">'lme4'</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">'multiwayvcov'</span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">'clusterSEs'</span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="http://ggplot2.tidyverse.org">'ggplot2'</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://dplyr.tidyverse.org">'dplyr'</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="http://haven.tidyverse.org">'haven'</a></span><span class="op">)</span>

<span class="va">gen_cluster</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">param</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.1</span>, <span class="fl">.5</span><span class="op">)</span>, <span class="va">n</span> <span class="op">=</span> <span class="fl">1000</span>, <span class="va">n_cluster</span> <span class="op">=</span> <span class="fl">50</span>, <span class="va">rho</span> <span class="op">=</span> <span class="fl">.5</span><span class="op">)</span> <span class="op">{</span>
  <span class="co"># Function to generate clustered data</span>
  <span class="co"># Required package: mvtnorm</span>
  
  <span class="co"># individual level</span>
  <span class="va">Sigma_i</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0</span>, <span class="fl">0</span>, <span class="fl">1</span> <span class="op">-</span> <span class="va">rho</span><span class="op">)</span>, ncol <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>
  <span class="va">values_i</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/mvtnorm/man/Mvnorm.html">rmvnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, sigma <span class="op">=</span> <span class="va">Sigma_i</span><span class="op">)</span>
  
  <span class="co"># cluster level</span>
  <span class="va">cluster_name</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">n_cluster</span>, each <span class="op">=</span> <span class="va">n</span> <span class="op">/</span> <span class="va">n_cluster</span><span class="op">)</span>
  <span class="va">Sigma_cl</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0</span>, <span class="fl">0</span>, <span class="va">rho</span><span class="op">)</span>, ncol <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>
  <span class="va">values_cl</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/mvtnorm/man/Mvnorm.html">rmvnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n_cluster</span>, sigma <span class="op">=</span> <span class="va">Sigma_cl</span><span class="op">)</span>
  
  <span class="co"># predictor var consists of individual- and cluster-level components</span>
  <span class="va">x</span> <span class="op">&lt;-</span> <span class="va">values_i</span><span class="op">[</span> , <span class="fl">1</span><span class="op">]</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="va">values_cl</span><span class="op">[</span> , <span class="fl">1</span><span class="op">]</span>, each <span class="op">=</span> <span class="va">n</span> <span class="op">/</span> <span class="va">n_cluster</span><span class="op">)</span>
  
  <span class="co"># error consists of individual- and cluster-level components</span>
  <span class="va">error</span> <span class="op">&lt;-</span> <span class="va">values_i</span><span class="op">[</span> , <span class="fl">2</span><span class="op">]</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="va">values_cl</span><span class="op">[</span> , <span class="fl">2</span><span class="op">]</span>, each <span class="op">=</span> <span class="va">n</span> <span class="op">/</span> <span class="va">n_cluster</span><span class="op">)</span>
  
  <span class="co"># data generating process</span>
  <span class="va">y</span> <span class="op">&lt;-</span> <span class="va">param</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">+</span> <span class="va">param</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">*</span><span class="va">x</span> <span class="op">+</span> <span class="va">error</span>
  
  <span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, cluster <span class="op">=</span> <span class="va">cluster_name</span><span class="op">)</span>
  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">df</span><span class="op">)</span>
<span class="op">}</span>

<span class="co"># Simulate a dataset with clusters and fit OLS</span>
<span class="co"># Calculate cluster-robust SE when cluster_robust = TRUE</span>
<span class="va">cluster_sim</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">param</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.1</span>, <span class="fl">.5</span><span class="op">)</span>, <span class="va">n</span> <span class="op">=</span> <span class="fl">1000</span>, <span class="va">n_cluster</span> <span class="op">=</span> <span class="fl">50</span>,
                        <span class="va">rho</span> <span class="op">=</span> <span class="fl">.5</span>, <span class="va">cluster_robust</span> <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span>
  <span class="co"># Required packages: mvtnorm, multiwayvcov</span>
  <span class="va">df</span> <span class="op">&lt;-</span> <span class="fu">gen_cluster</span><span class="op">(</span>param <span class="op">=</span> <span class="va">param</span>, n <span class="op">=</span> <span class="va">n</span> , n_cluster <span class="op">=</span> <span class="va">n_cluster</span>, rho <span class="op">=</span> <span class="va">rho</span><span class="op">)</span>
  <span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, data <span class="op">=</span> <span class="va">df</span><span class="op">)</span>
  <span class="va">b1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>
  <span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="va">cluster_robust</span><span class="op">)</span> <span class="op">{</span>
    <span class="va">Sigma</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/vcov.html">vcov</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span>
    <span class="va">se</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">Sigma</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span>
    <span class="va">b1_ci95</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/confint.html">confint</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span>, <span class="op">]</span>
  <span class="op">}</span> <span class="kw">else</span> <span class="op">{</span> <span class="co"># cluster-robust SE</span>
    <span class="va">Sigma</span> <span class="op">&lt;-</span> <span class="fu">cluster.vcov</span><span class="op">(</span><span class="va">fit</span>, <span class="op">~</span> <span class="va">cluster</span><span class="op">)</span>
    <span class="va">se</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">Sigma</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span>
    <span class="va">t_critical</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">qt</a></span><span class="op">(</span><span class="fl">.025</span>, df <span class="op">=</span> <span class="va">n</span> <span class="op">-</span> <span class="fl">2</span>, lower.tail <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span>
    <span class="va">lower</span> <span class="op">&lt;-</span> <span class="va">b1</span> <span class="op">-</span> <span class="va">t_critical</span><span class="op">*</span><span class="va">se</span>
    <span class="va">upper</span> <span class="op">&lt;-</span> <span class="va">b1</span> <span class="op">+</span> <span class="va">t_critical</span><span class="op">*</span><span class="va">se</span>
    <span class="va">b1_ci95</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">lower</span>, <span class="va">upper</span><span class="op">)</span>
  <span class="op">}</span>
  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">b1</span>, <span class="va">se</span>, <span class="va">b1_ci95</span><span class="op">)</span><span class="op">)</span>
<span class="op">}</span>

<span class="co"># Function to iterate the simulation. A data frame is returned.</span>
<span class="va">run_cluster_sim</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">n_sims</span> <span class="op">=</span> <span class="fl">1000</span>, <span class="va">param</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.1</span>, <span class="fl">.5</span><span class="op">)</span>, <span class="va">n</span> <span class="op">=</span> <span class="fl">1000</span>,
                            <span class="va">n_cluster</span> <span class="op">=</span> <span class="fl">50</span>, <span class="va">rho</span> <span class="op">=</span> <span class="fl">.5</span>, <span class="va">cluster_robust</span> <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span>
  <span class="co"># Required packages: mvtnorm, multiwayvcov, dplyr</span>
  <span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">replicate</a></span><span class="op">(</span><span class="va">n_sims</span>, <span class="fu">cluster_sim</span><span class="op">(</span>param <span class="op">=</span> <span class="va">param</span>, n <span class="op">=</span> <span class="va">n</span>, rho <span class="op">=</span> <span class="va">rho</span>,
                                      n_cluster <span class="op">=</span> <span class="va">n_cluster</span>,
                                      cluster_robust <span class="op">=</span> <span class="va">cluster_robust</span><span class="op">)</span><span class="op">)</span>
  <span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html">as.data.frame</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">df</span><span class="op">)</span><span class="op">)</span>
  <span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">df</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">'b1'</span>, <span class="st">'se_b1'</span>, <span class="st">'ci95_lower'</span>, <span class="st">'ci95_upper'</span><span class="op">)</span>
  <span class="va">df</span> <span class="op">&lt;-</span> <span class="va">df</span> <span class="op">%&gt;%</span> 
    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>id <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/context.html">n</a></span><span class="op">(</span><span class="op">)</span>,
           param_caught <span class="op">=</span> <span class="va">ci95_lower</span> <span class="op">&lt;=</span> <span class="va">param</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">&amp;</span> <span class="va">ci95_upper</span> <span class="op">&gt;=</span> <span class="va">param</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span>
  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">df</span><span class="op">)</span>
<span class="op">}</span>

<span class="co"># Distribution of the estimator and confidence intervals</span>
<span class="va">sim_params</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.4</span>, <span class="fl">0</span><span class="op">)</span>   <span class="co"># beta1 = 0: no effect of x on y</span>
<span class="va">sim_nocluster</span> <span class="op">&lt;-</span> <span class="fu">run_cluster_sim</span><span class="op">(</span>n_sims <span class="op">=</span> <span class="fl">10000</span>, param <span class="op">=</span> <span class="va">sim_params</span>, rho <span class="op">=</span> <span class="fl">0</span><span class="op">)</span>
<span class="va">hist_nocluster</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">sim_nocluster</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">b1</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_histogram.html">geom_histogram</a></span><span class="op">(</span>color <span class="op">=</span> <span class="st">'black'</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_vline</a></span><span class="op">(</span>xintercept <span class="op">=</span> <span class="va">sim_params</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, color <span class="op">=</span> <span class="st">'red'</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">hist_nocluster</span><span class="op">)</span>

<span class="va">ci95_nocluster</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/sample_n.html">sample_n</a></span><span class="op">(</span><span class="va">sim_nocluster</span>, <span class="fl">100</span><span class="op">)</span>,
                         <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/reorder.factor.html">reorder</a></span><span class="op">(</span><span class="va">id</span>, <span class="va">b1</span><span class="op">)</span>, y <span class="op">=</span> <span class="va">b1</span>, 
                             ymin <span class="op">=</span> <span class="va">ci95_lower</span>, ymax <span class="op">=</span> <span class="va">ci95_upper</span>,
                             color <span class="op">=</span> <span class="va">param_caught</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_hline</a></span><span class="op">(</span>yintercept <span class="op">=</span> <span class="va">sim_params</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, linetype <span class="op">=</span> <span class="st">'dashed'</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_linerange.html">geom_pointrange</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>x <span class="op">=</span> <span class="st">'sim ID'</span>, y <span class="op">=</span> <span class="st">'b1'</span>, title <span class="op">=</span> <span class="st">'Randomly Chosen 100 95% CIs'</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_hue.html">scale_color_discrete</a></span><span class="op">(</span>name <span class="op">=</span> <span class="st">'True param value'</span>, labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">'missed'</span>, <span class="st">'hit'</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/coord_flip.html">coord_flip</a></span><span class="op">(</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">ci95_nocluster</span><span class="op">)</span>

<span class="va">sim_nocluster</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarize</a></span><span class="op">(</span>type1_error <span class="op">=</span> <span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">param_caught</span><span class="op">)</span><span class="op">/</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/context.html">n</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p>As we can see in Figure <a href="ch1.html#fig:ls-dist-noclust">2.5</a>, the least squares estimate is centered on its true population parameter.</p>
<div class="figure">
<span id="fig:ls-dist-noclust"></span>
<img src="graphics/ls_dist_hist_noclust.jpg" alt="Distribution of the least squares estimator over 1,000 random draws." width="100%"><p class="caption">
Figure 2.5: Distribution of the least squares estimator over 1,000 random draws.
</p>
</div>
<p>Setting the significance level at 5%, we should incorrectly reject the null that <span class="math inline">\(\beta_1=0\)</span> about 5% of the time in our simulations. But let’s check the confidence intervals. As can be seen in Figure <a href="ch1.html#fig:ls-ci-noclust">2.6</a>, about 95% of the 95% confidence intervals contain the true value of <span class="math inline">\(\beta_1\)</span>, which is zero. In words, this means that we incorrectly reject the null about 5% of the time.</p>
<div class="figure">
<span id="fig:ls-ci-noclust"></span>
<img src="graphics/ls_ci_noclust.jpg" alt="Distribution of the 95% confidence intervals with coloring showing those which are incorrectly rejecting the null." width="100%"><p class="caption">
Figure 2.6: Distribution of the 95% confidence intervals with coloring showing those which are incorrectly rejecting the null.
</p>
</div>
<p>But what happens when we use least squares with <em>clustered</em> data? To see that, let’s resimulate our data with observations that are no longer independent draws in a given cluster of observations.</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/cluster2.do"><code>cluster2.do</code></a></em></p>
<div class="sourceCode" id="cb11"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb11-1"><a href="ch1.html#cb11-1" aria-hidden="true"></a><span class="kw">clear</span> <span class="ot">all</span></span>
<span id="cb11-2"><a href="ch1.html#cb11-2" aria-hidden="true"></a><span class="kw">set</span> <span class="dv">seed</span> 20140</span>
<span id="cb11-3"><a href="ch1.html#cb11-3" aria-hidden="true"></a><span class="kw">local</span> n_sims = 1000</span>
<span id="cb11-4"><a href="ch1.html#cb11-4" aria-hidden="true"></a><span class="kw">set</span> <span class="kw">obs</span> <span class="ot">`n_sims'</span></span>
<span id="cb11-5"><a href="ch1.html#cb11-5" aria-hidden="true"></a></span>
<span id="cb11-6"><a href="ch1.html#cb11-6" aria-hidden="true"></a>* Create the variables that will contain the results <span class="kw">of</span> each simulation</span>
<span id="cb11-7"><a href="ch1.html#cb11-7" aria-hidden="true"></a><span class="kw">generate</span> beta_0 = .</span>
<span id="cb11-8"><a href="ch1.html#cb11-8" aria-hidden="true"></a><span class="kw">generate</span> beta_0_l = .</span>
<span id="cb11-9"><a href="ch1.html#cb11-9" aria-hidden="true"></a><span class="kw">generate</span> beta_0_u = .</span>
<span id="cb11-10"><a href="ch1.html#cb11-10" aria-hidden="true"></a><span class="kw">generate</span> beta_1 = .</span>
<span id="cb11-11"><a href="ch1.html#cb11-11" aria-hidden="true"></a><span class="kw">generate</span> beta_1_l = .</span>
<span id="cb11-12"><a href="ch1.html#cb11-12" aria-hidden="true"></a><span class="kw">generate</span> beta_1_u = .</span>
<span id="cb11-13"><a href="ch1.html#cb11-13" aria-hidden="true"></a></span>
<span id="cb11-14"><a href="ch1.html#cb11-14" aria-hidden="true"></a></span>
<span id="cb11-15"><a href="ch1.html#cb11-15" aria-hidden="true"></a>* Provide the true population parameters</span>
<span id="cb11-16"><a href="ch1.html#cb11-16" aria-hidden="true"></a><span class="kw">local</span> beta_0_true = 0.4</span>
<span id="cb11-17"><a href="ch1.html#cb11-17" aria-hidden="true"></a><span class="kw">local</span> beta_1_true = 0</span>
<span id="cb11-18"><a href="ch1.html#cb11-18" aria-hidden="true"></a><span class="kw">local</span> rho = 0.5</span>
<span id="cb11-19"><a href="ch1.html#cb11-19" aria-hidden="true"></a></span>
<span id="cb11-20"><a href="ch1.html#cb11-20" aria-hidden="true"></a>* Simulate a linear regression. Clustered <span class="kw">data</span> (x and <span class="fu">e</span> are clustered)</span>
<span id="cb11-21"><a href="ch1.html#cb11-21" aria-hidden="true"></a></span>
<span id="cb11-22"><a href="ch1.html#cb11-22" aria-hidden="true"></a></span>
<span id="cb11-23"><a href="ch1.html#cb11-23" aria-hidden="true"></a><span class="kw">quietly</span> {</span>
<span id="cb11-24"><a href="ch1.html#cb11-24" aria-hidden="true"></a><span class="kw">forvalues</span> i = 1(1) <span class="ot">`n_sims'</span> {</span>
<span id="cb11-25"><a href="ch1.html#cb11-25" aria-hidden="true"></a>    <span class="kw">preserve</span></span>
<span id="cb11-26"><a href="ch1.html#cb11-26" aria-hidden="true"></a>    <span class="kw">clear</span></span>
<span id="cb11-27"><a href="ch1.html#cb11-27" aria-hidden="true"></a>    <span class="kw">set</span> <span class="kw">obs</span> 50</span>
<span id="cb11-28"><a href="ch1.html#cb11-28" aria-hidden="true"></a>    </span>
<span id="cb11-29"><a href="ch1.html#cb11-29" aria-hidden="true"></a>    * Generate <span class="kw">cluster</span> <span class="dv">level</span> <span class="kw">data</span>: clustered x and <span class="fu">e</span></span>
<span id="cb11-30"><a href="ch1.html#cb11-30" aria-hidden="true"></a>    <span class="kw">generate</span> <span class="kw">int</span> cluster_ID = <span class="dt">_n</span></span>
<span id="cb11-31"><a href="ch1.html#cb11-31" aria-hidden="true"></a>    <span class="kw">generate</span> x_cluster = rnormal(0,1)</span>
<span id="cb11-32"><a href="ch1.html#cb11-32" aria-hidden="true"></a>    <span class="kw">generate</span> e_cluster = rnormal(0, <span class="fu">sqrt</span>(<span class="ot">`rho'</span>))</span>
<span id="cb11-33"><a href="ch1.html#cb11-33" aria-hidden="true"></a>    expand 20</span>
<span id="cb11-34"><a href="ch1.html#cb11-34" aria-hidden="true"></a>    <span class="kw">bysort</span> cluster_ID : <span class="kw">gen</span> <span class="kw">int</span> ind_in_clusterID = <span class="dt">_n</span></span>
<span id="cb11-35"><a href="ch1.html#cb11-35" aria-hidden="true"></a></span>
<span id="cb11-36"><a href="ch1.html#cb11-36" aria-hidden="true"></a>    * Generate individual <span class="dv">level</span> <span class="kw">data</span></span>
<span id="cb11-37"><a href="ch1.html#cb11-37" aria-hidden="true"></a>    <span class="kw">generate</span> x_individual = rnormal(0,1)</span>
<span id="cb11-38"><a href="ch1.html#cb11-38" aria-hidden="true"></a>    <span class="kw">generate</span> e_individual = rnormal(0,<span class="fu">sqrt</span>(1 - <span class="ot">`rho'</span>))</span>
<span id="cb11-39"><a href="ch1.html#cb11-39" aria-hidden="true"></a></span>
<span id="cb11-40"><a href="ch1.html#cb11-40" aria-hidden="true"></a>    * Generate x and <span class="fu">e</span></span>
<span id="cb11-41"><a href="ch1.html#cb11-41" aria-hidden="true"></a>    <span class="kw">generate</span> x = x_individual + x_cluster</span>
<span id="cb11-42"><a href="ch1.html#cb11-42" aria-hidden="true"></a>    <span class="kw">generate</span> <span class="fu">e</span> = e_individual + e_cluster</span>
<span id="cb11-43"><a href="ch1.html#cb11-43" aria-hidden="true"></a>    <span class="kw">generate</span> <span class="fu">y</span> = <span class="ot">`beta_0_true'</span> + <span class="ot">`beta_1_true'</span>*x + <span class="fu">e</span></span>
<span id="cb11-44"><a href="ch1.html#cb11-44" aria-hidden="true"></a>    </span>
<span id="cb11-45"><a href="ch1.html#cb11-45" aria-hidden="true"></a>* Least Squares Estimates</span>
<span id="cb11-46"><a href="ch1.html#cb11-46" aria-hidden="true"></a>    <span class="kw">regress</span> <span class="fu">y</span> x</span>
<span id="cb11-47"><a href="ch1.html#cb11-47" aria-hidden="true"></a>    <span class="kw">local</span> b0 = _b[<span class="dt">_cons</span>]</span>
<span id="cb11-48"><a href="ch1.html#cb11-48" aria-hidden="true"></a>    <span class="kw">local</span> b1 = _b[x]</span>
<span id="cb11-49"><a href="ch1.html#cb11-49" aria-hidden="true"></a>    <span class="kw">local</span> df = <span class="fu">e</span>(df_r)</span>
<span id="cb11-50"><a href="ch1.html#cb11-50" aria-hidden="true"></a>    <span class="kw">local</span> critical_value = invt(<span class="ot">`df'</span>, 0.975)</span>
<span id="cb11-51"><a href="ch1.html#cb11-51" aria-hidden="true"></a>    * Save the results</span>
<span id="cb11-52"><a href="ch1.html#cb11-52" aria-hidden="true"></a>    <span class="kw">restore</span></span>
<span id="cb11-53"><a href="ch1.html#cb11-53" aria-hidden="true"></a>    <span class="kw">replace</span> beta_0 = <span class="ot">`b0'</span> <span class="kw">in</span> <span class="ot">`i'</span></span>
<span id="cb11-54"><a href="ch1.html#cb11-54" aria-hidden="true"></a>    <span class="kw">replace</span> beta_0_l = beta_0 - <span class="ot">`critical_value'</span>*_se[<span class="dt">_cons</span>]</span>
<span id="cb11-55"><a href="ch1.html#cb11-55" aria-hidden="true"></a>    <span class="kw">replace</span> beta_0_u = beta_0 + <span class="ot">`critical_value'</span>*_se[<span class="dt">_cons</span>]</span>
<span id="cb11-56"><a href="ch1.html#cb11-56" aria-hidden="true"></a>    <span class="kw">replace</span> beta_1 = <span class="ot">`b1'</span> <span class="kw">in</span> <span class="ot">`i'</span></span>
<span id="cb11-57"><a href="ch1.html#cb11-57" aria-hidden="true"></a>    <span class="kw">replace</span> beta_1_l = beta_1 - <span class="ot">`critical_value'</span>*_se[x]</span>
<span id="cb11-58"><a href="ch1.html#cb11-58" aria-hidden="true"></a>    <span class="kw">replace</span> beta_1_u = beta_1 + <span class="ot">`critical_value'</span>*_se[x]</span>
<span id="cb11-59"><a href="ch1.html#cb11-59" aria-hidden="true"></a>}</span>
<span id="cb11-60"><a href="ch1.html#cb11-60" aria-hidden="true"></a>}</span>
<span id="cb11-61"><a href="ch1.html#cb11-61" aria-hidden="true"></a></span>
<span id="cb11-62"><a href="ch1.html#cb11-62" aria-hidden="true"></a><span class="kw">gen</span> false = (beta_1_l &gt; 0 )</span>
<span id="cb11-63"><a href="ch1.html#cb11-63" aria-hidden="true"></a><span class="kw">replace</span> false = 2 <span class="kw">if</span> beta_1_u &lt; 0</span>
<span id="cb11-64"><a href="ch1.html#cb11-64" aria-hidden="true"></a><span class="kw">replace</span> false = 3 <span class="kw">if</span> false == 0</span>
<span id="cb11-65"><a href="ch1.html#cb11-65" aria-hidden="true"></a><span class="kw">tab</span> false</span>
<span id="cb11-66"><a href="ch1.html#cb11-66" aria-hidden="true"></a></span>
<span id="cb11-67"><a href="ch1.html#cb11-67" aria-hidden="true"></a>* Plot the parameter estimate</span>
<span id="cb11-68"><a href="ch1.html#cb11-68" aria-hidden="true"></a><span class="kw">hist</span> beta_1, <span class="kw">frequency</span> <span class="bn">addplot</span>(pci 0 0 100 0) <span class="bn">title</span>(<span class="st">"Least squares estimates of clustered Data"</span>) <span class="bn">subtitle</span>(<span class="st">" Monte Carlo simulation of the slope"</span>) <span class="bn">legend</span>(<span class="kw">label</span>(1 <span class="st">"Distribution of least squares estimates"</span>) <span class="kw">label</span>(2 <span class="st">"True population parameter"</span>)) <span class="bn">xtitle</span>(<span class="st">"Parameter estimate"</span>)</span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/cluster2.R"><code>cluster2.R</code></a></em></p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">#- Analysis of Clustered Data - part 2</span>
<span class="co">#- Courtesy of Dr. Yuki Yanai, </span>
<span class="co">#- http://yukiyanai.github.io/teaching/rm1/contents/R/clustered-data-analysis.html</span>

<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">'arm'</span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="http://mvtnorm.R-forge.R-project.org">'mvtnorm'</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://github.com/lme4/lme4/">'lme4'</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">'multiwayvcov'</span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">'clusterSEs'</span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="http://ggplot2.tidyverse.org">'ggplot2'</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://dplyr.tidyverse.org">'dplyr'</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="http://haven.tidyverse.org">'haven'</a></span><span class="op">)</span>

<span class="co">#Data with clusters</span>
<span class="va">sim_params</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.4</span>, <span class="fl">0</span><span class="op">)</span>   <span class="co"># beta1 = 0: no effect of x on y</span>
<span class="va">sim_cluster_ols</span> <span class="op">&lt;-</span> <span class="fu">run_cluster_sim</span><span class="op">(</span>n_sims <span class="op">=</span> <span class="fl">10000</span>, param <span class="op">=</span> <span class="va">sim_params</span><span class="op">)</span>
<span class="va">hist_cluster_ols</span> <span class="op">&lt;-</span> <span class="va">hist_nocluster</span> <span class="op">%+%</span> <span class="va">sim_cluster_ols</span>
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">hist_cluster_ols</span><span class="op">)</span></code></pre></div>
<div class="figure">
<span id="fig:ls-dist-clust"></span>
<img src="graphics/ls_dist_clust.jpg" alt="Distribution of the least squares estimator over 1,000 random draws." width="100%"><p class="caption">
Figure 2.7: Distribution of the least squares estimator over 1,000 random draws.
</p>
</div>
<p>As can be seen in Figure <a href="ch1.html#fig:ls-dist-clust">2.7</a>, the least squares estimate has a narrower spread than that of the estimates when the data isn’t clustered. But to see this a bit more clearly, let’s look at the confidence intervals again.</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/cluster3.do"><code>cluster3.do</code></a></em></p>
<div class="sourceCode" id="cb13"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb13-1"><a href="ch1.html#cb13-1" aria-hidden="true"></a><span class="kw">sort</span> beta_1</span>
<span id="cb13-2"><a href="ch1.html#cb13-2" aria-hidden="true"></a><span class="kw">gen</span> <span class="kw">int</span> sim_ID = <span class="dt">_n</span></span>
<span id="cb13-3"><a href="ch1.html#cb13-3" aria-hidden="true"></a><span class="kw">gen</span> beta_1_True = 0</span>
<span id="cb13-4"><a href="ch1.html#cb13-4" aria-hidden="true"></a></span>
<span id="cb13-5"><a href="ch1.html#cb13-5" aria-hidden="true"></a>* Plot <span class="kw">of</span> the Confidence Interval</span>
<span id="cb13-6"><a href="ch1.html#cb13-6" aria-hidden="true"></a><span class="kw">twoway</span> rcap beta_1_l beta_1_u sim_ID <span class="kw">if</span> beta_1_l &gt; 0 | beta_1_u &lt; 0  , horizontal lcolor(<span class="bn">pink</span>) || || <span class="co">///</span></span>
<span id="cb13-7"><a href="ch1.html#cb13-7" aria-hidden="true"></a>rcap beta_1_l beta_1_u sim_ID <span class="kw">if</span> beta_1_l &lt; 0 &amp; beta_1_u &gt; 0 , horizontal ysc(<span class="fu">r</span>(0)) || || <span class="co">///</span></span>
<span id="cb13-8"><a href="ch1.html#cb13-8" aria-hidden="true"></a>connected sim_ID beta_1 || || <span class="co">///</span></span>
<span id="cb13-9"><a href="ch1.html#cb13-9" aria-hidden="true"></a><span class="kw">line</span> sim_ID beta_1_True, lpattern(<span class="kw">dash</span>) lcolor(<span class="bn">black</span>) lwidth(1) <span class="co">///  </span></span>
<span id="cb13-10"><a href="ch1.html#cb13-10" aria-hidden="true"></a><span class="bn">title</span>(<span class="st">"Least squares estimates of clustered data"</span>) <span class="bn">subtitle</span>(<span class="st">" 95% Confidence interval of the slope"</span>) <span class="co">///</span></span>
<span id="cb13-11"><a href="ch1.html#cb13-11" aria-hidden="true"></a><span class="bn">legend</span>(<span class="kw">label</span>(1 <span class="st">"Missed"</span>) <span class="kw">label</span>(2 <span class="st">"Hit"</span>) <span class="kw">label</span>(3 <span class="st">"OLS estimates"</span>) <span class="kw">label</span>(4 <span class="st">"True population parameter"</span>)) <span class="bn">xtitle</span>(<span class="st">"Parameter estimates"</span>) <span class="co">///</span></span>
<span id="cb13-12"><a href="ch1.html#cb13-12" aria-hidden="true"></a><span class="bn">ytitle</span>(<span class="st">"Simulation"</span>)</span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/cluster3.R"><code>cluster3.R</code></a></em></p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">#- Analysis of Clustered Data - part 3</span>
<span class="co">#- Courtesy of Dr. Yuki Yanai, </span>
<span class="co">#- http://yukiyanai.github.io/teaching/rm1/contents/R/clustered-data-analysis.html</span>

<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">'arm'</span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="http://mvtnorm.R-forge.R-project.org">'mvtnorm'</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://github.com/lme4/lme4/">'lme4'</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">'multiwayvcov'</span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">'clusterSEs'</span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="http://ggplot2.tidyverse.org">'ggplot2'</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://dplyr.tidyverse.org">'dplyr'</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="http://haven.tidyverse.org">'haven'</a></span><span class="op">)</span>

<span class="co">#Confidence interval</span>
<span class="va">ci95_cluster_ols</span> <span class="op">&lt;-</span> <span class="va">ci95_nocluster</span> <span class="op">%+%</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/sample_n.html">sample_n</a></span><span class="op">(</span><span class="va">sim_cluster_ols</span>, <span class="fl">100</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">ci95_cluster_ols</span><span class="op">)</span>

<span class="va">sim_cluster_ols</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarize</a></span><span class="op">(</span>type1_error <span class="op">=</span> <span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">param_caught</span><span class="op">)</span><span class="op">/</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/context.html">n</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<div class="figure">
<span id="fig:ls-ci-clust"></span>
<img src="graphics/ls_ci_clust.jpg" alt="Distribution of 1,000 95% confidence intervals with dashed region representing those estimates that incorrectly reject the null." width="100%"><p class="caption">
Figure 2.8: Distribution of 1,000 95% confidence intervals with dashed region representing those estimates that incorrectly reject the null.
</p>
</div>
<p>Figure <a href="ch1.html#fig:ls-ci-clust">2.8</a> shows the distribution of 95% confidence intervals from the least squares estimates. As can be seen, a much larger number of estimates incorrectly rejected the null hypothesis when the data was clustered. The standard deviation of the estimator shrinks under clustered data, causing us to reject the null incorrectly too often. So what can we do?</p>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/Do/cluster4.do"><code>cluster4.do</code></a></em></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb15-1"><a href="ch1.html#cb15-1" aria-hidden="true"></a>* Robust Estimates</span>
<span id="cb15-2"><a href="ch1.html#cb15-2" aria-hidden="true"></a><span class="kw">clear</span> <span class="ot">all</span></span>
<span id="cb15-3"><a href="ch1.html#cb15-3" aria-hidden="true"></a><span class="kw">local</span> n_sims = 1000</span>
<span id="cb15-4"><a href="ch1.html#cb15-4" aria-hidden="true"></a><span class="kw">set</span> <span class="kw">obs</span> <span class="ot">`n_sims'</span></span>
<span id="cb15-5"><a href="ch1.html#cb15-5" aria-hidden="true"></a></span>
<span id="cb15-6"><a href="ch1.html#cb15-6" aria-hidden="true"></a>* Create the variables that will contain the results <span class="kw">of</span> each simulation</span>
<span id="cb15-7"><a href="ch1.html#cb15-7" aria-hidden="true"></a><span class="kw">generate</span> beta_0_robust = .</span>
<span id="cb15-8"><a href="ch1.html#cb15-8" aria-hidden="true"></a><span class="kw">generate</span> beta_0_l_robust = .</span>
<span id="cb15-9"><a href="ch1.html#cb15-9" aria-hidden="true"></a><span class="kw">generate</span> beta_0_u_robust = .</span>
<span id="cb15-10"><a href="ch1.html#cb15-10" aria-hidden="true"></a><span class="kw">generate</span> beta_1_robust = .</span>
<span id="cb15-11"><a href="ch1.html#cb15-11" aria-hidden="true"></a><span class="kw">generate</span> beta_1_l_robust = .</span>
<span id="cb15-12"><a href="ch1.html#cb15-12" aria-hidden="true"></a><span class="kw">generate</span> beta_1_u_robust = .</span>
<span id="cb15-13"><a href="ch1.html#cb15-13" aria-hidden="true"></a></span>
<span id="cb15-14"><a href="ch1.html#cb15-14" aria-hidden="true"></a>* Provide the true population parameters</span>
<span id="cb15-15"><a href="ch1.html#cb15-15" aria-hidden="true"></a><span class="kw">local</span> beta_0_true = 0.4</span>
<span id="cb15-16"><a href="ch1.html#cb15-16" aria-hidden="true"></a><span class="kw">local</span> beta_1_true = 0</span>
<span id="cb15-17"><a href="ch1.html#cb15-17" aria-hidden="true"></a><span class="kw">local</span> rho = 0.5</span>
<span id="cb15-18"><a href="ch1.html#cb15-18" aria-hidden="true"></a></span>
<span id="cb15-19"><a href="ch1.html#cb15-19" aria-hidden="true"></a><span class="kw">quietly</span> {</span>
<span id="cb15-20"><a href="ch1.html#cb15-20" aria-hidden="true"></a><span class="kw">forvalues</span> i = 1(1) <span class="ot">`n_sims'</span> {</span>
<span id="cb15-21"><a href="ch1.html#cb15-21" aria-hidden="true"></a>    <span class="kw">preserve</span></span>
<span id="cb15-22"><a href="ch1.html#cb15-22" aria-hidden="true"></a>    <span class="kw">clear</span></span>
<span id="cb15-23"><a href="ch1.html#cb15-23" aria-hidden="true"></a>    <span class="kw">set</span> <span class="kw">obs</span> 50</span>
<span id="cb15-24"><a href="ch1.html#cb15-24" aria-hidden="true"></a>    </span>
<span id="cb15-25"><a href="ch1.html#cb15-25" aria-hidden="true"></a>    * Generate <span class="kw">cluster</span> <span class="dv">level</span> <span class="kw">data</span>: clustered x and <span class="fu">e</span></span>
<span id="cb15-26"><a href="ch1.html#cb15-26" aria-hidden="true"></a>    <span class="kw">generate</span> <span class="kw">int</span> cluster_ID = <span class="dt">_n</span></span>
<span id="cb15-27"><a href="ch1.html#cb15-27" aria-hidden="true"></a>    <span class="kw">generate</span> x_cluster = rnormal(0,1)</span>
<span id="cb15-28"><a href="ch1.html#cb15-28" aria-hidden="true"></a>    <span class="kw">generate</span> e_cluster = rnormal(0, <span class="fu">sqrt</span>(<span class="ot">`rho'</span>))</span>
<span id="cb15-29"><a href="ch1.html#cb15-29" aria-hidden="true"></a>    expand 20</span>
<span id="cb15-30"><a href="ch1.html#cb15-30" aria-hidden="true"></a>    <span class="kw">bysort</span> cluster_ID : <span class="kw">gen</span> <span class="kw">int</span> ind_in_clusterID = <span class="dt">_n</span></span>
<span id="cb15-31"><a href="ch1.html#cb15-31" aria-hidden="true"></a></span>
<span id="cb15-32"><a href="ch1.html#cb15-32" aria-hidden="true"></a>    * Generate individual <span class="dv">level</span> <span class="kw">data</span></span>
<span id="cb15-33"><a href="ch1.html#cb15-33" aria-hidden="true"></a>    <span class="kw">generate</span> x_individual = rnormal(0,1)</span>
<span id="cb15-34"><a href="ch1.html#cb15-34" aria-hidden="true"></a>    <span class="kw">generate</span> e_individual = rnormal(0,<span class="fu">sqrt</span>(1 - <span class="ot">`rho'</span>))</span>
<span id="cb15-35"><a href="ch1.html#cb15-35" aria-hidden="true"></a></span>
<span id="cb15-36"><a href="ch1.html#cb15-36" aria-hidden="true"></a>    * Generate x and <span class="fu">e</span></span>
<span id="cb15-37"><a href="ch1.html#cb15-37" aria-hidden="true"></a>    <span class="kw">generate</span> x = x_individual + x_cluster</span>
<span id="cb15-38"><a href="ch1.html#cb15-38" aria-hidden="true"></a>    <span class="kw">generate</span> <span class="fu">e</span> = e_individual + e_cluster</span>
<span id="cb15-39"><a href="ch1.html#cb15-39" aria-hidden="true"></a>    <span class="kw">generate</span> <span class="fu">y</span> = <span class="ot">`beta_0_true'</span> + <span class="ot">`beta_1_true'</span>*x + <span class="fu">e</span></span>
<span id="cb15-40"><a href="ch1.html#cb15-40" aria-hidden="true"></a>    <span class="kw">regress</span> <span class="fu">y</span> x, cl(cluster_ID)</span>
<span id="cb15-41"><a href="ch1.html#cb15-41" aria-hidden="true"></a>    <span class="kw">local</span> b0_robust = _b[<span class="dt">_cons</span>]</span>
<span id="cb15-42"><a href="ch1.html#cb15-42" aria-hidden="true"></a>    <span class="kw">local</span> b1_robust = _b[x]</span>
<span id="cb15-43"><a href="ch1.html#cb15-43" aria-hidden="true"></a>    <span class="kw">local</span> df = <span class="fu">e</span>(df_r)</span>
<span id="cb15-44"><a href="ch1.html#cb15-44" aria-hidden="true"></a>    <span class="kw">local</span> critical_value = invt(<span class="ot">`df'</span>, 0.975)</span>
<span id="cb15-45"><a href="ch1.html#cb15-45" aria-hidden="true"></a>    * Save the results</span>
<span id="cb15-46"><a href="ch1.html#cb15-46" aria-hidden="true"></a>    <span class="kw">restore</span></span>
<span id="cb15-47"><a href="ch1.html#cb15-47" aria-hidden="true"></a>    <span class="kw">replace</span> beta_0_robust = <span class="ot">`b0_robust'</span> <span class="kw">in</span> <span class="ot">`i'</span></span>
<span id="cb15-48"><a href="ch1.html#cb15-48" aria-hidden="true"></a>    <span class="kw">replace</span> beta_0_l_robust = beta_0_robust - <span class="ot">`critical_value'</span>*_se[<span class="dt">_cons</span>]</span>
<span id="cb15-49"><a href="ch1.html#cb15-49" aria-hidden="true"></a>    <span class="kw">replace</span> beta_0_u_robust = beta_0_robust + <span class="ot">`critical_value'</span>*_se[<span class="dt">_cons</span>]</span>
<span id="cb15-50"><a href="ch1.html#cb15-50" aria-hidden="true"></a>    <span class="kw">replace</span> beta_1_robust = <span class="ot">`b1_robust'</span> <span class="kw">in</span> <span class="ot">`i'</span></span>
<span id="cb15-51"><a href="ch1.html#cb15-51" aria-hidden="true"></a>    <span class="kw">replace</span> beta_1_l_robust = beta_1_robust - <span class="ot">`critical_value'</span>*_se[x]</span>
<span id="cb15-52"><a href="ch1.html#cb15-52" aria-hidden="true"></a>    <span class="kw">replace</span> beta_1_u_robust = beta_1_robust + <span class="ot">`critical_value'</span>*_se[x]</span>
<span id="cb15-53"><a href="ch1.html#cb15-53" aria-hidden="true"></a></span>
<span id="cb15-54"><a href="ch1.html#cb15-54" aria-hidden="true"></a>}</span>
<span id="cb15-55"><a href="ch1.html#cb15-55" aria-hidden="true"></a>}</span>
<span id="cb15-56"><a href="ch1.html#cb15-56" aria-hidden="true"></a></span>
<span id="cb15-57"><a href="ch1.html#cb15-57" aria-hidden="true"></a>* Plot the <span class="kw">histogram</span> <span class="kw">of</span> the parameters <span class="kw">estimates</span> <span class="kw">of</span> the <span class="kw">robust</span> least squares</span>
<span id="cb15-58"><a href="ch1.html#cb15-58" aria-hidden="true"></a><span class="kw">gen</span> false = (beta_1_l_robust &gt; 0 )</span>
<span id="cb15-59"><a href="ch1.html#cb15-59" aria-hidden="true"></a><span class="kw">replace</span> false = 2 <span class="kw">if</span> beta_1_u_robust &lt; 0</span>
<span id="cb15-60"><a href="ch1.html#cb15-60" aria-hidden="true"></a><span class="kw">replace</span> false = 3 <span class="kw">if</span> false == 0</span>
<span id="cb15-61"><a href="ch1.html#cb15-61" aria-hidden="true"></a><span class="kw">tab</span> false</span>
<span id="cb15-62"><a href="ch1.html#cb15-62" aria-hidden="true"></a></span>
<span id="cb15-63"><a href="ch1.html#cb15-63" aria-hidden="true"></a>* Plot the parameter estimate</span>
<span id="cb15-64"><a href="ch1.html#cb15-64" aria-hidden="true"></a><span class="kw">hist</span> beta_1_robust, <span class="kw">frequency</span> <span class="bn">addplot</span>(pci 0 0 110 0) <span class="bn">title</span>(<span class="st">"Robust least squares estimates of clustered data"</span>) <span class="bn">subtitle</span>(<span class="st">" Monte Carlo simulation of the slope"</span>) <span class="bn">legend</span>(<span class="kw">label</span>(1 <span class="st">"Distribution of robust least squares estimates"</span>) <span class="kw">label</span>(2 <span class="st">"True population parameter"</span>)) <span class="bn">xtitle</span>(<span class="st">"Parameter estimate"</span>)</span>
<span id="cb15-65"><a href="ch1.html#cb15-65" aria-hidden="true"></a></span>
<span id="cb15-66"><a href="ch1.html#cb15-66" aria-hidden="true"></a><span class="kw">sort</span> beta_1_robust</span>
<span id="cb15-67"><a href="ch1.html#cb15-67" aria-hidden="true"></a><span class="kw">gen</span> <span class="kw">int</span> sim_ID = <span class="dt">_n</span></span>
<span id="cb15-68"><a href="ch1.html#cb15-68" aria-hidden="true"></a><span class="kw">gen</span> beta_1_True = 0</span>
<span id="cb15-69"><a href="ch1.html#cb15-69" aria-hidden="true"></a></span>
<span id="cb15-70"><a href="ch1.html#cb15-70" aria-hidden="true"></a>* Plot <span class="kw">of</span> the Confidence Interval</span>
<span id="cb15-71"><a href="ch1.html#cb15-71" aria-hidden="true"></a><span class="kw">twoway</span> rcap beta_1_l_robust beta_1_u_robust sim_ID <span class="kw">if</span> beta_1_l_robust &gt; 0 | beta_1_u_robust &lt; 0, horizontal lcolor(<span class="bn">pink</span>) || || rcap beta_1_l_robust beta_1_u_robust sim_ID <span class="kw">if</span> beta_1_l_robust &lt; 0 &amp; beta_1_u_robust &gt; 0 , horizontal ysc(<span class="fu">r</span>(0)) || || connected sim_ID beta_1_robust || || <span class="kw">line</span> sim_ID beta_1_True, lpattern(<span class="kw">dash</span>) lcolor(<span class="bn">black</span>) lwidth(1) <span class="bn">title</span>(<span class="st">"Robust least squares estimates of clustered data"</span>) <span class="bn">subtitle</span>(<span class="st">" 95% Confidence interval of the slope"</span>) <span class="bn">legend</span>(<span class="kw">label</span>(1 <span class="st">"Missed"</span>) <span class="kw">label</span>(2 <span class="st">"Hit"</span>) <span class="kw">label</span>(3 <span class="st">"Robust estimates"</span>) <span class="kw">label</span>(4 <span class="st">"True population parameter"</span>)) <span class="bn">xtitle</span>(<span class="st">"Parameter estimates"</span>) <span class="bn">ytitle</span>(<span class="st">"Simulation"</span>)</span></code></pre></div>
<p><em><a href="https://github.com/scunning1975/mixtape/blob/master/R/cluster4.R"><code>cluster4.R</code></a></em></p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">#- Analysis of Clustered Data - part 4</span>
<span class="co">#- Courtesy of Dr. Yuki Yanai, </span>
<span class="co">#- http://yukiyanai.github.io/teaching/rm1/contents/R/clustered-data-analysis.html</span>

<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">'arm'</span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="http://mvtnorm.R-forge.R-project.org">'mvtnorm'</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://github.com/lme4/lme4/">'lme4'</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">'multiwayvcov'</span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">'clusterSEs'</span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="http://ggplot2.tidyverse.org">'ggplot2'</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://dplyr.tidyverse.org">'dplyr'</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="http://haven.tidyverse.org">'haven'</a></span><span class="op">)</span>

<span class="co">#clustered robust</span>
<span class="va">sim_params</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.4</span>, <span class="fl">0</span><span class="op">)</span>   <span class="co"># beta1 = 0: no effect of x on y</span>
<span class="va">sim_cluster_robust</span> <span class="op">&lt;-</span> <span class="fu">run_cluster_sim</span><span class="op">(</span>n_sims <span class="op">=</span> <span class="fl">10000</span>, param <span class="op">=</span> <span class="va">sim_params</span>,
                                      cluster_robust <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>

<span class="va">hist_cluster_robust</span> <span class="op">&lt;-</span> <span class="va">hist_nocluster</span> <span class="op">%+%</span> <span class="va">sim_cluster_ols</span>
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">hist_cluster_robust</span><span class="op">)</span>

<span class="co">#Confidence Intervals</span>
<span class="va">ci95_cluster_robust</span> <span class="op">&lt;-</span> <span class="va">ci95_nocluster</span> <span class="op">%+%</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/sample_n.html">sample_n</a></span><span class="op">(</span><span class="va">sim_cluster_robust</span>, <span class="fl">100</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">ci95_cluster_robust</span><span class="op">)</span>

<span class="va">sim_cluster_robust</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarize</a></span><span class="op">(</span>type1_error <span class="op">=</span> <span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">param_caught</span><span class="op">)</span><span class="op">/</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/context.html">n</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p>Now in this case, notice that we included the “, cluster(cluster_ID)” syntax in our regression command. Before we dive in to what this syntax did, let’s look at how the confidence intervals changed. Figure <a href="ch1.html#fig:ls-ci-robust">2.9</a> shows the distribution of the 95% confidence intervals where, again, the darkest region represents those estimates that incorrectly rejected the null. Now, when there are observations whose errors are correlated within a cluster, we find that estimating the model using least squares leads us back to a situation in which the type I error has decreased considerably.</p>
<div class="figure">
<span id="fig:ls-ci-robust"></span>
<img src="graphics/ls_ci_robust.jpg" alt="Distribution of 1,000 95% confidence intervals from a cluster robust least squares regression with dashed region representing those estimates that incorrectly reject the null." width="100%"><p class="caption">
Figure 2.9: Distribution of 1,000 95% confidence intervals from a cluster robust least squares regression with dashed region representing those estimates that incorrectly reject the null.
</p>
</div>
<p>This leads us to a natural question: what did the adjustment of the estimator’s variance do that caused the type I error to decrease by so much? Whatever it’s doing, it sure seems to be working! Let’s dive in to this adjustment with an example. Consider the following model:
<span class="math display">\[ 
y_{ig}=x_{ig}'\beta+u_{ig}\quad \text{where $1, \dots, G$}
\]</span>
and
<span class="math display">\[ 
E[u_{ig}u_{jg}']
\]</span>
which equals zero if <span class="math inline">\(g=g'\)</span> and equals <span class="math inline">\(\sigma_{(ij)g}\)</span> if <span class="math inline">\(g\ne g'\)</span>.</p>
<p>Let’s stack the data by cluster first.
<span class="math display">\[ 
y_g=x_g' \beta+u_g
\]</span>
The OLS estimator is still <span class="math inline">\(\widehat{\beta}= E[X'X]^{-1}X'Y\)</span>. We just stacked the data, which doesn’t affect the estimator itself. But it does change the variance.
<span class="math display">\[ 
V(\beta)=E\Big[[X'X]^{-1}X' \Omega X[X'X]^{-1}\Big]
\]</span>
With this in mind, we can now write the variance-covariance matrix for clustered data as
<span class="math display">\[ 
\widehat{V}(\widehat{\beta})=[X'X]^{-1}
   \left[ \sum_{i=1}^G x'_g \widehat{u}_g \widehat{u}_g'\right][X'X]^{-1}
\]</span></p>
<p>Adjusting for clustered data will be quite common in your applied work given the ubiquity of clustered data in the first place. It’s absolutely essential for working in the panel contexts, or in repeated cross-sections like the difference-in-differences design. But it also turns out to be important for experimental design, because often, the treatment will be at a higher level of aggregation than the microdata itself. In the real world, though, you can never assume that errors are independent draws from the same distribution. You need to know how your variables were constructed in the first place in order to choose the correct error structure for calculating your standard errors. If you have aggregate variables, like class size, then you’ll need to cluster at that level. If some treatment occurred at the state level, then you’ll need to cluster at that level. There’s a large literature available that looks at even more complex error structures, such as multi-way clustering <span class="citation">(Cameron, Gelbach, and Miller <a href="references.html#ref-Cameron2011" role="doc-biblioref">2011</a>)</span>.</p>
<p>But even the concept of the sample as the basis of standard errors may be shifting. It’s becoming increasingly less the case that researchers work with random samples; they are more likely working with administrative data containing the population itself, and thus the concept of sampling uncertainty becomes strained.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Usually we appeal to superpopulations in such situations where the observed population is simply itself a draw from some “super” population.&lt;/p&gt;"><sup>39</sup></a> For instance, <span class="citation">Manski and Pepper (<a href="references.html#ref-Manski2018" role="doc-biblioref">2018</a>)</span> wrote that “random sampling assumptions …are not natural when considering states or counties as units of observation.” So although a metaphor of a superpopulation may be useful for extending these classical uncertainty concepts, the ubiquity of digitized administrative data sets has led econometricians and statisticians to think about uncertainty in other ways.</p>
<p>New work by <span class="citation">Abadie et al. (<a href="references.html#ref-Abadie2020" role="doc-biblioref">2020</a>)</span> explores how sampling-based concepts of the standard error may not be the right way to think about uncertainty in the context of causal inference, or what they call <em>design-based uncertainty</em>. This work in many ways anticipates the next two chapters because of its direct reference to the concept of the counterfactual. Design-based uncertainty is a reflection of not knowing which values would have occurred had some intervention been different in counterfactual. And <span class="citation">Abadie et al. (<a href="references.html#ref-Abadie2020" role="doc-biblioref">2020</a>)</span> derive standard errors for design-based uncertainty, as opposed to sampling-based uncertainty. As luck would have it, those standard errors are usually <em>smaller</em>.</p>
<p>Let’s now move into these fundamental concepts of causality used in applied work and try to develop the tools to understand how counterfactuals and causality work together.</p>
<div class="cover-box">
<div class="row">
    <div class="col-xs-8 col-md-4 cover-img">
        <a href="https://www.amazon.com/dp/0300251688"><img src="images/cover.jpg" alt="Buy Today!"></a>
    </div>
    
    <div class="col-xs-12 col-md-8 cover-text-box">
            <h2> 
                Causal Inference: 
                <br><span style="font-style: italic; font-weight:bold; font-size: 20px;">The Mixtape.</span>
            </h2> 
            
        <div class="cover-text">
            <p>Buy the print version today:</p>
            
            <div class="chips">
                <a href="https://www.amazon.com/dp/0300251688" class="app-chip"> 
                    <i class="fab fa-amazon" aria-hidden="true"></i> Buy from Amazon 
                </a>
    
                <a href="https://yalebooks.yale.edu/book/9780300251685/causal-inference" class="app-chip"> 
                    <i class="fas fa-book" aria-hidden="true"></i> Buy from Yale Press 
                </a>
            </div>
        </div>
    </div>
</div>
</div>

</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="introduction.html"><span class="header-section-number">1</span> Introduction</a></div>
<div class="next"><a href="ch2.html"><span class="header-section-number">3</span> Directed Acyclic Graphs</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#ch1"><span class="header-section-number">2</span> Probability and Regression Review</a></li>
<li><a class="nav-link" href="#basic-probability-theory"><span class="header-section-number">2.1</span> Basic probability theory</a></li>
<li><a class="nav-link" href="#events-and-conditional-probability"><span class="header-section-number">2.2</span> Events and conditional probability</a></li>
<li><a class="nav-link" href="#probability-tree"><span class="header-section-number">2.3</span> Probability tree</a></li>
<li><a class="nav-link" href="#venn-diagrams-and-sets"><span class="header-section-number">2.4</span> Venn diagrams and sets</a></li>
<li><a class="nav-link" href="#contingency-tables"><span class="header-section-number">2.5</span> Contingency tables</a></li>
<li><a class="nav-link" href="#monty-hall-example"><span class="header-section-number">2.6</span> Monty Hall example</a></li>
<li><a class="nav-link" href="#summation-operator"><span class="header-section-number">2.7</span> Summation operator</a></li>
<li><a class="nav-link" href="#expected-value"><span class="header-section-number">2.8</span> Expected value</a></li>
<li><a class="nav-link" href="#variance"><span class="header-section-number">2.9</span> Variance</a></li>
<li><a class="nav-link" href="#covariance"><span class="header-section-number">2.10</span> Covariance</a></li>
<li><a class="nav-link" href="#population-model"><span class="header-section-number">2.11</span> Population model</a></li>
<li><a class="nav-link" href="#mean-independence"><span class="header-section-number">2.12</span> Mean independence</a></li>
<li><a class="nav-link" href="#ordinary-least-squares"><span class="header-section-number">2.13</span> Ordinary least squares</a></li>
<li><a class="nav-link" href="#algebraic-properties-of-ols"><span class="header-section-number">2.14</span> Algebraic Properties of OLS</a></li>
<li><a class="nav-link" href="#goodness-of-fit"><span class="header-section-number">2.15</span> Goodness-of-fit</a></li>
<li><a class="nav-link" href="#expected-value-of-ols"><span class="header-section-number">2.16</span> Expected value of OLS</a></li>
<li><a class="nav-link" href="#law-of-iterated-expectations"><span class="header-section-number">2.17</span> Law of iterated expectations</a></li>
<li><a class="nav-link" href="#cef-decomposition-property"><span class="header-section-number">2.18</span> CEF decomposition property</a></li>
<li><a class="nav-link" href="#cef-prediction-property"><span class="header-section-number">2.19</span> CEF prediction property</a></li>
<li><a class="nav-link" href="#anova-theory"><span class="header-section-number">2.20</span> ANOVA theory</a></li>
<li><a class="nav-link" href="#linear-cef-theorem"><span class="header-section-number">2.21</span> Linear CEF theorem</a></li>
<li><a class="nav-link" href="#best-linear-predictor-theorem"><span class="header-section-number">2.22</span> Best linear predictor theorem</a></li>
<li><a class="nav-link" href="#regression-cef-theorem"><span class="header-section-number">2.23</span> Regression CEF theorem</a></li>
<li><a class="nav-link" href="#regression-anatomy-theorem"><span class="header-section-number">2.24</span> Regression anatomy theorem</a></li>
<li><a class="nav-link" href="#variance-of-the-ols-estimators"><span class="header-section-number">2.25</span> Variance of the OLS estimators</a></li>
<li><a class="nav-link" href="#robust-standard-errors"><span class="header-section-number">2.26</span> Robust standard errors</a></li>
<li><a class="nav-link" href="#cluster-robust-standard-errors"><span class="header-section-number">2.27</span> Cluster robust standard errors</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/scunning1975/mixtape/blob/master/01-Probability_and_Regression.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/scunning1975/mixtape/edit/master/01-Probability_and_Regression.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong><span style="font-weight:bold">Causal Inference</span></strong>: <i>The Mixtape</i>" was written by Scott Cunningham. It was last built on 2020-12-19.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
